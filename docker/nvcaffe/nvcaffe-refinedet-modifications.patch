diff --git a/data/VOC0712Plus/README.md b/data/VOC0712Plus/README.md
new file mode 100644
index 0000000..7ae3cdd
--- /dev/null
+++ b/data/VOC0712Plus/README.md
@@ -0,0 +1,28 @@
+1. Download and extract VOC2007 and VOC2012 dataset. By default, we assume the data is stored in `$HOME/data/`
+  ```Shell
+  cd $HOME/data
+  wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar
+  wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar
+  wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar
+  tar -xvf VOCtrainval_11-May-2012.tar
+  tar -xvf VOCtrainval_06-Nov-2007.tar
+  tar -xvf VOCtest_06-Nov-2007.tar
+  # You should download the test set of VOC2012 as follow:
+  Download VOC2012test.tar via http://host.robots.ox.ac.uk:8080/eval/downloads/VOC2012test.tar and upzip it.
+  ```  
+
+2. Create the trainval.txt, test.txt, and test_name_size.txt in `data/VOC0712Plus/`
+  ```Shell
+  cd $CAFFE_ROOT
+  ./data/VOC0712Plus/create_list.sh
+  ```
+
+3. Create the LMDB file.
+  ```Shell
+  # You can modify the parameters in create_data.sh if needed.
+  # It will create lmdb files for trainval with encoded original image:
+  #   - $HOME/data/VOCdevkit/VOC0712Plus/lmdb/VOC0712Plus_trainval_lmdb
+  # and make soft links at examples/VOC0712Plus/
+  cd $CAFFE_ROOT
+  ./data/VOC0712Plus/create_data.sh
+  ```
diff --git a/data/VOC0712Plus/coco_voc_map.txt b/data/VOC0712Plus/coco_voc_map.txt
new file mode 100644
index 0000000..7ff84d1
--- /dev/null
+++ b/data/VOC0712Plus/coco_voc_map.txt
@@ -0,0 +1,21 @@
+0,0,background
+5,1,aeroplane
+2,2,bicycle
+15,3,bird
+9,4,boat
+40,5,bottle
+6,6,bus
+3,7,car
+16,8,cat
+57,9,chair
+20,10,cow
+61,11,diningtable
+17,12,dog
+18,13,horse
+4,14,motorbike
+1,15,person
+59,16,pottedplant
+19,17,sheep
+58,18,sofa
+7,19,train
+63,20,tvmonitor
diff --git a/data/VOC0712Plus/create_data.sh b/data/VOC0712Plus/create_data.sh
new file mode 100644
index 0000000..8e2a717
--- /dev/null
+++ b/data/VOC0712Plus/create_data.sh
@@ -0,0 +1,25 @@
+cur_dir=$(cd $( dirname ${BASH_SOURCE[0]} ) && pwd )
+root_dir=$cur_dir/../..
+
+cd $root_dir
+
+redo=1
+data_root_dir="$HOME/data/VOCdevkit/"
+dataset_name="VOC0712Plus"
+mapfile="$root_dir/data/$dataset_name/labelmap_voc.prototxt"
+anno_type="detection"
+db="lmdb"
+min_dim=0
+max_dim=0
+width=0
+height=0
+
+extra_cmd="--encode-type=jpg --encoded"
+if [ $redo ]
+then
+  extra_cmd="$extra_cmd --redo"
+fi
+for subset in trainval
+do
+  python $root_dir/scripts/create_annoset.py --anno-type=$anno_type --label-map-file=$mapfile --min-dim=$min_dim --max-dim=$max_dim --resize-width=$width --resize-height=$height --check-label $extra_cmd $data_root_dir $root_dir/data/$dataset_name/$subset.txt $data_root_dir/$dataset_name/$db/$dataset_name"_"$subset"_"$db examples/$dataset_name
+done
diff --git a/data/VOC0712Plus/create_list.sh b/data/VOC0712Plus/create_list.sh
new file mode 100644
index 0000000..2bbb4b4
--- /dev/null
+++ b/data/VOC0712Plus/create_list.sh
@@ -0,0 +1,77 @@
+#!/bin/bash
+
+root_dir=$HOME/data/VOCdevkit/
+sub_dir=ImageSets/Main
+bash_dir="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
+
+dst_file1=$bash_dir/trainval.txt
+if [ -f $dst_file1 ]
+then
+  rm -f $dst_file1
+fi
+
+dst_file2=$bash_dir/test.txt
+if [ -f $dst_file2 ]
+then
+  rm -f $dst_file2
+fi
+
+for dataset in trainval test
+do
+  for name in VOC2007 VOC2012
+  do
+    if [[ $dataset == "test" && $name == "VOC2012" ]]
+    then
+      echo "Create list for $name $dataset..."
+      dataset_file=$root_dir/$name/$sub_dir/$dataset.txt
+
+      img_file=$bash_dir/$dataset"_img.txt"
+      cp $dataset_file $img_file
+      sed -i "s/^/$name\/JPEGImages\//g" $img_file
+      sed -i "s/$/.jpg/g" $img_file
+
+      label_file=$bash_dir/$dataset"_label.txt"
+      cp $dataset_file $label_file
+      sed -i "s/^/$name\/Annotations\//g" $label_file
+      sed -i "s/$/.xml/g" $label_file
+
+      paste -d' ' $img_file $label_file >> $dst_file2
+
+      rm -f $label_file
+      rm -f $img_file
+      continue
+    else
+      echo "Create list for $name $dataset..."
+      dataset_file=$root_dir/$name/$sub_dir/$dataset.txt
+
+      img_file=$bash_dir/$dataset"_img.txt"
+      cp $dataset_file $img_file
+      sed -i "s/^/$name\/JPEGImages\//g" $img_file
+      sed -i "s/$/.jpg/g" $img_file
+
+      label_file=$bash_dir/$dataset"_label.txt"
+      cp $dataset_file $label_file
+      sed -i "s/^/$name\/Annotations\//g" $label_file
+      sed -i "s/$/.xml/g" $label_file
+
+      paste -d' ' $img_file $label_file >> $dst_file1
+
+      rm -f $label_file
+      rm -f $img_file
+    fi
+  done
+
+  # Generate image name and size infomation.
+  if [[ $dataset == "test" && $name == "VOC2012" ]]
+  then
+    $bash_dir/../../build/tools/get_image_size $root_dir $dst_file2 $bash_dir/$dataset"_name_size.txt"
+  fi
+
+  # Shuffle trainval file.
+  if [ $dataset == "trainval" ]
+  then
+    rand_file=$dst_file1.random
+    cat $dst_file1 | perl -MList::Util=shuffle -e 'print shuffle(<STDIN>);' > $rand_file
+    mv $rand_file $dst_file1
+  fi
+done
diff --git a/data/VOC0712Plus/labelmap_voc.prototxt b/data/VOC0712Plus/labelmap_voc.prototxt
new file mode 100644
index 0000000..b5c177b
--- /dev/null
+++ b/data/VOC0712Plus/labelmap_voc.prototxt
@@ -0,0 +1,105 @@
+item {
+  name: "none_of_the_above"
+  label: 0
+  display_name: "background"
+}
+item {
+  name: "aeroplane"
+  label: 1
+  display_name: "aeroplane"
+}
+item {
+  name: "bicycle"
+  label: 2
+  display_name: "bicycle"
+}
+item {
+  name: "bird"
+  label: 3
+  display_name: "bird"
+}
+item {
+  name: "boat"
+  label: 4
+  display_name: "boat"
+}
+item {
+  name: "bottle"
+  label: 5
+  display_name: "bottle"
+}
+item {
+  name: "bus"
+  label: 6
+  display_name: "bus"
+}
+item {
+  name: "car"
+  label: 7
+  display_name: "car"
+}
+item {
+  name: "cat"
+  label: 8
+  display_name: "cat"
+}
+item {
+  name: "chair"
+  label: 9
+  display_name: "chair"
+}
+item {
+  name: "cow"
+  label: 10
+  display_name: "cow"
+}
+item {
+  name: "diningtable"
+  label: 11
+  display_name: "diningtable"
+}
+item {
+  name: "dog"
+  label: 12
+  display_name: "dog"
+}
+item {
+  name: "horse"
+  label: 13
+  display_name: "horse"
+}
+item {
+  name: "motorbike"
+  label: 14
+  display_name: "motorbike"
+}
+item {
+  name: "person"
+  label: 15
+  display_name: "person"
+}
+item {
+  name: "pottedplant"
+  label: 16
+  display_name: "pottedplant"
+}
+item {
+  name: "sheep"
+  label: 17
+  display_name: "sheep"
+}
+item {
+  name: "sofa"
+  label: 18
+  display_name: "sofa"
+}
+item {
+  name: "train"
+  label: 19
+  display_name: "train"
+}
+item {
+  name: "tvmonitor"
+  label: 20
+  display_name: "tvmonitor"
+}
diff --git a/data/bdd100k/README.md b/data/bdd100k/README.md
new file mode 100644
index 0000000..e73f114
--- /dev/null
+++ b/data/bdd100k/README.md
@@ -0,0 +1,18 @@
+1. Download and extract bdd100k dataset. By default, we assume the data is stored in `$HOME/data/`
+
+2. Create the train.txt, val.txt, and val_name_size.txt in `data/bdd100k/`
+  ```Shell
+  cd $CAFFE_ROOT/datda/bdd100k
+  python ./create_list.py
+  ```
+
+3. Create the LMDB file.
+  ```Shell
+  # You can modify the parameters in create_data.sh if needed.
+  # It will create lmdb files for train and val with encoded original image:
+  #   - $HOME/data/BDD/bdd100k/bdd100k/lmdb/bdd100k_train_lmdb
+  #   - $HOME/data/BDD/bdd100k/bdd100k/lmdb/bdd100k_val_lmdb
+  # and make soft links at examples/bdd100k/
+  cd $CAFFE_ROOT
+  ./data/bdd100k/create_data.sh
+  ```
\ No newline at end of file
diff --git a/data/bdd100k/create_data.sh b/data/bdd100k/create_data.sh
new file mode 100644
index 0000000..8673be7
--- /dev/null
+++ b/data/bdd100k/create_data.sh
@@ -0,0 +1,37 @@
+cur_dir=$(cd $( dirname ${BASH_SOURCE[0]} ) && pwd )
+root_dir=$cur_dir/../..
+
+cd $root_dir
+
+redo=1
+data_root_dir="$HOME/data/BDD/bdd100k/"
+dataset_name="bdd100k"
+mapfile="$root_dir/data/$dataset_name/labelmap_bdd100k.prototxt"
+anno_type="detection"
+db="lmdb"
+min_dim=0
+max_dim=0
+width=0
+height=0
+
+extra_cmd="--encode-type=jpg --encoded"
+if [ $redo ]
+then
+  extra_cmd="$extra_cmd --redo"
+fi
+for subset in train val
+do
+  python $root_dir/scripts/create_annoset.py \
+    --anno-type=$anno_type \
+    --label-map-file=$mapfile \
+    --min-dim=$min_dim \
+    --max-dim=$max_dim \
+    --resize-width=$width \
+    --resize-height=$height \
+    --check-label \
+    $extra_cmd \
+    $data_root_dir \
+    $root_dir/data/$dataset_name/$subset.txt \
+    $data_root_dir/$dataset_name/$db/$dataset_name"_"$subset"_"$db \
+    examples/$dataset_name
+done
diff --git a/data/bdd100k/create_list.py b/data/bdd100k/create_list.py
new file mode 100644
index 0000000..114a55f
--- /dev/null
+++ b/data/bdd100k/create_list.py
@@ -0,0 +1,53 @@
+import glob
+import os
+import random
+from xml.etree.ElementTree import parse
+
+
+def basename_(filename):
+    return os.path.splitext(os.path.basename(filename))[0]
+
+
+if __name__ == "__main__":
+    save_dir = os.getcwd()
+    root_dir = '{}/data/BDD/bdd100k'.format(os.getenv("HOME"))
+    os.chdir(root_dir)
+    img_dir = 'images/100k'
+    xml_dir = 'xml'
+    sub_dir = {'train': 'train', 'val': 'val'}
+
+    for dataset in ['train', 'val']:
+        dst_file = "{}/{}.txt".format(save_dir, dataset)
+
+        xml_list = glob.glob(os.path.join(xml_dir, sub_dir[dataset], '*.xml'))
+        xml_list.sort()
+
+        with open(dst_file, 'w') as f:
+            for xml in xml_list:
+                img = xml.replace('.xml', '.jpg').replace(xml_dir, img_dir)
+                assert basename_(img) == basename_(xml), "img: %s, xml: %s" % (basename_(img), basename_(xml))
+                line = img + ' ' + xml + '\n'
+                f.write(line)
+
+        if dataset in ['train']:
+            with open(dst_file) as f:
+                lines = f.readlines()
+
+            random.shuffle(lines)
+            with open(dst_file, 'w') as f:
+                for line in lines:
+                    f.write(line)
+
+        # create name size txt file
+        if dataset in ['val']:
+            dst_file = "{}/{}_name_size.txt".format(save_dir, dataset)
+            with open(dst_file, 'w') as f:
+                for xml in xml_list:
+                    tree = parse(xml)
+                    note = tree.getroot()
+                    img_filename = os.path.splitext(note.findtext("filename"))[0]
+                    width = note.find("size").findtext("width")
+                    height = note.find("size").findtext('height')
+
+                    line = img_filename + ' ' + width + ' ' + height + '\n'
+                    f.write(line)
diff --git a/data/bdd100k/labelmap_bdd100k.prototxt b/data/bdd100k/labelmap_bdd100k.prototxt
new file mode 100644
index 0000000..87d7b53
--- /dev/null
+++ b/data/bdd100k/labelmap_bdd100k.prototxt
@@ -0,0 +1,55 @@
+item {
+  name: "none_of_the_above"
+  label: 0
+  display_name: "background"
+}
+item {
+  name: "traffic light"
+  label: 1
+  display_name: "traffic light"
+}
+item {
+  name: "traffic sign"
+  label: 2
+  display_name: "traffic sign"
+}
+item {
+  name: "person"
+  label: 3
+  display_name: "person"
+}
+item {
+  name: "rider"
+  label: 4
+  display_name: "rider"
+}
+item {
+  name: "car"
+  label: 5
+  display_name: "car"
+}
+item {
+  name: "truck"
+  label: 6
+  display_name: "truck"
+}
+item {
+  name: "bus"
+  label: 7
+  display_name: "bus"
+}
+item {
+  name: "train"
+  label: 8
+  display_name: "train"
+}
+item {
+  name: "motor"
+  label: 9
+  display_name: "motor"
+}
+item {
+  name: "bike"
+  label: 10
+  display_name: "bike"
+}
\ No newline at end of file
diff --git a/examples/refinedet/ResNet101_COCO_320.py b/examples/refinedet/ResNet101_COCO_320.py
new file mode 100644
index 0000000..7abf53e
--- /dev/null
+++ b/examples/refinedet/ResNet101_COCO_320.py
@@ -0,0 +1,584 @@
+from __future__ import print_function
+import sys
+sys.path.append("./python")
+import caffe
+from caffe.model_libs import *
+from google.protobuf import text_format
+
+import math
+import os
+import shutil
+import stat
+import subprocess
+
+# Add extra layers on top of a "base" network (e.g. VGGNet or ResNet).
+def AddExtraLayers(net, arm_source_layers=[], use_batchnorm=True):
+    use_relu = True
+
+    # Add additional convolutional layers.
+    # 320/32: 10 x 10
+    last_layer = net.keys()[-1]
+    from_layer = last_layer
+
+    # 320/64: 5 x 5
+    ResBody(net, from_layer, '6', out2a=128, out2b=128, out2c=512, stride=2, use_branch1=True)
+
+    arm_source_layers.reverse()
+    num_p = 6
+    for index, layer in enumerate(arm_source_layers):
+        from_layer = layer
+        out_layer = "TL{}_{}".format(num_p, 1)
+        ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 3, 1, 1)
+
+        if num_p == 6:
+            from_layer = out_layer
+            out_layer = "TL{}_{}".format(num_p, 2)
+            ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 3, 1, 1)
+
+            from_layer = out_layer
+            out_layer = "P{}".format(num_p)
+            ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 3, 1, 1)
+
+        else:
+            from_layer = out_layer
+            out_layer = "TL{}_{}".format(num_p, 2)
+            ConvBNLayer(net, from_layer, out_layer, use_batchnorm, False, 256, 3, 1, 1)
+
+            from_layer = "P{}".format(num_p+1)
+            out_layer = "P{}-up".format(num_p+1)
+            DeconvBNLayer(net, from_layer, out_layer, use_batchnorm, False, 256, 2, 0, 2)
+
+            from_layer = ["TL{}_{}".format(num_p, 2), "P{}-up".format(num_p+1)]
+            out_layer = "Elt{}".format(num_p)
+            EltwiseLayer(net, from_layer, out_layer)
+            relu_name = '{}_relu'.format(out_layer)
+            net[relu_name] = L.ReLU(net[out_layer], in_place=True)
+            out_layer = relu_name
+
+            from_layer = out_layer
+            out_layer = "P{}".format(num_p)
+            ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 3, 1, 1)
+        num_p = num_p - 1
+
+    return net
+
+
+### Modify the following parameters accordingly ###
+# The directory which contains the caffe code.
+# We assume you are running the script at the CAFFE_ROOT.
+caffe_root = os.getcwd()
+
+# Set true if you want to start training right after generating all files.
+run_soon = True
+# Set true if you want to load from most recently saved snapshot.
+# Otherwise, we will load from the pretrain_model defined below.
+resume_training = True
+# If true, Remove old model files.
+remove_old_models = False
+
+# The database file for training data. Created by data/coco/create_data.sh
+train_data = "examples/coco/coco_train_lmdb"
+# The database file for testing data. Created by data/coco/create_data.sh
+test_data = "examples/coco/coco_minival_lmdb"
+# Specify the batch sampler.
+resize_width = 320
+resize_height = 320
+resize = "{}x{}".format(resize_width, resize_height)
+batch_sampler = [
+        {
+                'sampler': {
+                        },
+                'max_trials': 1,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.1,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.3,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.5,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.7,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.9,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'max_jaccard_overlap': 1.0,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        ]
+train_transform_param = {
+        'mirror': True,
+        'mean_value': [104, 117, 123],
+        'force_color': True,
+        'resize_param': {
+                'prob': 1,
+                'resize_mode': P.Resize.WARP,
+                'height': resize_height,
+                'width': resize_width,
+                'interp_mode': [
+                        P.Resize.LINEAR,
+                        P.Resize.AREA,
+                        P.Resize.NEAREST,
+                        P.Resize.CUBIC,
+                        P.Resize.LANCZOS4,
+                        ],
+                },
+        'distort_param': {
+                'brightness_prob': 0.5,
+                'brightness_delta': 32,
+                'contrast_prob': 0.5,
+                'contrast_lower': 0.5,
+                'contrast_upper': 1.5,
+                'hue_prob': 0.5,
+                'hue_delta': 18,
+                'saturation_prob': 0.5,
+                'saturation_lower': 0.5,
+                'saturation_upper': 1.5,
+                'random_order_prob': 0.0,
+                },
+        'expand_param': {
+                'prob': 0.5,
+                'max_expand_ratio': 4.0,
+                },
+        'emit_constraint': {
+            'emit_type': caffe_pb2.EmitConstraint.CENTER,
+            }
+        }
+test_transform_param = {
+        'mean_value': [104, 117, 123],
+        'force_color': True,
+        'resize_param': {
+                'prob': 1,
+                'resize_mode': P.Resize.WARP,
+                'height': resize_height,
+                'width': resize_width,
+                'interp_mode': [P.Resize.LINEAR],
+                },
+        }
+
+
+# A learning rate for batch_size = 1, num_gpus = 1.
+base_lr = 0.00004 #0.00004
+
+# Modify the job name if you want.
+job_name = "refinedet_resnet101_{}".format(resize)
+# The name of the model. Modify it if you want.
+model_name = "coco_{}".format(job_name)
+
+# Directory which stores the model .prototxt file.
+save_dir = "models/ResNet/coco/{}".format(job_name)
+# Directory which stores the snapshot of models.
+snapshot_dir = "models/ResNet/coco/{}".format(job_name)
+# Directory which stores the job script and log file.
+job_dir = "jobs/ResNet/coco/{}".format(job_name)
+
+# model definition files.
+train_net_file = "{}/train.prototxt".format(save_dir)
+test_net_file = "{}/test.prototxt".format(save_dir)
+deploy_net_file = "{}/deploy.prototxt".format(save_dir)
+solver_file = "{}/solver.prototxt".format(save_dir)
+# snapshot prefix.
+snapshot_prefix = "{}/{}".format(snapshot_dir, model_name)
+# job script path.
+job_file = "{}/{}.sh".format(job_dir, model_name)
+
+# Stores the test image names and sizes. Created by data/coco/create_list.sh
+name_size_file = "data/coco/minival2014_name_size.txt"
+# The pretrained ResNet101 model from https://github.com/KaimingHe/deep-residual-networks.
+pretrain_model = "models/ResNet/ResNet-101-model.caffemodel"
+# Stores LabelMapItem.
+label_map_file = "data/coco/labelmap_coco.prototxt"
+
+# MultiBoxLoss parameters.
+num_classes = 81
+share_location = True
+background_label_id = 0
+train_on_diff_gt = False
+normalization_mode = P.Loss.VALID
+code_type = P.PriorBox.CENTER_SIZE
+ignore_cross_boundary_bbox = False
+mining_type = P.MultiBoxLoss.MAX_NEGATIVE
+neg_pos_ratio = 3.
+loc_weight = (neg_pos_ratio + 1.) / 4.
+multibox_loss_param = {
+    'loc_loss_type': P.MultiBoxLoss.SMOOTH_L1,
+    'conf_loss_type': P.MultiBoxLoss.SOFTMAX,
+    'loc_weight': loc_weight,
+    'num_classes': num_classes,
+    'share_location': share_location,
+    'match_type': P.MultiBoxLoss.PER_PREDICTION,
+    'overlap_threshold': 0.5,
+    'use_prior_for_matching': True,
+    'background_label_id': background_label_id,
+    'use_difficult_gt': train_on_diff_gt,
+    'mining_type': mining_type,
+    'neg_pos_ratio': neg_pos_ratio,
+    'neg_overlap': 0.5,
+    'code_type': code_type,
+    'ignore_cross_boundary_bbox': ignore_cross_boundary_bbox,
+    'objectness_score': 0.01,
+    }
+loss_param = {
+    'normalization': normalization_mode,
+    }
+
+# parameters for generating priors.
+# minimum dimension of input image
+min_dim = 320
+# res3b3_relu ==> 40 x 40
+# res4b22_relu ==> 20 x 20
+# res5c_relu ==> 10 x 10
+# res5c_relu/conv1_2_relu ==> 5 x 5
+arm_source_layers = ['res3b3_relu', 'res4b22_relu', 'res5c_relu', 'res6_relu']
+odm_source_layers = ['P3', 'P4', 'P5', 'P6']
+min_sizes = [32, 64, 128, 256]
+max_sizes = [[], [], [], []]
+steps = [8, 16, 32, 64]
+aspect_ratios = [[2], [2], [2], [2]]
+# variance used to encode/decode prior bboxes.
+if code_type == P.PriorBox.CENTER_SIZE:
+  prior_variance = [0.1, 0.1, 0.2, 0.2]
+else:
+  prior_variance = [0.1]
+flip = True
+clip = False
+
+# Solver parameters.
+# Defining which GPUs to use.
+gpus = "0,1,2,3"
+gpulist = gpus.split(",")
+num_gpus = len(gpulist)
+
+# Divide the mini-batch to different GPUs.
+batch_size = 32
+accum_batch_size = 32
+iter_size = accum_batch_size / batch_size
+solver_mode = P.Solver.CPU
+device_id = 0
+batch_size_per_device = batch_size
+if num_gpus > 0:
+  batch_size_per_device = int(math.ceil(float(batch_size) / num_gpus))
+  iter_size = int(math.ceil(float(accum_batch_size) / (batch_size_per_device * num_gpus)))
+  solver_mode = P.Solver.GPU
+  device_id = int(gpulist[0])
+
+if normalization_mode == P.Loss.NONE:
+  base_lr /= batch_size_per_device
+elif normalization_mode == P.Loss.VALID:
+  base_lr *= 25. / loc_weight
+elif normalization_mode == P.Loss.FULL:
+  # Roughly there are 2000 prior bboxes per image.
+  # TODO(weiliu89): Estimate the exact # of priors.
+  base_lr *= 2000.
+
+# Evaluate on whole test set.
+num_test_image = 5000
+test_batch_size = 1
+test_iter = num_test_image / test_batch_size
+
+solver_param = {
+    # Train parameters
+    'base_lr': base_lr,
+    'weight_decay': 0.0005,
+    'lr_policy': "multistep",
+    'stepvalue': [280000, 360000, 400000],
+    'gamma': 0.1,
+    'momentum': 0.9,
+    'iter_size': iter_size,
+    'max_iter': 400000,
+    'snapshot': 10000,
+    'display': 10,
+    'average_loss': 10,
+    'type': "SGD",
+    'solver_mode': solver_mode,
+    'device_id': device_id,
+    'debug_info': False,
+    'snapshot_after_train': True,
+    # Test parameters
+    # 'test_iter': [test_iter],
+    # 'test_interval': 10000,
+    # 'eval_type': "detection",
+    # 'ap_version': "11point",
+    # 'test_initialization': False,
+    }
+
+# parameters for generating detection output.
+det_out_param = {
+    'num_classes': num_classes,
+    'share_location': share_location,
+    'background_label_id': background_label_id,
+    'nms_param': {'nms_threshold': 0.45, 'top_k': 1000},
+    'keep_top_k': 500,
+    'confidence_threshold': 0.01,
+    'code_type': code_type,
+    'objectness_score': 0.01,
+    }
+
+# parameters for evaluating detection results.
+det_eval_param = {
+    'num_classes': num_classes,
+    'background_label_id': background_label_id,
+    'overlap_threshold': 0.5,
+    'evaluate_difficult_gt': False,
+    'name_size_file': name_size_file,
+    }
+
+### Hopefully you don't need to change the following ###
+# Check file.
+check_if_exist(train_data)
+check_if_exist(test_data)
+check_if_exist(label_map_file)
+check_if_exist(pretrain_model)
+make_if_not_exist(save_dir)
+make_if_not_exist(job_dir)
+make_if_not_exist(snapshot_dir)
+
+# Create train net.
+net = caffe.NetSpec()
+net.data, net.label = CreateAnnotatedDataLayer(train_data, batch_size=batch_size_per_device,
+        train=True, output_label=True, label_map_file=label_map_file,
+        transform_param=train_transform_param, batch_sampler=batch_sampler)
+
+ResNet101Body(net, from_layer='data', use_pool5=False, use_dilation_conv5=False)
+
+AddExtraLayers(net, arm_source_layers, use_batchnorm=True)
+arm_source_layers.reverse()
+
+mbox_layers = CreateRefineDetHead(net, data_layer='data', from_layers=arm_source_layers,
+        use_batchnorm=False, min_sizes=min_sizes, max_sizes=max_sizes,
+        aspect_ratios=aspect_ratios, steps=steps, normalizations=[],
+        num_classes=num_classes, share_location=share_location, flip=flip, clip=clip,
+        prior_variance=prior_variance, kernel_size=3, pad=1, lr_mult=1, from_layers2=odm_source_layers,
+        inter_layer_depth = [1, 1, 1, 1])
+
+name = "arm_loss"
+mbox_layers_arm = []
+mbox_layers_arm.append(mbox_layers[0])
+mbox_layers_arm.append(mbox_layers[1])
+mbox_layers_arm.append(mbox_layers[2])
+mbox_layers_arm.append(net.label)
+multibox_loss_param_arm = multibox_loss_param.copy()
+multibox_loss_param_arm['num_classes'] = 2
+net[name] = L.MultiBoxLoss(*mbox_layers_arm, multibox_loss_param=multibox_loss_param_arm,
+        loss_param=loss_param, include=dict(phase=caffe_pb2.Phase.Value('TRAIN')),
+        propagate_down=[True, True, False, False])
+
+# Create the MultiBoxLossLayer.
+conf_name = "arm_conf"
+reshape_name = "{}_reshape".format(conf_name)
+net[reshape_name] = L.Reshape(net[conf_name], shape=dict(dim=[0, -1, 2]))
+softmax_name = "{}_softmax".format(conf_name)
+net[softmax_name] = L.Softmax(net[reshape_name], axis=2)
+flatten_name = "{}_flatten".format(conf_name)
+net[flatten_name] = L.Flatten(net[softmax_name], axis=1)
+
+name = "odm_loss"
+mbox_layers_odm = []
+mbox_layers_odm.append(mbox_layers[3])
+mbox_layers_odm.append(mbox_layers[4])
+mbox_layers_odm.append(mbox_layers[2])
+mbox_layers_odm.append(net.label)
+mbox_layers_odm.append(net[flatten_name])
+mbox_layers_odm.append(mbox_layers[0])
+net[name] = L.MultiBoxLoss(*mbox_layers_odm, multibox_loss_param=multibox_loss_param,
+        loss_param=loss_param, include=dict(phase=caffe_pb2.Phase.Value('TRAIN')),
+        propagate_down=[True, True, False, False, False, False])
+
+
+with open(train_net_file, 'w') as f:
+    print('name: "{}_train"'.format(model_name), file=f)
+    print(net.to_proto(), file=f)
+shutil.copy(train_net_file, job_dir)
+
+# Create test net.
+net = caffe.NetSpec()
+net.data, net.label = CreateAnnotatedDataLayer(test_data, batch_size=test_batch_size,
+        train=False, output_label=True, label_map_file=label_map_file,
+        transform_param=test_transform_param)
+
+ResNet101Body(net, from_layer='data', use_pool5=False, use_dilation_conv5=False)
+
+AddExtraLayers(net, arm_source_layers, use_batchnorm=True)
+arm_source_layers.reverse()
+
+mbox_layers = CreateRefineDetHead(net, data_layer='data', from_layers=arm_source_layers,
+        use_batchnorm=False, min_sizes=min_sizes, max_sizes=max_sizes,
+        aspect_ratios=aspect_ratios, steps=steps, normalizations=[],
+        num_classes=num_classes, share_location=share_location, flip=flip, clip=clip,
+        prior_variance=prior_variance, kernel_size=3, pad=1, lr_mult=1, from_layers2=odm_source_layers,
+        inter_layer_depth = [1, 1, 1, 1])
+
+mbox_layers_out = []
+mbox_layers_out.append(mbox_layers[3])
+mbox_layers_out.append(mbox_layers[4])
+mbox_layers_out.append(mbox_layers[2])
+mbox_layers_out.append(mbox_layers[1])
+mbox_layers_out.append(mbox_layers[0])
+
+conf_name = "arm_conf"
+reshape_name = "{}_reshape".format(conf_name)
+net[reshape_name] = L.Reshape(net[conf_name], shape=dict(dim=[0, -1, 2]))
+softmax_name = "{}_softmax".format(conf_name)
+net[softmax_name] = L.Softmax(net[reshape_name], axis=2)
+flatten_name = "{}_flatten".format(conf_name)
+net[flatten_name] = L.Flatten(net[softmax_name], axis=1)
+mbox_layers_out[3] = net[flatten_name]
+
+conf_name = "odm_conf"
+reshape_name = "{}_reshape".format(conf_name)
+net[reshape_name] = L.Reshape(net[conf_name], shape=dict(dim=[0, -1, num_classes]))
+softmax_name = "{}_softmax".format(conf_name)
+net[softmax_name] = L.Softmax(net[reshape_name], axis=2)
+flatten_name = "{}_flatten".format(conf_name)
+net[flatten_name] = L.Flatten(net[softmax_name], axis=1)
+mbox_layers_out[1] = net[flatten_name]
+
+net.detection_out = L.DetectionOutput(*mbox_layers_out,
+    detection_output_param=det_out_param,
+    include=dict(phase=caffe_pb2.Phase.Value('TEST')))
+net.detection_eval = L.DetectionEvaluate(net.detection_out, net.label,
+    detection_evaluate_param=det_eval_param,
+    include=dict(phase=caffe_pb2.Phase.Value('TEST')))
+
+with open(test_net_file, 'w') as f:
+    print('name: "{}_test"'.format(model_name), file=f)
+    print(net.to_proto(), file=f)
+shutil.copy(test_net_file, job_dir)
+
+# Create deploy net.
+# Remove the first and last layer from test net.
+deploy_net = net
+with open(deploy_net_file, 'w') as f:
+    net_param = deploy_net.to_proto()
+    # Remove the first (AnnotatedData) and last (DetectionEvaluate) layer from test net.
+    del net_param.layer[0]
+    del net_param.layer[-1]
+    net_param.name = '{}_deploy'.format(model_name)
+    net_param.input.extend(['data'])
+    net_param.input_shape.extend([
+        caffe_pb2.BlobShape(dim=[1, 3, resize_height, resize_width])])
+    print(net_param, file=f)
+shutil.copy(deploy_net_file, job_dir)
+
+# Create solver.
+solver = caffe_pb2.SolverParameter(
+        train_net=train_net_file,
+        # test_net=[test_net_file],
+        snapshot_prefix=snapshot_prefix,
+        **solver_param)
+
+with open(solver_file, 'w') as f:
+    print(solver, file=f)
+shutil.copy(solver_file, job_dir)
+
+max_iter = 0
+# Find most recent snapshot.
+for file in os.listdir(snapshot_dir):
+  if file.endswith(".solverstate"):
+    basename = os.path.splitext(file)[0]
+    iter = int(basename.split("{}_iter_".format(model_name))[1])
+    if iter > max_iter:
+      max_iter = iter
+
+train_src_param = '--weights="{}" \\\n'.format(pretrain_model)
+if resume_training:
+  if max_iter > 0:
+    train_src_param = '--snapshot="{}_iter_{}.solverstate" \\\n'.format(snapshot_prefix, max_iter)
+
+if remove_old_models:
+  # Remove any snapshots smaller than max_iter.
+  for file in os.listdir(snapshot_dir):
+    if file.endswith(".solverstate"):
+      basename = os.path.splitext(file)[0]
+      iter = int(basename.split("{}_iter_".format(model_name))[1])
+      if max_iter > iter:
+        os.remove("{}/{}".format(snapshot_dir, file))
+    if file.endswith(".caffemodel"):
+      basename = os.path.splitext(file)[0]
+      iter = int(basename.split("{}_iter_".format(model_name))[1])
+      if max_iter > iter:
+        os.remove("{}/{}".format(snapshot_dir, file))
+
+# Create job file.
+with open(job_file, 'w') as f:
+  f.write('cd {}\n'.format(caffe_root))
+  f.write('./build/tools/caffe train \\\n')
+  f.write('--solver="{}" \\\n'.format(solver_file))
+  f.write(train_src_param)
+  if solver_param['solver_mode'] == P.Solver.GPU:
+    f.write('--gpu {} 2>&1 | tee {}/{}.log\n'.format(gpus, job_dir, model_name))
+  else:
+    f.write('2>&1 | tee {}/{}.log\n'.format(job_dir, model_name))
+
+# Copy the python script to job_dir.
+py_file = os.path.abspath(__file__)
+shutil.copy(py_file, job_dir)
+
+# Run the job.
+os.chmod(job_file, stat.S_IRWXU)
+if run_soon:
+  subprocess.call(job_file, shell=True)
diff --git a/examples/refinedet/ResNet101_COCO_512.py b/examples/refinedet/ResNet101_COCO_512.py
new file mode 100644
index 0000000..d741af6
--- /dev/null
+++ b/examples/refinedet/ResNet101_COCO_512.py
@@ -0,0 +1,584 @@
+from __future__ import print_function
+import sys
+sys.path.append("./python")
+import caffe
+from caffe.model_libs import *
+from google.protobuf import text_format
+
+import math
+import os
+import shutil
+import stat
+import subprocess
+
+# Add extra layers on top of a "base" network (e.g. VGGNet or ResNet).
+def AddExtraLayers(net, arm_source_layers=[], use_batchnorm=True):
+    use_relu = True
+
+    # Add additional convolutional layers.
+    # 512/32: 16 x 16
+    last_layer = net.keys()[-1]
+    from_layer = last_layer
+
+    # 512/64: 8 x 8
+    ResBody(net, from_layer, '6', out2a=128, out2b=128, out2c=512, stride=2, use_branch1=True)
+
+    arm_source_layers.reverse()
+    num_p = 6
+    for index, layer in enumerate(arm_source_layers):
+        from_layer = layer
+        out_layer = "TL{}_{}".format(num_p, 1)
+        ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 3, 1, 1)
+
+        if num_p == 6:
+            from_layer = out_layer
+            out_layer = "TL{}_{}".format(num_p, 2)
+            ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 3, 1, 1)
+
+            from_layer = out_layer
+            out_layer = "P{}".format(num_p)
+            ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 3, 1, 1)
+
+        else:
+            from_layer = out_layer
+            out_layer = "TL{}_{}".format(num_p, 2)
+            ConvBNLayer(net, from_layer, out_layer, use_batchnorm, False, 256, 3, 1, 1)
+
+            from_layer = "P{}".format(num_p+1)
+            out_layer = "P{}-up".format(num_p+1)
+            DeconvBNLayer(net, from_layer, out_layer, use_batchnorm, False, 256, 2, 0, 2)
+
+            from_layer = ["TL{}_{}".format(num_p, 2), "P{}-up".format(num_p+1)]
+            out_layer = "Elt{}".format(num_p)
+            EltwiseLayer(net, from_layer, out_layer)
+            relu_name = '{}_relu'.format(out_layer)
+            net[relu_name] = L.ReLU(net[out_layer], in_place=True)
+            out_layer = relu_name
+
+            from_layer = out_layer
+            out_layer = "P{}".format(num_p)
+            ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 3, 1, 1)
+        num_p = num_p - 1
+
+    return net
+
+
+### Modify the following parameters accordingly ###
+# The directory which contains the caffe code.
+# We assume you are running the script at the CAFFE_ROOT.
+caffe_root = os.getcwd()
+
+# Set true if you want to start training right after generating all files.
+run_soon = True
+# Set true if you want to load from most recently saved snapshot.
+# Otherwise, we will load from the pretrain_model defined below.
+resume_training = True
+# If true, Remove old model files.
+remove_old_models = False
+
+# The database file for training data. Created by data/coco/create_data.sh
+train_data = "examples/coco/coco_train_lmdb"
+# The database file for testing data. Created by data/coco/create_data.sh
+test_data = "examples/coco/coco_minival_lmdb"
+# Specify the batch sampler.
+resize_width = 512
+resize_height = 512
+resize = "{}x{}".format(resize_width, resize_height)
+batch_sampler = [
+        {
+                'sampler': {
+                        },
+                'max_trials': 1,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.1,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.3,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.5,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.7,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.9,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'max_jaccard_overlap': 1.0,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        ]
+train_transform_param = {
+        'mirror': True,
+        'mean_value': [104, 117, 123],
+        'force_color': True,
+        'resize_param': {
+                'prob': 1,
+                'resize_mode': P.Resize.WARP,
+                'height': resize_height,
+                'width': resize_width,
+                'interp_mode': [
+                        P.Resize.LINEAR,
+                        P.Resize.AREA,
+                        P.Resize.NEAREST,
+                        P.Resize.CUBIC,
+                        P.Resize.LANCZOS4,
+                        ],
+                },
+        'distort_param': {
+                'brightness_prob': 0.5,
+                'brightness_delta': 32,
+                'contrast_prob': 0.5,
+                'contrast_lower': 0.5,
+                'contrast_upper': 1.5,
+                'hue_prob': 0.5,
+                'hue_delta': 18,
+                'saturation_prob': 0.5,
+                'saturation_lower': 0.5,
+                'saturation_upper': 1.5,
+                'random_order_prob': 0.0,
+                },
+        'expand_param': {
+                'prob': 0.5,
+                'max_expand_ratio': 4.0,
+                },
+        'emit_constraint': {
+            'emit_type': caffe_pb2.EmitConstraint.CENTER,
+            }
+        }
+test_transform_param = {
+        'mean_value': [104, 117, 123],
+        'force_color': True,
+        'resize_param': {
+                'prob': 1,
+                'resize_mode': P.Resize.WARP,
+                'height': resize_height,
+                'width': resize_width,
+                'interp_mode': [P.Resize.LINEAR],
+                },
+        }
+
+
+# A learning rate for batch_size = 1, num_gpus = 1.
+base_lr = 0.00004 #0.00004
+
+# Modify the job name if you want.
+job_name = "refinedet_resnet101_{}".format(resize)
+# The name of the model. Modify it if you want.
+model_name = "coco_{}".format(job_name)
+
+# Directory which stores the model .prototxt file.
+save_dir = "models/ResNet/coco/{}".format(job_name)
+# Directory which stores the snapshot of models.
+snapshot_dir = "models/ResNet/coco/{}".format(job_name)
+# Directory which stores the job script and log file.
+job_dir = "jobs/ResNet/coco/{}".format(job_name)
+
+# model definition files.
+train_net_file = "{}/train.prototxt".format(save_dir)
+test_net_file = "{}/test.prototxt".format(save_dir)
+deploy_net_file = "{}/deploy.prototxt".format(save_dir)
+solver_file = "{}/solver.prototxt".format(save_dir)
+# snapshot prefix.
+snapshot_prefix = "{}/{}".format(snapshot_dir, model_name)
+# job script path.
+job_file = "{}/{}.sh".format(job_dir, model_name)
+
+# Stores the test image names and sizes. Created by data/coco/create_list.sh
+name_size_file = "data/coco/minival2014_name_size.txt"
+# The pretrained ResNet101 model from https://github.com/KaimingHe/deep-residual-networks.
+pretrain_model = "models/ResNet/ResNet-101-model.caffemodel"
+# Stores LabelMapItem.
+label_map_file = "data/coco/labelmap_coco.prototxt"
+
+# MultiBoxLoss parameters.
+num_classes = 81
+share_location = True
+background_label_id = 0
+train_on_diff_gt = False
+normalization_mode = P.Loss.VALID
+code_type = P.PriorBox.CENTER_SIZE
+ignore_cross_boundary_bbox = False
+mining_type = P.MultiBoxLoss.MAX_NEGATIVE
+neg_pos_ratio = 3.
+loc_weight = (neg_pos_ratio + 1.) / 4.
+multibox_loss_param = {
+    'loc_loss_type': P.MultiBoxLoss.SMOOTH_L1,
+    'conf_loss_type': P.MultiBoxLoss.SOFTMAX,
+    'loc_weight': loc_weight,
+    'num_classes': num_classes,
+    'share_location': share_location,
+    'match_type': P.MultiBoxLoss.PER_PREDICTION,
+    'overlap_threshold': 0.5,
+    'use_prior_for_matching': True,
+    'background_label_id': background_label_id,
+    'use_difficult_gt': train_on_diff_gt,
+    'mining_type': mining_type,
+    'neg_pos_ratio': neg_pos_ratio,
+    'neg_overlap': 0.5,
+    'code_type': code_type,
+    'ignore_cross_boundary_bbox': ignore_cross_boundary_bbox,
+    'objectness_score': 0.01,
+    }
+loss_param = {
+    'normalization': normalization_mode,
+    }
+
+# parameters for generating priors.
+# minimum dimension of input image
+min_dim = 512
+# res3b3_relu ==> 64 x 64
+# res4b22_relu ==> 32 x 32
+# res5c_relu ==> 16 x 16
+# res5c_relu/conv1_2_relu ==> 8 x 8
+arm_source_layers = ['res3b3_relu', 'res4b22_relu', 'res5c_relu', 'res6_relu']
+odm_source_layers = ['P3', 'P4', 'P5', 'P6']
+min_sizes = [32, 64, 128, 256]
+max_sizes = [[], [], [], []]
+steps = [8, 16, 32, 64]
+aspect_ratios = [[2], [2], [2], [2]]
+# variance used to encode/decode prior bboxes.
+if code_type == P.PriorBox.CENTER_SIZE:
+  prior_variance = [0.1, 0.1, 0.2, 0.2]
+else:
+  prior_variance = [0.1]
+flip = True
+clip = False
+
+# Solver parameters.
+# Defining which GPUs to use.
+gpus = "0,1,2,3"
+gpulist = gpus.split(",")
+num_gpus = len(gpulist)
+
+# Divide the mini-batch to different GPUs.
+batch_size = 20
+accum_batch_size = 20
+iter_size = accum_batch_size / batch_size
+solver_mode = P.Solver.CPU
+device_id = 0
+batch_size_per_device = batch_size
+if num_gpus > 0:
+  batch_size_per_device = int(math.ceil(float(batch_size) / num_gpus))
+  iter_size = int(math.ceil(float(accum_batch_size) / (batch_size_per_device * num_gpus)))
+  solver_mode = P.Solver.GPU
+  device_id = int(gpulist[0])
+
+if normalization_mode == P.Loss.NONE:
+  base_lr /= batch_size_per_device
+elif normalization_mode == P.Loss.VALID:
+  base_lr *= 25. / loc_weight
+elif normalization_mode == P.Loss.FULL:
+  # Roughly there are 2000 prior bboxes per image.
+  # TODO(weiliu89): Estimate the exact # of priors.
+  base_lr *= 2000.
+
+# Evaluate on whole test set.
+num_test_image = 5000
+test_batch_size = 1
+test_iter = num_test_image / test_batch_size
+
+solver_param = {
+    # Train parameters
+    'base_lr': base_lr,
+    'weight_decay': 0.0005,
+    'lr_policy': "multistep",
+    'stepvalue': [400000, 480000, 540000],
+    'gamma': 0.1,
+    'momentum': 0.9,
+    'iter_size': iter_size,
+    'max_iter': 540000,
+    'snapshot': 10000,
+    'display': 10,
+    'average_loss': 10,
+    'type': "SGD",
+    'solver_mode': solver_mode,
+    'device_id': device_id,
+    'debug_info': False,
+    'snapshot_after_train': True,
+    # Test parameters
+    # 'test_iter': [test_iter],
+    # 'test_interval': 10000,
+    # 'eval_type': "detection",
+    # 'ap_version': "11point",
+    # 'test_initialization': False,
+    }
+
+# parameters for generating detection output.
+det_out_param = {
+    'num_classes': num_classes,
+    'share_location': share_location,
+    'background_label_id': background_label_id,
+    'nms_param': {'nms_threshold': 0.45, 'top_k': 1000},
+    'keep_top_k': 500,
+    'confidence_threshold': 0.01,
+    'code_type': code_type,
+    'objectness_score': 0.01,
+    }
+
+# parameters for evaluating detection results.
+det_eval_param = {
+    'num_classes': num_classes,
+    'background_label_id': background_label_id,
+    'overlap_threshold': 0.5,
+    'evaluate_difficult_gt': False,
+    'name_size_file': name_size_file,
+    }
+
+### Hopefully you don't need to change the following ###
+# Check file.
+check_if_exist(train_data)
+check_if_exist(test_data)
+check_if_exist(label_map_file)
+check_if_exist(pretrain_model)
+make_if_not_exist(save_dir)
+make_if_not_exist(job_dir)
+make_if_not_exist(snapshot_dir)
+
+# Create train net.
+net = caffe.NetSpec()
+net.data, net.label = CreateAnnotatedDataLayer(train_data, batch_size=batch_size_per_device,
+        train=True, output_label=True, label_map_file=label_map_file,
+        transform_param=train_transform_param, batch_sampler=batch_sampler)
+
+ResNet101Body(net, from_layer='data', use_pool5=False, use_dilation_conv5=False)
+
+AddExtraLayers(net, arm_source_layers, use_batchnorm=True)
+arm_source_layers.reverse()
+
+mbox_layers = CreateRefineDetHead(net, data_layer='data', from_layers=arm_source_layers,
+        use_batchnorm=False, min_sizes=min_sizes, max_sizes=max_sizes,
+        aspect_ratios=aspect_ratios, steps=steps, normalizations=[],
+        num_classes=num_classes, share_location=share_location, flip=flip, clip=clip,
+        prior_variance=prior_variance, kernel_size=3, pad=1, lr_mult=1, from_layers2=odm_source_layers,
+        inter_layer_depth = [1, 1, 1, 1])
+
+name = "arm_loss"
+mbox_layers_arm = []
+mbox_layers_arm.append(mbox_layers[0])
+mbox_layers_arm.append(mbox_layers[1])
+mbox_layers_arm.append(mbox_layers[2])
+mbox_layers_arm.append(net.label)
+multibox_loss_param_arm = multibox_loss_param.copy()
+multibox_loss_param_arm['num_classes'] = 2
+net[name] = L.MultiBoxLoss(*mbox_layers_arm, multibox_loss_param=multibox_loss_param_arm,
+        loss_param=loss_param, include=dict(phase=caffe_pb2.Phase.Value('TRAIN')),
+        propagate_down=[True, True, False, False])
+
+# Create the MultiBoxLossLayer.
+conf_name = "arm_conf"
+reshape_name = "{}_reshape".format(conf_name)
+net[reshape_name] = L.Reshape(net[conf_name], shape=dict(dim=[0, -1, 2]))
+softmax_name = "{}_softmax".format(conf_name)
+net[softmax_name] = L.Softmax(net[reshape_name], axis=2)
+flatten_name = "{}_flatten".format(conf_name)
+net[flatten_name] = L.Flatten(net[softmax_name], axis=1)
+
+name = "odm_loss"
+mbox_layers_odm = []
+mbox_layers_odm.append(mbox_layers[3])
+mbox_layers_odm.append(mbox_layers[4])
+mbox_layers_odm.append(mbox_layers[2])
+mbox_layers_odm.append(net.label)
+mbox_layers_odm.append(net[flatten_name])
+mbox_layers_odm.append(mbox_layers[0])
+net[name] = L.MultiBoxLoss(*mbox_layers_odm, multibox_loss_param=multibox_loss_param,
+        loss_param=loss_param, include=dict(phase=caffe_pb2.Phase.Value('TRAIN')),
+        propagate_down=[True, True, False, False, False, False])
+
+
+with open(train_net_file, 'w') as f:
+    print('name: "{}_train"'.format(model_name), file=f)
+    print(net.to_proto(), file=f)
+shutil.copy(train_net_file, job_dir)
+
+# Create test net.
+net = caffe.NetSpec()
+net.data, net.label = CreateAnnotatedDataLayer(test_data, batch_size=test_batch_size,
+        train=False, output_label=True, label_map_file=label_map_file,
+        transform_param=test_transform_param)
+
+ResNet101Body(net, from_layer='data', use_pool5=False, use_dilation_conv5=False)
+
+AddExtraLayers(net, arm_source_layers, use_batchnorm=True)
+arm_source_layers.reverse()
+
+mbox_layers = CreateRefineDetHead(net, data_layer='data', from_layers=arm_source_layers,
+        use_batchnorm=False, min_sizes=min_sizes, max_sizes=max_sizes,
+        aspect_ratios=aspect_ratios, steps=steps, normalizations=[],
+        num_classes=num_classes, share_location=share_location, flip=flip, clip=clip,
+        prior_variance=prior_variance, kernel_size=3, pad=1, lr_mult=1, from_layers2=odm_source_layers,
+        inter_layer_depth = [1, 1, 1, 1])
+
+mbox_layers_out = []
+mbox_layers_out.append(mbox_layers[3])
+mbox_layers_out.append(mbox_layers[4])
+mbox_layers_out.append(mbox_layers[2])
+mbox_layers_out.append(mbox_layers[1])
+mbox_layers_out.append(mbox_layers[0])
+
+conf_name = "arm_conf"
+reshape_name = "{}_reshape".format(conf_name)
+net[reshape_name] = L.Reshape(net[conf_name], shape=dict(dim=[0, -1, 2]))
+softmax_name = "{}_softmax".format(conf_name)
+net[softmax_name] = L.Softmax(net[reshape_name], axis=2)
+flatten_name = "{}_flatten".format(conf_name)
+net[flatten_name] = L.Flatten(net[softmax_name], axis=1)
+mbox_layers_out[3] = net[flatten_name]
+
+conf_name = "odm_conf"
+reshape_name = "{}_reshape".format(conf_name)
+net[reshape_name] = L.Reshape(net[conf_name], shape=dict(dim=[0, -1, num_classes]))
+softmax_name = "{}_softmax".format(conf_name)
+net[softmax_name] = L.Softmax(net[reshape_name], axis=2)
+flatten_name = "{}_flatten".format(conf_name)
+net[flatten_name] = L.Flatten(net[softmax_name], axis=1)
+mbox_layers_out[1] = net[flatten_name]
+
+net.detection_out = L.DetectionOutput(*mbox_layers_out,
+    detection_output_param=det_out_param,
+    include=dict(phase=caffe_pb2.Phase.Value('TEST')))
+net.detection_eval = L.DetectionEvaluate(net.detection_out, net.label,
+    detection_evaluate_param=det_eval_param,
+    include=dict(phase=caffe_pb2.Phase.Value('TEST')))
+
+with open(test_net_file, 'w') as f:
+    print('name: "{}_test"'.format(model_name), file=f)
+    print(net.to_proto(), file=f)
+shutil.copy(test_net_file, job_dir)
+
+# Create deploy net.
+# Remove the first and last layer from test net.
+deploy_net = net
+with open(deploy_net_file, 'w') as f:
+    net_param = deploy_net.to_proto()
+    # Remove the first (AnnotatedData) and last (DetectionEvaluate) layer from test net.
+    del net_param.layer[0]
+    del net_param.layer[-1]
+    net_param.name = '{}_deploy'.format(model_name)
+    net_param.input.extend(['data'])
+    net_param.input_shape.extend([
+        caffe_pb2.BlobShape(dim=[1, 3, resize_height, resize_width])])
+    print(net_param, file=f)
+shutil.copy(deploy_net_file, job_dir)
+
+# Create solver.
+solver = caffe_pb2.SolverParameter(
+        train_net=train_net_file,
+        # test_net=[test_net_file],
+        snapshot_prefix=snapshot_prefix,
+        **solver_param)
+
+with open(solver_file, 'w') as f:
+    print(solver, file=f)
+shutil.copy(solver_file, job_dir)
+
+max_iter = 0
+# Find most recent snapshot.
+for file in os.listdir(snapshot_dir):
+  if file.endswith(".solverstate"):
+    basename = os.path.splitext(file)[0]
+    iter = int(basename.split("{}_iter_".format(model_name))[1])
+    if iter > max_iter:
+      max_iter = iter
+
+train_src_param = '--weights="{}" \\\n'.format(pretrain_model)
+if resume_training:
+  if max_iter > 0:
+    train_src_param = '--snapshot="{}_iter_{}.solverstate" \\\n'.format(snapshot_prefix, max_iter)
+
+if remove_old_models:
+  # Remove any snapshots smaller than max_iter.
+  for file in os.listdir(snapshot_dir):
+    if file.endswith(".solverstate"):
+      basename = os.path.splitext(file)[0]
+      iter = int(basename.split("{}_iter_".format(model_name))[1])
+      if max_iter > iter:
+        os.remove("{}/{}".format(snapshot_dir, file))
+    if file.endswith(".caffemodel"):
+      basename = os.path.splitext(file)[0]
+      iter = int(basename.split("{}_iter_".format(model_name))[1])
+      if max_iter > iter:
+        os.remove("{}/{}".format(snapshot_dir, file))
+
+# Create job file.
+with open(job_file, 'w') as f:
+  f.write('cd {}\n'.format(caffe_root))
+  f.write('./build/tools/caffe train \\\n')
+  f.write('--solver="{}" \\\n'.format(solver_file))
+  f.write(train_src_param)
+  if solver_param['solver_mode'] == P.Solver.GPU:
+    f.write('--gpu {} 2>&1 | tee {}/{}.log\n'.format(gpus, job_dir, model_name))
+  else:
+    f.write('2>&1 | tee {}/{}.log\n'.format(job_dir, model_name))
+
+# Copy the python script to job_dir.
+py_file = os.path.abspath(__file__)
+shutil.copy(py_file, job_dir)
+
+# Run the job.
+os.chmod(job_file, stat.S_IRWXU)
+if run_soon:
+  subprocess.call(job_file, shell=True)
diff --git a/examples/refinedet/VGG16_COCO_320.py b/examples/refinedet/VGG16_COCO_320.py
new file mode 100644
index 0000000..08bd67b
--- /dev/null
+++ b/examples/refinedet/VGG16_COCO_320.py
@@ -0,0 +1,607 @@
+from __future__ import print_function
+import sys
+sys.path.append("./python")
+import caffe
+from caffe.model_libs import *
+from google.protobuf import text_format
+
+import math
+import os
+import shutil
+import stat
+import subprocess
+
+# Add extra layers on top of a "base" network (e.g. VGGNet or ResNet).
+def AddExtraLayers(net, use_batchnorm=True, arm_source_layers=[], normalizations=[], lr_mult=1):
+    use_relu = True
+
+    # Add additional convolutional layers.
+    # 320/32: 10 x 10
+    from_layer = net.keys()[-1]
+
+    # 320/64: 5 x 5
+    out_layer = "conv6_1"
+    ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 1, 0, 1, lr_mult=lr_mult)
+
+    from_layer = out_layer
+    out_layer = "conv6_2"
+    ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 512, 3, 1, 2, lr_mult=lr_mult)
+
+    arm_source_layers.reverse()
+    normalizations.reverse()
+    num_p = 6
+    for index, layer in enumerate(arm_source_layers):
+        out_layer = layer
+        if normalizations:
+            if normalizations[index] != -1:
+                norm_name = "{}_norm".format(layer)
+                net[norm_name] = L.Normalize(net[layer], scale_filler=dict(type="constant", value=normalizations[index]),
+                    across_spatial=False, channel_shared=False)
+                out_layer = norm_name
+                arm_source_layers[index] = norm_name
+        from_layer = out_layer
+        out_layer = "TL{}_{}".format(num_p, 1)
+        ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 3, 1, 1, lr_mult=lr_mult)
+
+        if num_p == 6:
+            from_layer = out_layer
+            out_layer = "TL{}_{}".format(num_p, 2)
+            ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 3, 1, 1, lr_mult=lr_mult)
+
+            from_layer = out_layer
+            out_layer = "P{}".format(num_p)
+            ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 3, 1, 1, lr_mult=lr_mult)
+        else:
+            from_layer = out_layer
+            out_layer = "TL{}_{}".format(num_p, 2)
+            ConvBNLayer(net, from_layer, out_layer, use_batchnorm, False, 256, 3, 1, 1, lr_mult=lr_mult)
+
+            from_layer = "P{}".format(num_p+1)
+            out_layer = "P{}-up".format(num_p+1)
+            DeconvBNLayer(net, from_layer, out_layer, use_batchnorm, False, 256, 2, 0, 2, lr_mult=lr_mult)
+
+            from_layer = ["TL{}_{}".format(num_p, 2), "P{}-up".format(num_p+1)]
+            out_layer = "Elt{}".format(num_p)
+            EltwiseLayer(net, from_layer, out_layer)
+            relu_name = '{}_relu'.format(out_layer)
+            net[relu_name] = L.ReLU(net[out_layer], in_place=True)
+            out_layer = relu_name
+
+            from_layer = out_layer
+            out_layer = "P{}".format(num_p)
+            ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 3, 1, 1, lr_mult=lr_mult)
+
+        num_p = num_p - 1
+
+    return net
+
+
+### Modify the following parameters accordingly ###
+# The directory which contains the caffe code.
+# We assume you are running the script at the CAFFE_ROOT.
+caffe_root = os.getcwd()
+
+# Set true if you want to start training right after generating all files.
+run_soon = True
+# Set true if you want to load from most recently saved snapshot.
+# Otherwise, we will load from the pretrain_model defined below.
+resume_training = True
+# If true, Remove old model files.
+remove_old_models = False
+
+# The database file for training data. Created by data/coco/create_data.sh
+train_data = "examples/coco/coco_train_lmdb"
+# The database file for testing data. Created by data/coco/create_data.sh
+test_data = "examples/coco/coco_minival_lmdb"
+# Specify the batch sampler.
+resize_width = 320
+resize_height = 320
+resize = "{}x{}".format(resize_width, resize_height)
+batch_sampler = [
+        {
+                'sampler': {
+                        },
+                'max_trials': 1,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.1,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.3,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.5,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.7,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.9,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'max_jaccard_overlap': 1.0,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        ]
+train_transform_param = {
+        'mirror': True,
+        'mean_value': [104, 117, 123],
+        'force_color': True,
+        'resize_param': {
+                'prob': 1,
+                'resize_mode': P.Resize.WARP,
+                'height': resize_height,
+                'width': resize_width,
+                'interp_mode': [
+                        P.Resize.LINEAR,
+                        P.Resize.AREA,
+                        P.Resize.NEAREST,
+                        P.Resize.CUBIC,
+                        P.Resize.LANCZOS4,
+                        ],
+                },
+        'distort_param': {
+                'brightness_prob': 0.5,
+                'brightness_delta': 32,
+                'contrast_prob': 0.5,
+                'contrast_lower': 0.5,
+                'contrast_upper': 1.5,
+                'hue_prob': 0.5,
+                'hue_delta': 18,
+                'saturation_prob': 0.5,
+                'saturation_lower': 0.5,
+                'saturation_upper': 1.5,
+                'random_order_prob': 0.0,
+                },
+        'expand_param': {
+                'prob': 0.5,
+                'max_expand_ratio': 4.0,
+                },
+        'emit_constraint': {
+            'emit_type': caffe_pb2.EmitConstraint.CENTER,
+            }
+        }
+test_transform_param = {
+        'mean_value': [104, 117, 123],
+        'force_color': True,
+        'resize_param': {
+                'prob': 1,
+                'resize_mode': P.Resize.WARP,
+                'height': resize_height,
+                'width': resize_width,
+                'interp_mode': [P.Resize.LINEAR],
+                },
+        }
+
+# If true, use batch norm for all newly added layers.
+# Currently only the non batch norm version has been tested.
+use_batchnorm = False
+lr_mult = 1
+# Use different initial learning rate.
+if use_batchnorm:
+    base_lr = 0.0004
+else:
+    # A learning rate for batch_size = 1, num_gpus = 1.
+    base_lr = 0.00004
+
+# Modify the job name if you want.
+job_name = "refinedet_vgg16_{}".format(resize)
+# The name of the model. Modify it if you want.
+model_name = "coco_{}".format(job_name)
+
+# Directory which stores the model .prototxt file.
+save_dir = "models/VGGNet/coco/{}".format(job_name)
+# Directory which stores the snapshot of models.
+snapshot_dir = "models/VGGNet/coco/{}".format(job_name)
+# Directory which stores the job script and log file.
+job_dir = "jobs/VGGNet/coco/{}".format(job_name)
+
+# model definition files.
+train_net_file = "{}/train.prototxt".format(save_dir)
+test_net_file = "{}/test.prototxt".format(save_dir)
+deploy_net_file = "{}/deploy.prototxt".format(save_dir)
+solver_file = "{}/solver.prototxt".format(save_dir)
+# snapshot prefix.
+snapshot_prefix = "{}/{}".format(snapshot_dir, model_name)
+# job script path.
+job_file = "{}/{}.sh".format(job_dir, model_name)
+
+# Stores the test image names and sizes. Created by data/coco/create_list.sh
+name_size_file = "data/coco/minival2014_name_size.txt"
+# The pretrained model. We use the Fully convolutional reduced (atrous) VGGNet.
+pretrain_model = "models/VGGNet/VGG_ILSVRC_16_layers_fc_reduced.caffemodel"
+# Stores LabelMapItem.
+label_map_file = "data/coco/labelmap_coco.prototxt"
+
+# MultiBoxLoss parameters.
+num_classes = 81
+share_location = True
+background_label_id = 0
+train_on_diff_gt = False
+normalization_mode = P.Loss.VALID
+code_type = P.PriorBox.CENTER_SIZE
+ignore_cross_boundary_bbox = False
+mining_type = P.MultiBoxLoss.MAX_NEGATIVE
+neg_pos_ratio = 3.
+loc_weight = (neg_pos_ratio + 1.) / 4.
+multibox_loss_param = {
+    'loc_loss_type': P.MultiBoxLoss.SMOOTH_L1,
+    'conf_loss_type': P.MultiBoxLoss.SOFTMAX,
+    'loc_weight': loc_weight,
+    'num_classes': num_classes,
+    'share_location': share_location,
+    'match_type': P.MultiBoxLoss.PER_PREDICTION,
+    'overlap_threshold': 0.5,
+    'use_prior_for_matching': True,
+    'background_label_id': background_label_id,
+    'use_difficult_gt': train_on_diff_gt,
+    'mining_type': mining_type,
+    'neg_pos_ratio': neg_pos_ratio,
+    'neg_overlap': 0.5,
+    'code_type': code_type,
+    'ignore_cross_boundary_bbox': ignore_cross_boundary_bbox,
+    'objectness_score': 0.01,
+    }
+loss_param = {
+    'normalization': normalization_mode,
+    }
+
+# parameters for generating priors.
+# minimum dimension of input image
+# min_dim = 320
+# conv4_3 ==> 40 x 40
+# conv5_3 ==> 20 x 20
+# fc7 ==> 10 x 10
+# conv6_2 ==> 5 x 5
+arm_source_layers = ['conv4_3', 'conv5_3', 'fc7', 'conv6_2']
+odm_source_layers = ['P3', 'P4', 'P5', 'P6']
+min_sizes = [32, 64, 128, 256]
+max_sizes = [[], [], [], []]
+steps = [8, 16, 32, 64]
+aspect_ratios = [[2], [2], [2], [2]]
+# L2 normalize conv4_3 and conv5_3.
+normalizations = [10, 8, -1, -1]
+# variance used to encode/decode prior bboxes.
+if code_type == P.PriorBox.CENTER_SIZE:
+  prior_variance = [0.1, 0.1, 0.2, 0.2]
+else:
+  prior_variance = [0.1]
+flip = True
+clip = False
+
+# Solver parameters.
+# Defining which GPUs to use.
+gpus = "0,1,2,3"
+gpulist = gpus.split(",")
+num_gpus = len(gpulist)
+
+# Divide the mini-batch to different GPUs.
+batch_size = 32
+accum_batch_size = 32
+iter_size = accum_batch_size / batch_size
+solver_mode = P.Solver.CPU
+device_id = 0
+batch_size_per_device = batch_size
+if num_gpus > 0:
+  batch_size_per_device = int(math.ceil(float(batch_size) / num_gpus))
+  iter_size = int(math.ceil(float(accum_batch_size) / (batch_size_per_device * num_gpus)))
+  solver_mode = P.Solver.GPU
+  device_id = int(gpulist[0])
+
+if normalization_mode == P.Loss.NONE:
+  base_lr /= batch_size_per_device
+elif normalization_mode == P.Loss.VALID:
+  base_lr *= 25. / loc_weight
+elif normalization_mode == P.Loss.FULL:
+  # Roughly there are 2000 prior bboxes per image.
+  # TODO(weiliu89): Estimate the exact # of priors.
+  base_lr *= 2000.
+
+# Evaluate on whole test set.
+num_test_image = 5000
+test_batch_size = 1
+test_iter = num_test_image / test_batch_size
+
+solver_param = {
+    # Train parameters
+    'base_lr': base_lr,
+    'weight_decay': 0.0005,
+    'lr_policy': "multistep",
+    'stepvalue': [280000, 360000, 400000],
+    'gamma': 0.1,
+    'momentum': 0.9,
+    'iter_size': iter_size,
+    'max_iter': 400000,
+    'snapshot': 10000,
+    'display': 10,
+    'average_loss': 10,
+    'type': "SGD",
+    'solver_mode': solver_mode,
+    'device_id': device_id,
+    'debug_info': False,
+    'snapshot_after_train': True,
+    # Test parameters
+    # 'test_iter': [test_iter],
+    # 'test_interval': 10000,
+    # 'eval_type': "detection",
+    # 'ap_version': "11point",
+    # 'test_initialization': False,
+    }
+
+# parameters for generating detection output.
+det_out_param = {
+    'num_classes': num_classes,
+    'share_location': share_location,
+    'background_label_id': background_label_id,
+    'nms_param': {'nms_threshold': 0.45, 'top_k': 1000},
+    'keep_top_k': 500,
+    'confidence_threshold': 0.01,
+    'code_type': code_type,
+    'objectness_score': 0.01,
+    }
+
+# parameters for evaluating detection results.
+det_eval_param = {
+    'num_classes': num_classes,
+    'background_label_id': background_label_id,
+    'overlap_threshold': 0.5,
+    'evaluate_difficult_gt': False,
+    'name_size_file': name_size_file,
+    }
+
+### Hopefully you don't need to change the following ###
+# Check file.
+check_if_exist(train_data)
+check_if_exist(test_data)
+check_if_exist(label_map_file)
+check_if_exist(pretrain_model)
+make_if_not_exist(save_dir)
+make_if_not_exist(job_dir)
+make_if_not_exist(snapshot_dir)
+
+# Create train net.
+net = caffe.NetSpec()
+net.data, net.label = CreateAnnotatedDataLayer(train_data, batch_size=batch_size_per_device,
+        train=True, output_label=True, label_map_file=label_map_file,
+        transform_param=train_transform_param, batch_sampler=batch_sampler)
+
+VGGNetBody(net, from_layer='data', fully_conv=True, reduced=True, dilated=False, dropout=False)
+
+AddExtraLayers(net, use_batchnorm, arm_source_layers, normalizations, lr_mult=lr_mult)
+arm_source_layers.reverse()
+normalizations.reverse()
+
+mbox_layers = CreateRefineDetHead(net, data_layer='data', from_layers=arm_source_layers,
+        use_batchnorm=use_batchnorm, min_sizes=min_sizes, max_sizes=max_sizes,
+        aspect_ratios=aspect_ratios, steps=steps, normalizations=[],
+        num_classes=num_classes, share_location=share_location, flip=flip, clip=clip,
+        prior_variance=prior_variance, kernel_size=3, pad=1, lr_mult=lr_mult, from_layers2=odm_source_layers)
+
+name = "arm_loss"
+mbox_layers_arm = []
+mbox_layers_arm.append(mbox_layers[0])
+mbox_layers_arm.append(mbox_layers[1])
+mbox_layers_arm.append(mbox_layers[2])
+mbox_layers_arm.append(net.label)
+multibox_loss_param_arm = multibox_loss_param.copy()
+multibox_loss_param_arm['num_classes'] = 2
+net[name] = L.MultiBoxLoss(*mbox_layers_arm, multibox_loss_param=multibox_loss_param_arm,
+        loss_param=loss_param, include=dict(phase=caffe_pb2.Phase.Value('TRAIN')),
+        propagate_down=[True, True, False, False])
+
+# Create the MultiBoxLossLayer.
+conf_name = "arm_conf"
+reshape_name = "{}_reshape".format(conf_name)
+net[reshape_name] = L.Reshape(net[conf_name], shape=dict(dim=[0, -1, 2]))
+softmax_name = "{}_softmax".format(conf_name)
+net[softmax_name] = L.Softmax(net[reshape_name], axis=2)
+flatten_name = "{}_flatten".format(conf_name)
+net[flatten_name] = L.Flatten(net[softmax_name], axis=1)
+
+name = "odm_loss"
+mbox_layers_odm = []
+mbox_layers_odm.append(mbox_layers[3])
+mbox_layers_odm.append(mbox_layers[4])
+mbox_layers_odm.append(mbox_layers[2])
+mbox_layers_odm.append(net.label)
+mbox_layers_odm.append(net[flatten_name])
+mbox_layers_odm.append(mbox_layers[0])
+net[name] = L.MultiBoxLoss(*mbox_layers_odm, multibox_loss_param=multibox_loss_param,
+        loss_param=loss_param, include=dict(phase=caffe_pb2.Phase.Value('TRAIN')),
+        propagate_down=[True, True, False, False, False, False])
+
+
+with open(train_net_file, 'w') as f:
+    print('name: "{}_train"'.format(model_name), file=f)
+    print(net.to_proto(), file=f)
+shutil.copy(train_net_file, job_dir)
+
+# Create test net.
+net = caffe.NetSpec()
+net.data, net.label = CreateAnnotatedDataLayer(test_data, batch_size=test_batch_size,
+        train=False, output_label=True, label_map_file=label_map_file,
+        transform_param=test_transform_param)
+
+VGGNetBody(net, from_layer='data', fully_conv=True, reduced=True, dilated=False, dropout=False)
+
+arm_source_layers = ['conv4_3', 'conv5_3', 'fc7', 'conv6_2']
+AddExtraLayers(net, use_batchnorm, arm_source_layers, normalizations, lr_mult=lr_mult)
+arm_source_layers.reverse()
+normalizations.reverse()
+
+mbox_layers = CreateRefineDetHead(net, data_layer='data', from_layers=arm_source_layers,
+        use_batchnorm=use_batchnorm, min_sizes=min_sizes, max_sizes=max_sizes,
+        aspect_ratios=aspect_ratios, steps=steps, normalizations=[],
+        num_classes=num_classes, share_location=share_location, flip=flip, clip=clip,
+        prior_variance=prior_variance, kernel_size=3, pad=1, lr_mult=lr_mult, from_layers2=odm_source_layers)
+
+mbox_layers_out = []
+mbox_layers_out.append(mbox_layers[3])
+mbox_layers_out.append(mbox_layers[4])
+mbox_layers_out.append(mbox_layers[2])
+mbox_layers_out.append(mbox_layers[1])
+mbox_layers_out.append(mbox_layers[0])
+
+conf_name = "arm_conf"
+reshape_name = "{}_reshape".format(conf_name)
+net[reshape_name] = L.Reshape(net[conf_name], shape=dict(dim=[0, -1, 2]))
+softmax_name = "{}_softmax".format(conf_name)
+net[softmax_name] = L.Softmax(net[reshape_name], axis=2)
+flatten_name = "{}_flatten".format(conf_name)
+net[flatten_name] = L.Flatten(net[softmax_name], axis=1)
+mbox_layers_out[3] = net[flatten_name]
+
+conf_name = "odm_conf"
+reshape_name = "{}_reshape".format(conf_name)
+net[reshape_name] = L.Reshape(net[conf_name], shape=dict(dim=[0, -1, num_classes]))
+softmax_name = "{}_softmax".format(conf_name)
+net[softmax_name] = L.Softmax(net[reshape_name], axis=2)
+flatten_name = "{}_flatten".format(conf_name)
+net[flatten_name] = L.Flatten(net[softmax_name], axis=1)
+mbox_layers_out[1] = net[flatten_name]
+
+net.detection_out = L.DetectionOutput(*mbox_layers_out,
+    detection_output_param=det_out_param,
+    include=dict(phase=caffe_pb2.Phase.Value('TEST')))
+net.detection_eval = L.DetectionEvaluate(net.detection_out, net.label,
+    detection_evaluate_param=det_eval_param,
+    include=dict(phase=caffe_pb2.Phase.Value('TEST')))
+
+with open(test_net_file, 'w') as f:
+    print('name: "{}_test"'.format(model_name), file=f)
+    print(net.to_proto(), file=f)
+shutil.copy(test_net_file, job_dir)
+
+# Create deploy net.
+# Remove the first and last layer from test net.
+deploy_net = net
+with open(deploy_net_file, 'w') as f:
+    net_param = deploy_net.to_proto()
+    # Remove the first (AnnotatedData) and last (DetectionEvaluate) layer from test net.
+    del net_param.layer[0]
+    del net_param.layer[-1]
+    net_param.name = '{}_deploy'.format(model_name)
+    net_param.input.extend(['data'])
+    net_param.input_shape.extend([
+        caffe_pb2.BlobShape(dim=[1, 3, resize_height, resize_width])])
+    print(net_param, file=f)
+shutil.copy(deploy_net_file, job_dir)
+
+# Create solver.
+solver = caffe_pb2.SolverParameter(
+        train_net=train_net_file,
+        # test_net=[test_net_file],
+        snapshot_prefix=snapshot_prefix,
+        **solver_param)
+
+with open(solver_file, 'w') as f:
+    print(solver, file=f)
+shutil.copy(solver_file, job_dir)
+
+max_iter = 0
+# Find most recent snapshot.
+for file in os.listdir(snapshot_dir):
+  if file.endswith(".solverstate"):
+    basename = os.path.splitext(file)[0]
+    iter = int(basename.split("{}_iter_".format(model_name))[1])
+    if iter > max_iter:
+      max_iter = iter
+
+train_src_param = '--weights="{}" \\\n'.format(pretrain_model)
+if resume_training:
+  if max_iter > 0:
+    train_src_param = '--snapshot="{}_iter_{}.solverstate" \\\n'.format(snapshot_prefix, max_iter)
+
+if remove_old_models:
+  # Remove any snapshots smaller than max_iter.
+  for file in os.listdir(snapshot_dir):
+    if file.endswith(".solverstate"):
+      basename = os.path.splitext(file)[0]
+      iter = int(basename.split("{}_iter_".format(model_name))[1])
+      if max_iter > iter:
+        os.remove("{}/{}".format(snapshot_dir, file))
+    if file.endswith(".caffemodel"):
+      basename = os.path.splitext(file)[0]
+      iter = int(basename.split("{}_iter_".format(model_name))[1])
+      if max_iter > iter:
+        os.remove("{}/{}".format(snapshot_dir, file))
+
+# Create job file.
+with open(job_file, 'w') as f:
+  f.write('cd {}\n'.format(caffe_root))
+  f.write('./build/tools/caffe train \\\n')
+  f.write('--solver="{}" \\\n'.format(solver_file))
+  f.write(train_src_param)
+  if solver_param['solver_mode'] == P.Solver.GPU:
+    f.write('--gpu {} 2>&1 | tee {}/{}.log\n'.format(gpus, job_dir, model_name))
+  else:
+    f.write('2>&1 | tee {}/{}.log\n'.format(job_dir, model_name))
+
+# Copy the python script to job_dir.
+py_file = os.path.abspath(__file__)
+shutil.copy(py_file, job_dir)
+
+# Run the job.
+os.chmod(job_file, stat.S_IRWXU)
+if run_soon:
+  subprocess.call(job_file, shell=True)
diff --git a/examples/refinedet/VGG16_COCO_512.py b/examples/refinedet/VGG16_COCO_512.py
new file mode 100644
index 0000000..1f7452c
--- /dev/null
+++ b/examples/refinedet/VGG16_COCO_512.py
@@ -0,0 +1,608 @@
+from __future__ import print_function
+import sys
+sys.path.append("./python")
+import caffe
+from caffe.model_libs import *
+from google.protobuf import text_format
+
+import math
+import os
+import shutil
+import stat
+import subprocess
+
+# Add extra layers on top of a "base" network (e.g. VGGNet or ResNet).
+def AddExtraLayers(net, use_batchnorm=True, arm_source_layers=[], normalizations=[], lr_mult=1):
+    use_relu = True
+
+    # Add additional convolutional layers.
+    # 512/32: 16 x 16
+    from_layer = net.keys()[-1]
+
+    # 512/64: 8 x 8
+    out_layer = "conv6_1"
+    ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 1, 0, 1, lr_mult=lr_mult)
+
+    from_layer = out_layer
+    out_layer = "conv6_2"
+    ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 512, 3, 1, 2, lr_mult=lr_mult)
+
+    arm_source_layers.reverse()
+    normalizations.reverse()
+    num_p = 6
+    for index, layer in enumerate(arm_source_layers):
+        out_layer = layer
+        if normalizations:
+            if normalizations[index] != -1:
+                norm_name = "{}_norm".format(layer)
+                net[norm_name] = L.Normalize(net[layer], scale_filler=dict(type="constant", value=normalizations[index]),
+                    across_spatial=False, channel_shared=False)
+                out_layer = norm_name
+                arm_source_layers[index] = norm_name
+        from_layer = out_layer
+        out_layer = "TL{}_{}".format(num_p, 1)
+        ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 3, 1, 1, lr_mult=lr_mult)
+
+        if num_p == 6:
+            from_layer = out_layer
+            out_layer = "TL{}_{}".format(num_p, 2)
+            ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 3, 1, 1, lr_mult=lr_mult)
+
+            from_layer = out_layer
+            out_layer = "P{}".format(num_p)
+            ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 3, 1, 1, lr_mult=lr_mult)
+        else:
+            from_layer = out_layer
+            out_layer = "TL{}_{}".format(num_p, 2)
+            ConvBNLayer(net, from_layer, out_layer, use_batchnorm, False, 256, 3, 1, 1,
+                        lr_mult=lr_mult)
+
+            from_layer = "P{}".format(num_p+1)
+            out_layer = "P{}-up".format(num_p+1)
+            DeconvBNLayer(net, from_layer, out_layer, use_batchnorm, False, 256, 2, 0, 2, lr_mult=lr_mult)
+
+            from_layer = ["TL{}_{}".format(num_p, 2), "P{}-up".format(num_p+1)]
+            out_layer = "Elt{}".format(num_p)
+            EltwiseLayer(net, from_layer, out_layer)
+            relu_name = '{}_relu'.format(out_layer)
+            net[relu_name] = L.ReLU(net[out_layer], in_place=True)
+            out_layer = relu_name
+
+            from_layer = out_layer
+            out_layer = "P{}".format(num_p)
+            ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 3, 1, 1, lr_mult=lr_mult)
+
+        num_p = num_p - 1
+
+    return net
+
+
+### Modify the following parameters accordingly ###
+# The directory which contains the caffe code.
+# We assume you are running the script at the CAFFE_ROOT.
+caffe_root = os.getcwd()
+
+# Set true if you want to start training right after generating all files.
+run_soon = True
+# Set true if you want to load from most recently saved snapshot.
+# Otherwise, we will load from the pretrain_model defined below.
+resume_training = True
+# If true, Remove old model files.
+remove_old_models = False
+
+# The database file for training data. Created by data/coco/create_data.sh
+train_data = "examples/coco/coco_train_lmdb"
+# The database file for testing data. Created by data/coco/create_data.sh
+test_data = "examples/coco/coco_minival_lmdb"
+# Specify the batch sampler.
+resize_width = 512
+resize_height = 512
+resize = "{}x{}".format(resize_width, resize_height)
+batch_sampler = [
+        {
+                'sampler': {
+                        },
+                'max_trials': 1,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.1,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.3,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.5,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.7,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.9,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'max_jaccard_overlap': 1.0,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        ]
+train_transform_param = {
+        'mirror': True,
+        'mean_value': [104, 117, 123],
+        'force_color': True,
+        'resize_param': {
+                'prob': 1,
+                'resize_mode': P.Resize.WARP,
+                'height': resize_height,
+                'width': resize_width,
+                'interp_mode': [
+                        P.Resize.LINEAR,
+                        P.Resize.AREA,
+                        P.Resize.NEAREST,
+                        P.Resize.CUBIC,
+                        P.Resize.LANCZOS4,
+                        ],
+                },
+        'distort_param': {
+                'brightness_prob': 0.5,
+                'brightness_delta': 32,
+                'contrast_prob': 0.5,
+                'contrast_lower': 0.5,
+                'contrast_upper': 1.5,
+                'hue_prob': 0.5,
+                'hue_delta': 18,
+                'saturation_prob': 0.5,
+                'saturation_lower': 0.5,
+                'saturation_upper': 1.5,
+                'random_order_prob': 0.0,
+                },
+        'expand_param': {
+                'prob': 0.5,
+                'max_expand_ratio': 4.0,
+                },
+        'emit_constraint': {
+            'emit_type': caffe_pb2.EmitConstraint.CENTER,
+            }
+        }
+test_transform_param = {
+        'mean_value': [104, 117, 123],
+        'force_color': True,
+        'resize_param': {
+                'prob': 1,
+                'resize_mode': P.Resize.WARP,
+                'height': resize_height,
+                'width': resize_width,
+                'interp_mode': [P.Resize.LINEAR],
+                },
+        }
+
+# If true, use batch norm for all newly added layers.
+# Currently only the non batch norm version has been tested.
+use_batchnorm = False
+lr_mult = 1
+# Use different initial learning rate.
+if use_batchnorm:
+    base_lr = 0.0004
+else:
+    # A learning rate for batch_size = 1, num_gpus = 1.
+    base_lr = 0.00004
+
+# Modify the job name if you want.
+job_name = "refinedet_vgg16_{}".format(resize)
+# The name of the model. Modify it if you want.
+model_name = "coco_{}".format(job_name)
+
+# Directory which stores the model .prototxt file.
+save_dir = "models/VGGNet/coco/{}".format(job_name)
+# Directory which stores the snapshot of models.
+snapshot_dir = "models/VGGNet/coco/{}".format(job_name)
+# Directory which stores the job script and log file.
+job_dir = "jobs/VGGNet/coco/{}".format(job_name)
+
+# model definition files.
+train_net_file = "{}/train.prototxt".format(save_dir)
+test_net_file = "{}/test.prototxt".format(save_dir)
+deploy_net_file = "{}/deploy.prototxt".format(save_dir)
+solver_file = "{}/solver.prototxt".format(save_dir)
+# snapshot prefix.
+snapshot_prefix = "{}/{}".format(snapshot_dir, model_name)
+# job script path.
+job_file = "{}/{}.sh".format(job_dir, model_name)
+
+# Stores the test image names and sizes. Created by data/coco/create_list.sh
+name_size_file = "data/coco/minival2014_name_size.txt"
+# The pretrained model. We use the Fully convolutional reduced (atrous) VGGNet.
+pretrain_model = "models/VGGNet/VGG_ILSVRC_16_layers_fc_reduced.caffemodel"
+# Stores LabelMapItem.
+label_map_file = "data/coco/labelmap_coco.prototxt"
+
+# MultiBoxLoss parameters.
+num_classes = 81
+share_location = True
+background_label_id = 0
+train_on_diff_gt = False
+normalization_mode = P.Loss.VALID
+code_type = P.PriorBox.CENTER_SIZE
+ignore_cross_boundary_bbox = False
+mining_type = P.MultiBoxLoss.MAX_NEGATIVE
+neg_pos_ratio = 3.
+loc_weight = (neg_pos_ratio + 1.) / 4.
+multibox_loss_param = {
+    'loc_loss_type': P.MultiBoxLoss.SMOOTH_L1,
+    'conf_loss_type': P.MultiBoxLoss.SOFTMAX,
+    'loc_weight': loc_weight,
+    'num_classes': num_classes,
+    'share_location': share_location,
+    'match_type': P.MultiBoxLoss.PER_PREDICTION,
+    'overlap_threshold': 0.5,
+    'use_prior_for_matching': True,
+    'background_label_id': background_label_id,
+    'use_difficult_gt': train_on_diff_gt,
+    'mining_type': mining_type,
+    'neg_pos_ratio': neg_pos_ratio,
+    'neg_overlap': 0.5,
+    'code_type': code_type,
+    'ignore_cross_boundary_bbox': ignore_cross_boundary_bbox,
+    'objectness_score': 0.01,
+    }
+loss_param = {
+    'normalization': normalization_mode,
+    }
+
+# parameters for generating priors.
+# minimum dimension of input image
+# min_dim = 512
+# conv4_3 ==> 64 x 64
+# conv5_3 ==> 32 x 32
+# fc7 ==> 16 x 16
+# conv6_2 ==> 8 x 8
+arm_source_layers = ['conv4_3', 'conv5_3', 'fc7', 'conv6_2']
+odm_source_layers = ['P3', 'P4', 'P5', 'P6']
+min_sizes = [32, 64, 128, 256]
+max_sizes = [[], [], [], []]
+steps = [8, 16, 32, 64]
+aspect_ratios = [[2], [2], [2], [2]]
+# L2 normalize conv4_3 and conv5_3.
+normalizations = [10, 8, -1, -1]
+# variance used to encode/decode prior bboxes.
+if code_type == P.PriorBox.CENTER_SIZE:
+  prior_variance = [0.1, 0.1, 0.2, 0.2]
+else:
+  prior_variance = [0.1]
+flip = True
+clip = False
+
+# Solver parameters.
+# Defining which GPUs to use.
+gpus = "0,1,2,3"
+gpulist = gpus.split(",")
+num_gpus = len(gpulist)
+
+# Divide the mini-batch to different GPUs.
+batch_size = 32
+accum_batch_size = 32
+iter_size = accum_batch_size / batch_size
+solver_mode = P.Solver.CPU
+device_id = 0
+batch_size_per_device = batch_size
+if num_gpus > 0:
+  batch_size_per_device = int(math.ceil(float(batch_size) / num_gpus))
+  iter_size = int(math.ceil(float(accum_batch_size) / (batch_size_per_device * num_gpus)))
+  solver_mode = P.Solver.GPU
+  device_id = int(gpulist[0])
+
+if normalization_mode == P.Loss.NONE:
+  base_lr /= batch_size_per_device
+elif normalization_mode == P.Loss.VALID:
+  base_lr *= 25. / loc_weight
+elif normalization_mode == P.Loss.FULL:
+  # Roughly there are 2000 prior bboxes per image.
+  # TODO(weiliu89): Estimate the exact # of priors.
+  base_lr *= 2000.
+
+# Evaluate on whole test set.
+num_test_image = 5000
+test_batch_size = 1
+test_iter = num_test_image / test_batch_size
+
+solver_param = {
+    # Train parameters
+    'base_lr': base_lr,
+    'weight_decay': 0.0005,
+    'lr_policy': "multistep",
+    'stepvalue': [280000, 360000, 400000],
+    'gamma': 0.1,
+    'momentum': 0.9,
+    'iter_size': iter_size,
+    'max_iter': 400000,
+    'snapshot': 10000,
+    'display': 10,
+    'average_loss': 10,
+    'type': "SGD",
+    'solver_mode': solver_mode,
+    'device_id': device_id,
+    'debug_info': False,
+    'snapshot_after_train': True,
+    # Test parameters
+    # 'test_iter': [test_iter],
+    # 'test_interval': 10000,
+    # 'eval_type': "detection",
+    # 'ap_version': "11point",
+    # 'test_initialization': False,
+    }
+
+# parameters for generating detection output.
+det_out_param = {
+    'num_classes': num_classes,
+    'share_location': share_location,
+    'background_label_id': background_label_id,
+    'nms_param': {'nms_threshold': 0.45, 'top_k': 1000},
+    'keep_top_k': 500,
+    'confidence_threshold': 0.01,
+    'code_type': code_type,
+    'objectness_score': 0.01,
+    }
+
+# parameters for evaluating detection results.
+det_eval_param = {
+    'num_classes': num_classes,
+    'background_label_id': background_label_id,
+    'overlap_threshold': 0.5,
+    'evaluate_difficult_gt': False,
+    'name_size_file': name_size_file,
+    }
+
+### Hopefully you don't need to change the following ###
+# Check file.
+check_if_exist(train_data)
+check_if_exist(test_data)
+check_if_exist(label_map_file)
+check_if_exist(pretrain_model)
+make_if_not_exist(save_dir)
+make_if_not_exist(job_dir)
+make_if_not_exist(snapshot_dir)
+
+# Create train net.
+net = caffe.NetSpec()
+net.data, net.label = CreateAnnotatedDataLayer(train_data, batch_size=batch_size_per_device,
+        train=True, output_label=True, label_map_file=label_map_file,
+        transform_param=train_transform_param, batch_sampler=batch_sampler)
+
+VGGNetBody(net, from_layer='data', fully_conv=True, reduced=True, dilated=False, dropout=False)
+
+AddExtraLayers(net, use_batchnorm, arm_source_layers, normalizations, lr_mult=lr_mult)
+arm_source_layers.reverse()
+normalizations.reverse()
+
+mbox_layers = CreateRefineDetHead(net, data_layer='data', from_layers=arm_source_layers,
+        use_batchnorm=use_batchnorm, min_sizes=min_sizes, max_sizes=max_sizes,
+        aspect_ratios=aspect_ratios, steps=steps, normalizations=[],
+        num_classes=num_classes, share_location=share_location, flip=flip, clip=clip,
+        prior_variance=prior_variance, kernel_size=3, pad=1, lr_mult=lr_mult, from_layers2=odm_source_layers)
+
+name = "arm_loss"
+mbox_layers_arm = []
+mbox_layers_arm.append(mbox_layers[0])
+mbox_layers_arm.append(mbox_layers[1])
+mbox_layers_arm.append(mbox_layers[2])
+mbox_layers_arm.append(net.label)
+multibox_loss_param_arm = multibox_loss_param.copy()
+multibox_loss_param_arm['num_classes'] = 2
+net[name] = L.MultiBoxLoss(*mbox_layers_arm, multibox_loss_param=multibox_loss_param_arm,
+        loss_param=loss_param, include=dict(phase=caffe_pb2.Phase.Value('TRAIN')),
+        propagate_down=[True, True, False, False])
+
+# Create the MultiBoxLossLayer.
+conf_name = "arm_conf"
+reshape_name = "{}_reshape".format(conf_name)
+net[reshape_name] = L.Reshape(net[conf_name], shape=dict(dim=[0, -1, 2]))
+softmax_name = "{}_softmax".format(conf_name)
+net[softmax_name] = L.Softmax(net[reshape_name], axis=2)
+flatten_name = "{}_flatten".format(conf_name)
+net[flatten_name] = L.Flatten(net[softmax_name], axis=1)
+
+name = "odm_loss"
+mbox_layers_odm = []
+mbox_layers_odm.append(mbox_layers[3])
+mbox_layers_odm.append(mbox_layers[4])
+mbox_layers_odm.append(mbox_layers[2])
+mbox_layers_odm.append(net.label)
+mbox_layers_odm.append(net[flatten_name])
+mbox_layers_odm.append(mbox_layers[0])
+net[name] = L.MultiBoxLoss(*mbox_layers_odm, multibox_loss_param=multibox_loss_param,
+        loss_param=loss_param, include=dict(phase=caffe_pb2.Phase.Value('TRAIN')),
+        propagate_down=[True, True, False, False, False, False])
+
+
+with open(train_net_file, 'w') as f:
+    print('name: "{}_train"'.format(model_name), file=f)
+    print(net.to_proto(), file=f)
+shutil.copy(train_net_file, job_dir)
+
+# Create test net.
+net = caffe.NetSpec()
+net.data, net.label = CreateAnnotatedDataLayer(test_data, batch_size=test_batch_size,
+        train=False, output_label=True, label_map_file=label_map_file,
+        transform_param=test_transform_param)
+
+VGGNetBody(net, from_layer='data', fully_conv=True, reduced=True, dilated=False, dropout=False)
+
+arm_source_layers = ['conv4_3', 'conv5_3', 'fc7', 'conv6_2']
+AddExtraLayers(net, use_batchnorm, arm_source_layers, normalizations, lr_mult=lr_mult)
+arm_source_layers.reverse()
+normalizations.reverse()
+
+mbox_layers = CreateRefineDetHead(net, data_layer='data', from_layers=arm_source_layers,
+        use_batchnorm=use_batchnorm, min_sizes=min_sizes, max_sizes=max_sizes,
+        aspect_ratios=aspect_ratios, steps=steps, normalizations=[],
+        num_classes=num_classes, share_location=share_location, flip=flip, clip=clip,
+        prior_variance=prior_variance, kernel_size=3, pad=1, lr_mult=lr_mult, from_layers2=odm_source_layers)
+
+mbox_layers_out = []
+mbox_layers_out.append(mbox_layers[3])
+mbox_layers_out.append(mbox_layers[4])
+mbox_layers_out.append(mbox_layers[2])
+mbox_layers_out.append(mbox_layers[1])
+mbox_layers_out.append(mbox_layers[0])
+
+conf_name = "arm_conf"
+reshape_name = "{}_reshape".format(conf_name)
+net[reshape_name] = L.Reshape(net[conf_name], shape=dict(dim=[0, -1, 2]))
+softmax_name = "{}_softmax".format(conf_name)
+net[softmax_name] = L.Softmax(net[reshape_name], axis=2)
+flatten_name = "{}_flatten".format(conf_name)
+net[flatten_name] = L.Flatten(net[softmax_name], axis=1)
+mbox_layers_out[3] = net[flatten_name]
+
+conf_name = "odm_conf"
+reshape_name = "{}_reshape".format(conf_name)
+net[reshape_name] = L.Reshape(net[conf_name], shape=dict(dim=[0, -1, num_classes]))
+softmax_name = "{}_softmax".format(conf_name)
+net[softmax_name] = L.Softmax(net[reshape_name], axis=2)
+flatten_name = "{}_flatten".format(conf_name)
+net[flatten_name] = L.Flatten(net[softmax_name], axis=1)
+mbox_layers_out[1] = net[flatten_name]
+
+net.detection_out = L.DetectionOutput(*mbox_layers_out,
+    detection_output_param=det_out_param,
+    include=dict(phase=caffe_pb2.Phase.Value('TEST')))
+net.detection_eval = L.DetectionEvaluate(net.detection_out, net.label,
+    detection_evaluate_param=det_eval_param,
+    include=dict(phase=caffe_pb2.Phase.Value('TEST')))
+
+with open(test_net_file, 'w') as f:
+    print('name: "{}_test"'.format(model_name), file=f)
+    print(net.to_proto(), file=f)
+shutil.copy(test_net_file, job_dir)
+
+# Create deploy net.
+# Remove the first and last layer from test net.
+deploy_net = net
+with open(deploy_net_file, 'w') as f:
+    net_param = deploy_net.to_proto()
+    # Remove the first (AnnotatedData) and last (DetectionEvaluate) layer from test net.
+    del net_param.layer[0]
+    del net_param.layer[-1]
+    net_param.name = '{}_deploy'.format(model_name)
+    net_param.input.extend(['data'])
+    net_param.input_shape.extend([
+        caffe_pb2.BlobShape(dim=[1, 3, resize_height, resize_width])])
+    print(net_param, file=f)
+shutil.copy(deploy_net_file, job_dir)
+
+# Create solver.
+solver = caffe_pb2.SolverParameter(
+        train_net=train_net_file,
+        # test_net=[test_net_file],
+        snapshot_prefix=snapshot_prefix,
+        **solver_param)
+
+with open(solver_file, 'w') as f:
+    print(solver, file=f)
+shutil.copy(solver_file, job_dir)
+
+max_iter = 0
+# Find most recent snapshot.
+for file in os.listdir(snapshot_dir):
+  if file.endswith(".solverstate"):
+    basename = os.path.splitext(file)[0]
+    iter = int(basename.split("{}_iter_".format(model_name))[1])
+    if iter > max_iter:
+      max_iter = iter
+
+train_src_param = '--weights="{}" \\\n'.format(pretrain_model)
+if resume_training:
+  if max_iter > 0:
+    train_src_param = '--snapshot="{}_iter_{}.solverstate" \\\n'.format(snapshot_prefix, max_iter)
+
+if remove_old_models:
+  # Remove any snapshots smaller than max_iter.
+  for file in os.listdir(snapshot_dir):
+    if file.endswith(".solverstate"):
+      basename = os.path.splitext(file)[0]
+      iter = int(basename.split("{}_iter_".format(model_name))[1])
+      if max_iter > iter:
+        os.remove("{}/{}".format(snapshot_dir, file))
+    if file.endswith(".caffemodel"):
+      basename = os.path.splitext(file)[0]
+      iter = int(basename.split("{}_iter_".format(model_name))[1])
+      if max_iter > iter:
+        os.remove("{}/{}".format(snapshot_dir, file))
+
+# Create job file.
+with open(job_file, 'w') as f:
+  f.write('cd {}\n'.format(caffe_root))
+  f.write('./build/tools/caffe train \\\n')
+  f.write('--solver="{}" \\\n'.format(solver_file))
+  f.write(train_src_param)
+  if solver_param['solver_mode'] == P.Solver.GPU:
+    f.write('--gpu {} 2>&1 | tee {}/{}.log\n'.format(gpus, job_dir, model_name))
+  else:
+    f.write('2>&1 | tee {}/{}.log\n'.format(job_dir, model_name))
+
+# Copy the python script to job_dir.
+py_file = os.path.abspath(__file__)
+shutil.copy(py_file, job_dir)
+
+# Run the job.
+os.chmod(job_file, stat.S_IRWXU)
+if run_soon:
+  subprocess.call(job_file, shell=True)
diff --git a/examples/refinedet/VGG16_VOC2007_320.py b/examples/refinedet/VGG16_VOC2007_320.py
new file mode 100644
index 0000000..3d632c2
--- /dev/null
+++ b/examples/refinedet/VGG16_VOC2007_320.py
@@ -0,0 +1,605 @@
+from __future__ import print_function
+import sys
+sys.path.append("./python")
+import caffe
+from caffe.model_libs import *
+from google.protobuf import text_format
+
+import math
+import os
+import shutil
+import stat
+import subprocess
+
+# Add extra layers on top of a "base" network (e.g. VGGNet or ResNet).
+def AddExtraLayers(net, use_batchnorm=True, arm_source_layers=[], normalizations=[], lr_mult=1):
+    use_relu = True
+
+    # Add additional convolutional layers.
+    # 320/32: 10 x 10
+    from_layer = net.keys()[-1]
+
+    # 320/64: 5 x 5
+    out_layer = "conv6_1"
+    ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 1, 0, 1, lr_mult=lr_mult)
+
+    from_layer = out_layer
+    out_layer = "conv6_2"
+    ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 512, 3, 1, 2, lr_mult=lr_mult)
+
+    arm_source_layers.reverse()
+    normalizations.reverse()
+    num_p = 6
+    for index, layer in enumerate(arm_source_layers):
+        out_layer = layer
+        if normalizations:
+            if normalizations[index] != -1:
+                norm_name = "{}_norm".format(layer)
+                net[norm_name] = L.Normalize(net[layer], scale_filler=dict(type="constant", value=normalizations[index]),
+                    across_spatial=False, channel_shared=False)
+                out_layer = norm_name
+                arm_source_layers[index] = norm_name
+        from_layer = out_layer
+        out_layer = "TL{}_{}".format(num_p, 1)
+        ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 3, 1, 1, lr_mult=lr_mult)
+
+        if num_p == 6:
+            from_layer = out_layer
+            out_layer = "TL{}_{}".format(num_p, 2)
+            ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 3, 1, 1, lr_mult=lr_mult)
+
+            from_layer = out_layer
+            out_layer = "P{}".format(num_p)
+            ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 3, 1, 1, lr_mult=lr_mult)
+        else:
+            from_layer = out_layer
+            out_layer = "TL{}_{}".format(num_p, 2)
+            ConvBNLayer(net, from_layer, out_layer, use_batchnorm, False, 256, 3, 1, 1, lr_mult=lr_mult)
+
+            from_layer = "P{}".format(num_p+1)
+            out_layer = "P{}-up".format(num_p+1)
+            DeconvBNLayer(net, from_layer, out_layer, use_batchnorm, False, 256, 2, 0, 2, lr_mult=lr_mult)
+
+            from_layer = ["TL{}_{}".format(num_p, 2), "P{}-up".format(num_p+1)]
+            out_layer = "Elt{}".format(num_p)
+            EltwiseLayer(net, from_layer, out_layer)
+            relu_name = '{}_relu'.format(out_layer)
+            net[relu_name] = L.ReLU(net[out_layer], in_place=True)
+            out_layer = relu_name
+
+            from_layer = out_layer
+            out_layer = "P{}".format(num_p)
+            ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 3, 1, 1, lr_mult=lr_mult)
+
+        num_p = num_p - 1
+
+    return net
+
+
+### Modify the following parameters accordingly ###
+# The directory which contains the caffe code.
+# We assume you are running the script at the CAFFE_ROOT.
+caffe_root = os.getcwd()
+
+# Set true if you want to start training right after generating all files.
+run_soon = True
+# Set true if you want to load from most recently saved snapshot.
+# Otherwise, we will load from the pretrain_model defined below.
+resume_training = True
+# If true, Remove old model files.
+remove_old_models = False
+
+# The database file for training data. Created by data/VOC0712/create_data.sh
+train_data = "examples/VOC0712/VOC0712_trainval_lmdb"
+# The database file for testing data. Created by data/VOC0712/create_data.sh
+test_data = "examples/VOC0712/VOC0712_test_lmdb"
+# Specify the batch sampler.
+resize_width = 320
+resize_height = 320
+resize = "{}x{}".format(resize_width, resize_height)
+batch_sampler = [
+        {
+                'sampler': {
+                        },
+                'max_trials': 1,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.1,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.3,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.5,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.7,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.9,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'max_jaccard_overlap': 1.0,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        ]
+train_transform_param = {
+        'mirror': True,
+        'mean_value': [104, 117, 123],
+        'resize_param': {
+                'prob': 1,
+                'resize_mode': P.Resize.WARP,
+                'height': resize_height,
+                'width': resize_width,
+                'interp_mode': [
+                        P.Resize.LINEAR,
+                        P.Resize.AREA,
+                        P.Resize.NEAREST,
+                        P.Resize.CUBIC,
+                        P.Resize.LANCZOS4,
+                        ],
+                },
+        'distort_param': {
+                'brightness_prob': 0.5,
+                'brightness_delta': 32,
+                'contrast_prob': 0.5,
+                'contrast_lower': 0.5,
+                'contrast_upper': 1.5,
+                'hue_prob': 0.5,
+                'hue_delta': 18,
+                'saturation_prob': 0.5,
+                'saturation_lower': 0.5,
+                'saturation_upper': 1.5,
+                'random_order_prob': 0.0,
+                },
+        'expand_param': {
+                'prob': 0.5,
+                'max_expand_ratio': 4.0,
+                },
+        'emit_constraint': {
+            'emit_type': caffe_pb2.EmitConstraint.CENTER,
+            }
+        }
+test_transform_param = {
+        'mean_value': [104, 117, 123],
+        'resize_param': {
+                'prob': 1,
+                'resize_mode': P.Resize.WARP,
+                'height': resize_height,
+                'width': resize_width,
+                'interp_mode': [P.Resize.LINEAR],
+                },
+        }
+
+# If true, use batch norm for all newly added layers.
+# Currently only the non batch norm version has been tested.
+use_batchnorm = False
+lr_mult = 1
+# Use different initial learning rate.
+if use_batchnorm:
+    base_lr = 0.0004
+else:
+    # A learning rate for batch_size = 1, num_gpus = 1.
+    base_lr = 0.00004
+
+# Modify the job name if you want.
+job_name = "refinedet_vgg16_{}".format(resize)
+# The name of the model. Modify it if you want.
+model_name = "VOC0712_{}".format(job_name)
+
+# Directory which stores the model .prototxt file.
+save_dir = "models/VGGNet/VOC0712/{}".format(job_name)
+# Directory which stores the snapshot of models.
+snapshot_dir = "models/VGGNet/VOC0712/{}".format(job_name)
+# Directory which stores the job script and log file.
+job_dir = "jobs/VGGNet/VOC0712/{}".format(job_name)
+
+# model definition files.
+train_net_file = "{}/train.prototxt".format(save_dir)
+test_net_file = "{}/test.prototxt".format(save_dir)
+deploy_net_file = "{}/deploy.prototxt".format(save_dir)
+solver_file = "{}/solver.prototxt".format(save_dir)
+# snapshot prefix.
+snapshot_prefix = "{}/{}".format(snapshot_dir, model_name)
+# job script path.
+job_file = "{}/{}.sh".format(job_dir, model_name)
+
+# Stores the test image names and sizes. Created by data/VOC0712/create_list.sh
+name_size_file = "data/VOC0712/test_name_size.txt"
+# The pretrained model. We use the Fully convolutional reduced (atrous) VGGNet.
+pretrain_model = "models/VGGNet/VGG_ILSVRC_16_layers_fc_reduced.caffemodel"
+# Stores LabelMapItem.
+label_map_file = "data/VOC0712/labelmap_voc.prototxt"
+
+# MultiBoxLoss parameters.
+num_classes = 21
+share_location = True
+background_label_id = 0
+train_on_diff_gt = True
+normalization_mode = P.Loss.VALID
+code_type = P.PriorBox.CENTER_SIZE
+ignore_cross_boundary_bbox = False
+mining_type = P.MultiBoxLoss.MAX_NEGATIVE
+neg_pos_ratio = 3.
+loc_weight = (neg_pos_ratio + 1.) / 4.
+multibox_loss_param = {
+    'loc_loss_type': P.MultiBoxLoss.SMOOTH_L1,
+    'conf_loss_type': P.MultiBoxLoss.SOFTMAX,
+    'loc_weight': loc_weight,
+    'num_classes': num_classes,
+    'share_location': share_location,
+    'match_type': P.MultiBoxLoss.PER_PREDICTION,
+    'overlap_threshold': 0.5,
+    'use_prior_for_matching': True,
+    'background_label_id': background_label_id,
+    'use_difficult_gt': train_on_diff_gt,
+    'mining_type': mining_type,
+    'neg_pos_ratio': neg_pos_ratio,
+    'neg_overlap': 0.5,
+    'code_type': code_type,
+    'ignore_cross_boundary_bbox': ignore_cross_boundary_bbox,
+    'objectness_score': 0.01,
+    }
+loss_param = {
+    'normalization': normalization_mode,
+    }
+
+# parameters for generating priors.
+# minimum dimension of input image
+# min_dim = 320
+# conv4_3 ==> 40 x 40
+# conv5_3 ==> 20 x 20
+# fc7 ==> 10 x 10
+# conv6_2 ==> 5 x 5
+arm_source_layers = ['conv4_3', 'conv5_3', 'fc7', 'conv6_2']
+odm_source_layers = ['P3', 'P4', 'P5', 'P6']
+min_sizes = [32, 64, 128, 256]
+max_sizes = [[], [], [], []]
+steps = [8, 16, 32, 64]
+aspect_ratios = [[2], [2], [2], [2]]
+# L2 normalize conv4_3 and conv5_3.
+normalizations = [10, 8, -1, -1]
+# variance used to encode/decode prior bboxes.
+if code_type == P.PriorBox.CENTER_SIZE:
+  prior_variance = [0.1, 0.1, 0.2, 0.2]
+else:
+  prior_variance = [0.1]
+flip = True
+clip = False
+
+# Solver parameters.
+# Defining which GPUs to use.
+gpus = "0"
+gpulist = gpus.split(",")
+num_gpus = len(gpulist)
+
+# Divide the mini-batch to different GPUs.
+batch_size = 32
+accum_batch_size = 32
+iter_size = accum_batch_size / batch_size
+solver_mode = P.Solver.CPU
+device_id = 0
+batch_size_per_device = batch_size
+if num_gpus > 0:
+  batch_size_per_device = int(math.ceil(float(batch_size) / num_gpus))
+  iter_size = int(math.ceil(float(accum_batch_size) / (batch_size_per_device * num_gpus)))
+  solver_mode = P.Solver.GPU
+  device_id = int(gpulist[0])
+
+if normalization_mode == P.Loss.NONE:
+  base_lr /= batch_size_per_device
+elif normalization_mode == P.Loss.VALID:
+  base_lr *= 25. / loc_weight
+elif normalization_mode == P.Loss.FULL:
+  # Roughly there are 2000 prior bboxes per image.
+  # TODO(weiliu89): Estimate the exact # of priors.
+  base_lr *= 2000.
+
+# Evaluate on whole test set.
+num_test_image = 4952
+test_batch_size = 1
+test_iter = num_test_image / test_batch_size
+
+solver_param = {
+    # Train parameters
+    'base_lr': base_lr,
+    'weight_decay': 0.0005,
+    'lr_policy': "multistep",
+    'stepvalue': [80000, 100000, 120000],
+    'gamma': 0.1,
+    'momentum': 0.9,
+    'iter_size': iter_size,
+    'max_iter': 120000,
+    'snapshot': 5000,
+    'display': 10,
+    'average_loss': 10,
+    'type': "SGD",
+    'solver_mode': solver_mode,
+    'device_id': device_id,
+    'debug_info': False,
+    'snapshot_after_train': True,
+    # Test parameters
+    # 'test_iter': [test_iter],
+    # 'test_interval': 5000,
+    # 'eval_type': "detection",
+    # 'ap_version': "11point",
+    # 'test_initialization': False,
+    }
+
+# parameters for generating detection output.
+det_out_param = {
+    'num_classes': num_classes,
+    'share_location': share_location,
+    'background_label_id': background_label_id,
+    'nms_param': {'nms_threshold': 0.45, 'top_k': 1000},
+    'keep_top_k': 500,
+    'confidence_threshold': 0.01,
+    'code_type': code_type,
+    'objectness_score': 0.01,
+    }
+
+# parameters for evaluating detection results.
+det_eval_param = {
+    'num_classes': num_classes,
+    'background_label_id': background_label_id,
+    'overlap_threshold': 0.5,
+    'evaluate_difficult_gt': False,
+    'name_size_file': name_size_file,
+    }
+
+### Hopefully you don't need to change the following ###
+# Check file.
+check_if_exist(train_data)
+check_if_exist(test_data)
+check_if_exist(label_map_file)
+check_if_exist(pretrain_model)
+make_if_not_exist(save_dir)
+make_if_not_exist(job_dir)
+make_if_not_exist(snapshot_dir)
+
+# Create train net.
+net = caffe.NetSpec()
+net.data, net.label = CreateAnnotatedDataLayer(train_data, batch_size=batch_size_per_device,
+        train=True, output_label=True, label_map_file=label_map_file,
+        transform_param=train_transform_param, batch_sampler=batch_sampler)
+
+VGGNetBody(net, from_layer='data', fully_conv=True, reduced=True, dilated=False, dropout=False)
+
+AddExtraLayers(net, use_batchnorm, arm_source_layers, normalizations, lr_mult=lr_mult)
+arm_source_layers.reverse()
+normalizations.reverse()
+
+mbox_layers = CreateRefineDetHead(net, data_layer='data', from_layers=arm_source_layers,
+        use_batchnorm=use_batchnorm, min_sizes=min_sizes, max_sizes=max_sizes,
+        aspect_ratios=aspect_ratios, steps=steps, normalizations=[],
+        num_classes=num_classes, share_location=share_location, flip=flip, clip=clip,
+        prior_variance=prior_variance, kernel_size=3, pad=1, lr_mult=lr_mult, from_layers2=odm_source_layers)
+
+name = "arm_loss"
+mbox_layers_arm = []
+mbox_layers_arm.append(mbox_layers[0])
+mbox_layers_arm.append(mbox_layers[1])
+mbox_layers_arm.append(mbox_layers[2])
+mbox_layers_arm.append(net.label)
+multibox_loss_param_arm = multibox_loss_param.copy()
+multibox_loss_param_arm['num_classes'] = 2
+net[name] = L.MultiBoxLoss(*mbox_layers_arm, multibox_loss_param=multibox_loss_param_arm,
+        loss_param=loss_param, include=dict(phase=caffe_pb2.Phase.Value('TRAIN')),
+        propagate_down=[True, True, False, False])
+
+# Create the MultiBoxLossLayer.
+conf_name = "arm_conf"
+reshape_name = "{}_reshape".format(conf_name)
+net[reshape_name] = L.Reshape(net[conf_name], shape=dict(dim=[0, -1, 2]))
+softmax_name = "{}_softmax".format(conf_name)
+net[softmax_name] = L.Softmax(net[reshape_name], axis=2)
+flatten_name = "{}_flatten".format(conf_name)
+net[flatten_name] = L.Flatten(net[softmax_name], axis=1)
+
+name = "odm_loss"
+mbox_layers_odm = []
+mbox_layers_odm.append(mbox_layers[3])
+mbox_layers_odm.append(mbox_layers[4])
+mbox_layers_odm.append(mbox_layers[2])
+mbox_layers_odm.append(net.label)
+mbox_layers_odm.append(net[flatten_name])
+mbox_layers_odm.append(mbox_layers[0])
+net[name] = L.MultiBoxLoss(*mbox_layers_odm, multibox_loss_param=multibox_loss_param,
+        loss_param=loss_param, include=dict(phase=caffe_pb2.Phase.Value('TRAIN')),
+        propagate_down=[True, True, False, False, False, False])
+
+
+with open(train_net_file, 'w') as f:
+    print('name: "{}_train"'.format(model_name), file=f)
+    print(net.to_proto(), file=f)
+shutil.copy(train_net_file, job_dir)
+
+# Create test net.
+net = caffe.NetSpec()
+net.data, net.label = CreateAnnotatedDataLayer(test_data, batch_size=test_batch_size,
+        train=False, output_label=True, label_map_file=label_map_file,
+        transform_param=test_transform_param)
+
+VGGNetBody(net, from_layer='data', fully_conv=True, reduced=True, dilated=False, dropout=False)
+
+arm_source_layers = ['conv4_3', 'conv5_3', 'fc7', 'conv6_2']
+AddExtraLayers(net, use_batchnorm, arm_source_layers, normalizations, lr_mult=lr_mult)
+arm_source_layers.reverse()
+normalizations.reverse()
+
+mbox_layers = CreateRefineDetHead(net, data_layer='data', from_layers=arm_source_layers,
+        use_batchnorm=use_batchnorm, min_sizes=min_sizes, max_sizes=max_sizes,
+        aspect_ratios=aspect_ratios, steps=steps, normalizations=[],
+        num_classes=num_classes, share_location=share_location, flip=flip, clip=clip,
+        prior_variance=prior_variance, kernel_size=3, pad=1, lr_mult=lr_mult, from_layers2=odm_source_layers)
+
+mbox_layers_out = []
+mbox_layers_out.append(mbox_layers[3])
+mbox_layers_out.append(mbox_layers[4])
+mbox_layers_out.append(mbox_layers[2])
+mbox_layers_out.append(mbox_layers[1])
+mbox_layers_out.append(mbox_layers[0])
+
+conf_name = "arm_conf"
+reshape_name = "{}_reshape".format(conf_name)
+net[reshape_name] = L.Reshape(net[conf_name], shape=dict(dim=[0, -1, 2]))
+softmax_name = "{}_softmax".format(conf_name)
+net[softmax_name] = L.Softmax(net[reshape_name], axis=2)
+flatten_name = "{}_flatten".format(conf_name)
+net[flatten_name] = L.Flatten(net[softmax_name], axis=1)
+mbox_layers_out[3] = net[flatten_name]
+
+conf_name = "odm_conf"
+reshape_name = "{}_reshape".format(conf_name)
+net[reshape_name] = L.Reshape(net[conf_name], shape=dict(dim=[0, -1, num_classes]))
+softmax_name = "{}_softmax".format(conf_name)
+net[softmax_name] = L.Softmax(net[reshape_name], axis=2)
+flatten_name = "{}_flatten".format(conf_name)
+net[flatten_name] = L.Flatten(net[softmax_name], axis=1)
+mbox_layers_out[1] = net[flatten_name]
+
+net.detection_out = L.DetectionOutput(*mbox_layers_out,
+    detection_output_param=det_out_param,
+    include=dict(phase=caffe_pb2.Phase.Value('TEST')))
+net.detection_eval = L.DetectionEvaluate(net.detection_out, net.label,
+    detection_evaluate_param=det_eval_param,
+    include=dict(phase=caffe_pb2.Phase.Value('TEST')))
+
+with open(test_net_file, 'w') as f:
+    print('name: "{}_test"'.format(model_name), file=f)
+    print(net.to_proto(), file=f)
+shutil.copy(test_net_file, job_dir)
+
+# Create deploy net.
+# Remove the first and last layer from test net.
+deploy_net = net
+with open(deploy_net_file, 'w') as f:
+    net_param = deploy_net.to_proto()
+    # Remove the first (AnnotatedData) and last (DetectionEvaluate) layer from test net.
+    del net_param.layer[0]
+    del net_param.layer[-1]
+    net_param.name = '{}_deploy'.format(model_name)
+    net_param.input.extend(['data'])
+    net_param.input_shape.extend([
+        caffe_pb2.BlobShape(dim=[1, 3, resize_height, resize_width])])
+    print(net_param, file=f)
+shutil.copy(deploy_net_file, job_dir)
+
+# Create solver.
+solver = caffe_pb2.SolverParameter(
+        train_net=train_net_file,
+        # test_net=[test_net_file],
+        snapshot_prefix=snapshot_prefix,
+        **solver_param)
+
+with open(solver_file, 'w') as f:
+    print(solver, file=f)
+shutil.copy(solver_file, job_dir)
+
+max_iter = 0
+# Find most recent snapshot.
+for file in os.listdir(snapshot_dir):
+  if file.endswith(".solverstate"):
+    basename = os.path.splitext(file)[0]
+    iter = int(basename.split("{}_iter_".format(model_name))[1])
+    if iter > max_iter:
+      max_iter = iter
+
+train_src_param = '--weights="{}" \\\n'.format(pretrain_model)
+if resume_training:
+  if max_iter > 0:
+    train_src_param = '--snapshot="{}_iter_{}.solverstate" \\\n'.format(snapshot_prefix, max_iter)
+
+if remove_old_models:
+  # Remove any snapshots smaller than max_iter.
+  for file in os.listdir(snapshot_dir):
+    if file.endswith(".solverstate"):
+      basename = os.path.splitext(file)[0]
+      iter = int(basename.split("{}_iter_".format(model_name))[1])
+      if max_iter > iter:
+        os.remove("{}/{}".format(snapshot_dir, file))
+    if file.endswith(".caffemodel"):
+      basename = os.path.splitext(file)[0]
+      iter = int(basename.split("{}_iter_".format(model_name))[1])
+      if max_iter > iter:
+        os.remove("{}/{}".format(snapshot_dir, file))
+
+# Create job file.
+with open(job_file, 'w') as f:
+  f.write('cd {}\n'.format(caffe_root))
+  f.write('./build/tools/caffe train \\\n')
+  f.write('--solver="{}" \\\n'.format(solver_file))
+  f.write(train_src_param)
+  if solver_param['solver_mode'] == P.Solver.GPU:
+    f.write('--gpu {} 2>&1 | tee {}/{}.log\n'.format(gpus, job_dir, model_name))
+  else:
+    f.write('2>&1 | tee {}/{}.log\n'.format(job_dir, model_name))
+
+# Copy the python script to job_dir.
+py_file = os.path.abspath(__file__)
+shutil.copy(py_file, job_dir)
+
+# Run the job.
+os.chmod(job_file, stat.S_IRWXU)
+if run_soon:
+  subprocess.call(job_file, shell=True)
diff --git a/examples/refinedet/VGG16_VOC2007_512.py b/examples/refinedet/VGG16_VOC2007_512.py
new file mode 100644
index 0000000..ca5fcc2
--- /dev/null
+++ b/examples/refinedet/VGG16_VOC2007_512.py
@@ -0,0 +1,605 @@
+from __future__ import print_function
+import sys
+sys.path.append("./python")
+import caffe
+from caffe.model_libs import *
+from google.protobuf import text_format
+
+import math
+import os
+import shutil
+import stat
+import subprocess
+
+# Add extra layers on top of a "base" network (e.g. VGGNet or ResNet).
+def AddExtraLayers(net, use_batchnorm=True, arm_source_layers=[], normalizations=[], lr_mult=1):
+    use_relu = True
+
+    # Add additional convolutional layers.
+    # 512/32: 16 x 16
+    from_layer = net.keys()[-1]
+
+    # 512/64: 8 x 8
+    out_layer = "conv6_1"
+    ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 1, 0, 1, lr_mult=lr_mult)
+
+    from_layer = out_layer
+    out_layer = "conv6_2"
+    ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 512, 3, 1, 2, lr_mult=lr_mult)
+
+    arm_source_layers.reverse()
+    normalizations.reverse()
+    num_p = 6
+    for index, layer in enumerate(arm_source_layers):
+        out_layer = layer
+        if normalizations:
+            if normalizations[index] != -1:
+                norm_name = "{}_norm".format(layer)
+                net[norm_name] = L.Normalize(net[layer], scale_filler=dict(type="constant", value=normalizations[index]),
+                    across_spatial=False, channel_shared=False)
+                out_layer = norm_name
+                arm_source_layers[index] = norm_name
+        from_layer = out_layer
+        out_layer = "TL{}_{}".format(num_p, 1)
+        ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 3, 1, 1, lr_mult=lr_mult)
+
+        if num_p == 6:
+            from_layer = out_layer
+            out_layer = "TL{}_{}".format(num_p, 2)
+            ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 3, 1, 1, lr_mult=lr_mult)
+
+            from_layer = out_layer
+            out_layer = "P{}".format(num_p)
+            ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 3, 1, 1, lr_mult=lr_mult)
+        else:
+            from_layer = out_layer
+            out_layer = "TL{}_{}".format(num_p, 2)
+            ConvBNLayer(net, from_layer, out_layer, use_batchnorm, False, 256, 3, 1, 1, lr_mult=lr_mult)
+
+            from_layer = "P{}".format(num_p+1)
+            out_layer = "P{}-up".format(num_p+1)
+            DeconvBNLayer(net, from_layer, out_layer, use_batchnorm, False, 256, 2, 0, 2, lr_mult=lr_mult)
+
+            from_layer = ["TL{}_{}".format(num_p, 2), "P{}-up".format(num_p+1)]
+            out_layer = "Elt{}".format(num_p)
+            EltwiseLayer(net, from_layer, out_layer)
+            relu_name = '{}_relu'.format(out_layer)
+            net[relu_name] = L.ReLU(net[out_layer], in_place=True)
+            out_layer = relu_name
+
+            from_layer = out_layer
+            out_layer = "P{}".format(num_p)
+            ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 3, 1, 1, lr_mult=lr_mult)
+
+        num_p = num_p - 1
+
+    return net
+
+
+### Modify the following parameters accordingly ###
+# The directory which contains the caffe code.
+# We assume you are running the script at the CAFFE_ROOT.
+caffe_root = os.getcwd()
+
+# Set true if you want to start training right after generating all files.
+run_soon = True
+# Set true if you want to load from most recently saved snapshot.
+# Otherwise, we will load from the pretrain_model defined below.
+resume_training = True
+# If true, Remove old model files.
+remove_old_models = False
+
+# The database file for training data. Created by data/VOC0712/create_data.sh
+train_data = "examples/VOC0712/VOC0712_trainval_lmdb"
+# The database file for testing data. Created by data/VOC0712/create_data.sh
+test_data = "examples/VOC0712/VOC0712_test_lmdb"
+# Specify the batch sampler.
+resize_width = 512
+resize_height = 512
+resize = "{}x{}".format(resize_width, resize_height)
+batch_sampler = [
+        {
+                'sampler': {
+                        },
+                'max_trials': 1,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.1,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.3,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.5,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.7,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.9,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'max_jaccard_overlap': 1.0,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        ]
+train_transform_param = {
+        'mirror': True,
+        'mean_value': [104, 117, 123],
+        'resize_param': {
+                'prob': 1,
+                'resize_mode': P.Resize.WARP,
+                'height': resize_height,
+                'width': resize_width,
+                'interp_mode': [
+                        P.Resize.LINEAR,
+                        P.Resize.AREA,
+                        P.Resize.NEAREST,
+                        P.Resize.CUBIC,
+                        P.Resize.LANCZOS4,
+                        ],
+                },
+        'distort_param': {
+                'brightness_prob': 0.5,
+                'brightness_delta': 32,
+                'contrast_prob': 0.5,
+                'contrast_lower': 0.5,
+                'contrast_upper': 1.5,
+                'hue_prob': 0.5,
+                'hue_delta': 18,
+                'saturation_prob': 0.5,
+                'saturation_lower': 0.5,
+                'saturation_upper': 1.5,
+                'random_order_prob': 0.0,
+                },
+        'expand_param': {
+                'prob': 0.5,
+                'max_expand_ratio': 4.0,
+                },
+        'emit_constraint': {
+            'emit_type': caffe_pb2.EmitConstraint.CENTER,
+            }
+        }
+test_transform_param = {
+        'mean_value': [104, 117, 123],
+        'resize_param': {
+                'prob': 1,
+                'resize_mode': P.Resize.WARP,
+                'height': resize_height,
+                'width': resize_width,
+                'interp_mode': [P.Resize.LINEAR],
+                },
+        }
+
+# If true, use batch norm for all newly added layers.
+# Currently only the non batch norm version has been tested.
+use_batchnorm = False
+lr_mult = 1
+# Use different initial learning rate.
+if use_batchnorm:
+    base_lr = 0.0004
+else:
+    # A learning rate for batch_size = 1, num_gpus = 1.
+    base_lr = 0.00004
+
+# Modify the job name if you want.
+job_name = "refinedet_vgg16_{}".format(resize)
+# The name of the model. Modify it if you want.
+model_name = "VOC0712_{}".format(job_name)
+
+# Directory which stores the model .prototxt file.
+save_dir = "models/VGGNet/VOC0712/{}".format(job_name)
+# Directory which stores the snapshot of models.
+snapshot_dir = "models/VGGNet/VOC0712/{}".format(job_name)
+# Directory which stores the job script and log file.
+job_dir = "jobs/VGGNet/VOC0712/{}".format(job_name)
+
+# model definition files.
+train_net_file = "{}/train.prototxt".format(save_dir)
+test_net_file = "{}/test.prototxt".format(save_dir)
+deploy_net_file = "{}/deploy.prototxt".format(save_dir)
+solver_file = "{}/solver.prototxt".format(save_dir)
+# snapshot prefix.
+snapshot_prefix = "{}/{}".format(snapshot_dir, model_name)
+# job script path.
+job_file = "{}/{}.sh".format(job_dir, model_name)
+
+# Stores the test image names and sizes. Created by data/VOC0712/create_list.sh
+name_size_file = "data/VOC0712/test_name_size.txt"
+# The pretrained model. We use the Fully convolutional reduced (atrous) VGGNet.
+pretrain_model = "models/VGGNet/VGG_ILSVRC_16_layers_fc_reduced.caffemodel"
+# Stores LabelMapItem.
+label_map_file = "data/VOC0712/labelmap_voc.prototxt"
+
+# MultiBoxLoss parameters.
+num_classes = 21
+share_location = True
+background_label_id = 0
+train_on_diff_gt = True
+normalization_mode = P.Loss.VALID
+code_type = P.PriorBox.CENTER_SIZE
+ignore_cross_boundary_bbox = False
+mining_type = P.MultiBoxLoss.MAX_NEGATIVE
+neg_pos_ratio = 3.
+loc_weight = (neg_pos_ratio + 1.) / 4.
+multibox_loss_param = {
+    'loc_loss_type': P.MultiBoxLoss.SMOOTH_L1,
+    'conf_loss_type': P.MultiBoxLoss.SOFTMAX,
+    'loc_weight': loc_weight,
+    'num_classes': num_classes,
+    'share_location': share_location,
+    'match_type': P.MultiBoxLoss.PER_PREDICTION,
+    'overlap_threshold': 0.5,
+    'use_prior_for_matching': True,
+    'background_label_id': background_label_id,
+    'use_difficult_gt': train_on_diff_gt,
+    'mining_type': mining_type,
+    'neg_pos_ratio': neg_pos_ratio,
+    'neg_overlap': 0.5,
+    'code_type': code_type,
+    'ignore_cross_boundary_bbox': ignore_cross_boundary_bbox,
+    'objectness_score': 0.01,
+    }
+loss_param = {
+    'normalization': normalization_mode,
+    }
+
+# parameters for generating priors.
+# minimum dimension of input image
+# min_dim = 512
+# conv4_3 ==> 64 x 64
+# conv5_3 ==> 32 x 32
+# fc7 ==> 16 x 16
+# conv6_2 ==> 8 x 8
+arm_source_layers = ['conv4_3', 'conv5_3', 'fc7', 'conv6_2']
+odm_source_layers = ['P3', 'P4', 'P5', 'P6']
+min_sizes = [32, 64, 128, 256]
+max_sizes = [[], [], [], []]
+steps = [8, 16, 32, 64]
+aspect_ratios = [[2], [2], [2], [2]]
+# L2 normalize conv4_3 and conv5_3.
+normalizations = [10, 8, -1, -1]
+# variance used to encode/decode prior bboxes.
+if code_type == P.PriorBox.CENTER_SIZE:
+  prior_variance = [0.1, 0.1, 0.2, 0.2]
+else:
+  prior_variance = [0.1]
+flip = True
+clip = False
+
+# Solver parameters.
+# Defining which GPUs to use.
+gpus = "0,1,2,3"
+gpulist = gpus.split(",")
+num_gpus = len(gpulist)
+
+# Divide the mini-batch to different GPUs.
+batch_size = 32
+accum_batch_size = 32
+iter_size = accum_batch_size / batch_size
+solver_mode = P.Solver.CPU
+device_id = 0
+batch_size_per_device = batch_size
+if num_gpus > 0:
+  batch_size_per_device = int(math.ceil(float(batch_size) / num_gpus))
+  iter_size = int(math.ceil(float(accum_batch_size) / (batch_size_per_device * num_gpus)))
+  solver_mode = P.Solver.GPU
+  device_id = int(gpulist[0])
+
+if normalization_mode == P.Loss.NONE:
+  base_lr /= batch_size_per_device
+elif normalization_mode == P.Loss.VALID:
+  base_lr *= 25. / loc_weight
+elif normalization_mode == P.Loss.FULL:
+  # Roughly there are 2000 prior bboxes per image.
+  # TODO(weiliu89): Estimate the exact # of priors.
+  base_lr *= 2000.
+
+# Evaluate on whole test set.
+num_test_image = 4952
+test_batch_size = 1
+test_iter = num_test_image / test_batch_size
+
+solver_param = {
+    # Train parameters
+    'base_lr': base_lr,
+    'weight_decay': 0.0005,
+    'lr_policy': "multistep",
+    'stepvalue': [80000, 100000, 120000],
+    'gamma': 0.1,
+    'momentum': 0.9,
+    'iter_size': iter_size,
+    'max_iter': 120000,
+    'snapshot': 5000,
+    'display': 10,
+    'average_loss': 10,
+    'type': "SGD",
+    'solver_mode': solver_mode,
+    'device_id': device_id,
+    'debug_info': False,
+    'snapshot_after_train': True,
+    # Test parameters
+    # 'test_iter': [test_iter],
+    # 'test_interval': 5000,
+    # 'eval_type': "detection",
+    # 'ap_version': "11point",
+    # 'test_initialization': False,
+    }
+
+# parameters for generating detection output.
+det_out_param = {
+    'num_classes': num_classes,
+    'share_location': share_location,
+    'background_label_id': background_label_id,
+    'nms_param': {'nms_threshold': 0.45, 'top_k': 1000},
+    'keep_top_k': 500,
+    'confidence_threshold': 0.01,
+    'code_type': code_type,
+    'objectness_score': 0.01,
+    }
+
+# parameters for evaluating detection results.
+det_eval_param = {
+    'num_classes': num_classes,
+    'background_label_id': background_label_id,
+    'overlap_threshold': 0.5,
+    'evaluate_difficult_gt': False,
+    'name_size_file': name_size_file,
+    }
+
+### Hopefully you don't need to change the following ###
+# Check file.
+check_if_exist(train_data)
+check_if_exist(test_data)
+check_if_exist(label_map_file)
+check_if_exist(pretrain_model)
+make_if_not_exist(save_dir)
+make_if_not_exist(job_dir)
+make_if_not_exist(snapshot_dir)
+
+# Create train net.
+net = caffe.NetSpec()
+net.data, net.label = CreateAnnotatedDataLayer(train_data, batch_size=batch_size_per_device,
+        train=True, output_label=True, label_map_file=label_map_file,
+        transform_param=train_transform_param, batch_sampler=batch_sampler)
+
+VGGNetBody(net, from_layer='data', fully_conv=True, reduced=True, dilated=False, dropout=False)
+
+AddExtraLayers(net, use_batchnorm, arm_source_layers, normalizations, lr_mult=lr_mult)
+arm_source_layers.reverse()
+normalizations.reverse()
+
+mbox_layers = CreateRefineDetHead(net, data_layer='data', from_layers=arm_source_layers,
+        use_batchnorm=use_batchnorm, min_sizes=min_sizes, max_sizes=max_sizes,
+        aspect_ratios=aspect_ratios, steps=steps, normalizations=[],
+        num_classes=num_classes, share_location=share_location, flip=flip, clip=clip,
+        prior_variance=prior_variance, kernel_size=3, pad=1, lr_mult=lr_mult, from_layers2=odm_source_layers)
+
+name = "arm_loss"
+mbox_layers_arm = []
+mbox_layers_arm.append(mbox_layers[0])
+mbox_layers_arm.append(mbox_layers[1])
+mbox_layers_arm.append(mbox_layers[2])
+mbox_layers_arm.append(net.label)
+multibox_loss_param_arm = multibox_loss_param.copy()
+multibox_loss_param_arm['num_classes'] = 2
+net[name] = L.MultiBoxLoss(*mbox_layers_arm, multibox_loss_param=multibox_loss_param_arm,
+        loss_param=loss_param, include=dict(phase=caffe_pb2.Phase.Value('TRAIN')),
+        propagate_down=[True, True, False, False])
+
+# Create the MultiBoxLossLayer.
+conf_name = "arm_conf"
+reshape_name = "{}_reshape".format(conf_name)
+net[reshape_name] = L.Reshape(net[conf_name], shape=dict(dim=[0, -1, 2]))
+softmax_name = "{}_softmax".format(conf_name)
+net[softmax_name] = L.Softmax(net[reshape_name], axis=2)
+flatten_name = "{}_flatten".format(conf_name)
+net[flatten_name] = L.Flatten(net[softmax_name], axis=1)
+
+name = "odm_loss"
+mbox_layers_odm = []
+mbox_layers_odm.append(mbox_layers[3])
+mbox_layers_odm.append(mbox_layers[4])
+mbox_layers_odm.append(mbox_layers[2])
+mbox_layers_odm.append(net.label)
+mbox_layers_odm.append(net[flatten_name])
+mbox_layers_odm.append(mbox_layers[0])
+net[name] = L.MultiBoxLoss(*mbox_layers_odm, multibox_loss_param=multibox_loss_param,
+        loss_param=loss_param, include=dict(phase=caffe_pb2.Phase.Value('TRAIN')),
+        propagate_down=[True, True, False, False, False, False])
+
+
+with open(train_net_file, 'w') as f:
+    print('name: "{}_train"'.format(model_name), file=f)
+    print(net.to_proto(), file=f)
+shutil.copy(train_net_file, job_dir)
+
+# Create test net.
+net = caffe.NetSpec()
+net.data, net.label = CreateAnnotatedDataLayer(test_data, batch_size=test_batch_size,
+        train=False, output_label=True, label_map_file=label_map_file,
+        transform_param=test_transform_param)
+
+VGGNetBody(net, from_layer='data', fully_conv=True, reduced=True, dilated=False, dropout=False)
+
+arm_source_layers = ['conv4_3', 'conv5_3', 'fc7', 'conv6_2']
+AddExtraLayers(net, use_batchnorm, arm_source_layers, normalizations, lr_mult=lr_mult)
+arm_source_layers.reverse()
+normalizations.reverse()
+
+mbox_layers = CreateRefineDetHead(net, data_layer='data', from_layers=arm_source_layers,
+        use_batchnorm=use_batchnorm, min_sizes=min_sizes, max_sizes=max_sizes,
+        aspect_ratios=aspect_ratios, steps=steps, normalizations=[],
+        num_classes=num_classes, share_location=share_location, flip=flip, clip=clip,
+        prior_variance=prior_variance, kernel_size=3, pad=1, lr_mult=lr_mult, from_layers2=odm_source_layers)
+
+mbox_layers_out = []
+mbox_layers_out.append(mbox_layers[3])
+mbox_layers_out.append(mbox_layers[4])
+mbox_layers_out.append(mbox_layers[2])
+mbox_layers_out.append(mbox_layers[1])
+mbox_layers_out.append(mbox_layers[0])
+
+conf_name = "arm_conf"
+reshape_name = "{}_reshape".format(conf_name)
+net[reshape_name] = L.Reshape(net[conf_name], shape=dict(dim=[0, -1, 2]))
+softmax_name = "{}_softmax".format(conf_name)
+net[softmax_name] = L.Softmax(net[reshape_name], axis=2)
+flatten_name = "{}_flatten".format(conf_name)
+net[flatten_name] = L.Flatten(net[softmax_name], axis=1)
+mbox_layers_out[3] = net[flatten_name]
+
+conf_name = "odm_conf"
+reshape_name = "{}_reshape".format(conf_name)
+net[reshape_name] = L.Reshape(net[conf_name], shape=dict(dim=[0, -1, num_classes]))
+softmax_name = "{}_softmax".format(conf_name)
+net[softmax_name] = L.Softmax(net[reshape_name], axis=2)
+flatten_name = "{}_flatten".format(conf_name)
+net[flatten_name] = L.Flatten(net[softmax_name], axis=1)
+mbox_layers_out[1] = net[flatten_name]
+
+net.detection_out = L.DetectionOutput(*mbox_layers_out,
+    detection_output_param=det_out_param,
+    include=dict(phase=caffe_pb2.Phase.Value('TEST')))
+net.detection_eval = L.DetectionEvaluate(net.detection_out, net.label,
+    detection_evaluate_param=det_eval_param,
+    include=dict(phase=caffe_pb2.Phase.Value('TEST')))
+
+with open(test_net_file, 'w') as f:
+    print('name: "{}_test"'.format(model_name), file=f)
+    print(net.to_proto(), file=f)
+shutil.copy(test_net_file, job_dir)
+
+# Create deploy net.
+# Remove the first and last layer from test net.
+deploy_net = net
+with open(deploy_net_file, 'w') as f:
+    net_param = deploy_net.to_proto()
+    # Remove the first (AnnotatedData) and last (DetectionEvaluate) layer from test net.
+    del net_param.layer[0]
+    del net_param.layer[-1]
+    net_param.name = '{}_deploy'.format(model_name)
+    net_param.input.extend(['data'])
+    net_param.input_shape.extend([
+        caffe_pb2.BlobShape(dim=[1, 3, resize_height, resize_width])])
+    print(net_param, file=f)
+shutil.copy(deploy_net_file, job_dir)
+
+# Create solver.
+solver = caffe_pb2.SolverParameter(
+        train_net=train_net_file,
+        # test_net=[test_net_file],
+        snapshot_prefix=snapshot_prefix,
+        **solver_param)
+
+with open(solver_file, 'w') as f:
+    print(solver, file=f)
+shutil.copy(solver_file, job_dir)
+
+max_iter = 0
+# Find most recent snapshot.
+for file in os.listdir(snapshot_dir):
+  if file.endswith(".solverstate"):
+    basename = os.path.splitext(file)[0]
+    iter = int(basename.split("{}_iter_".format(model_name))[1])
+    if iter > max_iter:
+      max_iter = iter
+
+train_src_param = '--weights="{}" \\\n'.format(pretrain_model)
+if resume_training:
+  if max_iter > 0:
+    train_src_param = '--snapshot="{}_iter_{}.solverstate" \\\n'.format(snapshot_prefix, max_iter)
+
+if remove_old_models:
+  # Remove any snapshots smaller than max_iter.
+  for file in os.listdir(snapshot_dir):
+    if file.endswith(".solverstate"):
+      basename = os.path.splitext(file)[0]
+      iter = int(basename.split("{}_iter_".format(model_name))[1])
+      if max_iter > iter:
+        os.remove("{}/{}".format(snapshot_dir, file))
+    if file.endswith(".caffemodel"):
+      basename = os.path.splitext(file)[0]
+      iter = int(basename.split("{}_iter_".format(model_name))[1])
+      if max_iter > iter:
+        os.remove("{}/{}".format(snapshot_dir, file))
+
+# Create job file.
+with open(job_file, 'w') as f:
+  f.write('cd {}\n'.format(caffe_root))
+  f.write('./build/tools/caffe train \\\n')
+  f.write('--solver="{}" \\\n'.format(solver_file))
+  f.write(train_src_param)
+  if solver_param['solver_mode'] == P.Solver.GPU:
+    f.write('--gpu {} 2>&1 | tee {}/{}.log\n'.format(gpus, job_dir, model_name))
+  else:
+    f.write('2>&1 | tee {}/{}.log\n'.format(job_dir, model_name))
+
+# Copy the python script to job_dir.
+py_file = os.path.abspath(__file__)
+shutil.copy(py_file, job_dir)
+
+# Run the job.
+os.chmod(job_file, stat.S_IRWXU)
+if run_soon:
+  subprocess.call(job_file, shell=True)
diff --git a/examples/refinedet/VGG16_VOC2012_320.py b/examples/refinedet/VGG16_VOC2012_320.py
new file mode 100644
index 0000000..048e3e8
--- /dev/null
+++ b/examples/refinedet/VGG16_VOC2012_320.py
@@ -0,0 +1,605 @@
+from __future__ import print_function
+import sys
+sys.path.append("./python")
+import caffe
+from caffe.model_libs import *
+from google.protobuf import text_format
+
+import math
+import os
+import shutil
+import stat
+import subprocess
+
+# Add extra layers on top of a "base" network (e.g. VGGNet or ResNet).
+def AddExtraLayers(net, use_batchnorm=True, arm_source_layers=[], normalizations=[], lr_mult=1):
+    use_relu = True
+
+    # Add additional convolutional layers.
+    # 320/32: 10 x 10
+    from_layer = net.keys()[-1]
+
+    # 320/64: 5 x 5
+    out_layer = "conv6_1"
+    ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 1, 0, 1, lr_mult=lr_mult)
+
+    from_layer = out_layer
+    out_layer = "conv6_2"
+    ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 512, 3, 1, 2, lr_mult=lr_mult)
+
+    arm_source_layers.reverse()
+    normalizations.reverse()
+    num_p = 6
+    for index, layer in enumerate(arm_source_layers):
+        out_layer = layer
+        if normalizations:
+            if normalizations[index] != -1:
+                norm_name = "{}_norm".format(layer)
+                net[norm_name] = L.Normalize(net[layer], scale_filler=dict(type="constant", value=normalizations[index]),
+                    across_spatial=False, channel_shared=False)
+                out_layer = norm_name
+                arm_source_layers[index] = norm_name
+        from_layer = out_layer
+        out_layer = "TL{}_{}".format(num_p, 1)
+        ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 3, 1, 1, lr_mult=lr_mult)
+
+        if num_p == 6:
+            from_layer = out_layer
+            out_layer = "TL{}_{}".format(num_p, 2)
+            ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 3, 1, 1, lr_mult=lr_mult)
+
+            from_layer = out_layer
+            out_layer = "P{}".format(num_p)
+            ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 3, 1, 1, lr_mult=lr_mult)
+        else:
+            from_layer = out_layer
+            out_layer = "TL{}_{}".format(num_p, 2)
+            ConvBNLayer(net, from_layer, out_layer, use_batchnorm, False, 256, 3, 1, 1, lr_mult=lr_mult)
+
+            from_layer = "P{}".format(num_p+1)
+            out_layer = "P{}-up".format(num_p+1)
+            DeconvBNLayer(net, from_layer, out_layer, use_batchnorm, False, 256, 2, 0, 2, lr_mult=lr_mult)
+
+            from_layer = ["TL{}_{}".format(num_p, 2), "P{}-up".format(num_p+1)]
+            out_layer = "Elt{}".format(num_p)
+            EltwiseLayer(net, from_layer, out_layer)
+            relu_name = '{}_relu'.format(out_layer)
+            net[relu_name] = L.ReLU(net[out_layer], in_place=True)
+            out_layer = relu_name
+
+            from_layer = out_layer
+            out_layer = "P{}".format(num_p)
+            ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 3, 1, 1, lr_mult=lr_mult)
+
+        num_p = num_p - 1
+
+    return net
+
+
+### Modify the following parameters accordingly ###
+# The directory which contains the caffe code.
+# We assume you are running the script at the CAFFE_ROOT.
+caffe_root = os.getcwd()
+
+# Set true if you want to start training right after generating all files.
+run_soon = True
+# Set true if you want to load from most recently saved snapshot.
+# Otherwise, we will load from the pretrain_model defined below.
+resume_training = True
+# If true, Remove old model files.
+remove_old_models = False
+
+# The database file for training data. Created by data/VOC0712Plus/create_data.sh
+train_data = "examples/VOC0712Plus/VOC0712Plus_trainval_lmdb"
+# The database file for testing data. Created by data/VOC0712Plus/create_data.sh
+test_data = "examples/VOC0712Plus/VOC0712Plus_test_lmdb"
+# Specify the batch sampler.
+resize_width = 320
+resize_height = 320
+resize = "{}x{}".format(resize_width, resize_height)
+batch_sampler = [
+        {
+                'sampler': {
+                        },
+                'max_trials': 1,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.1,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.3,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.5,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.7,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.9,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'max_jaccard_overlap': 1.0,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        ]
+train_transform_param = {
+        'mirror': True,
+        'mean_value': [104, 117, 123],
+        'resize_param': {
+                'prob': 1,
+                'resize_mode': P.Resize.WARP,
+                'height': resize_height,
+                'width': resize_width,
+                'interp_mode': [
+                        P.Resize.LINEAR,
+                        P.Resize.AREA,
+                        P.Resize.NEAREST,
+                        P.Resize.CUBIC,
+                        P.Resize.LANCZOS4,
+                        ],
+                },
+        'distort_param': {
+                'brightness_prob': 0.5,
+                'brightness_delta': 32,
+                'contrast_prob': 0.5,
+                'contrast_lower': 0.5,
+                'contrast_upper': 1.5,
+                'hue_prob': 0.5,
+                'hue_delta': 18,
+                'saturation_prob': 0.5,
+                'saturation_lower': 0.5,
+                'saturation_upper': 1.5,
+                'random_order_prob': 0.0,
+                },
+        'expand_param': {
+                'prob': 0.5,
+                'max_expand_ratio': 4.0,
+                },
+        'emit_constraint': {
+            'emit_type': caffe_pb2.EmitConstraint.CENTER,
+            }
+        }
+test_transform_param = {
+        'mean_value': [104, 117, 123],
+        'resize_param': {
+                'prob': 1,
+                'resize_mode': P.Resize.WARP,
+                'height': resize_height,
+                'width': resize_width,
+                'interp_mode': [P.Resize.LINEAR],
+                },
+        }
+
+# If true, use batch norm for all newly added layers.
+# Currently only the non batch norm version has been tested.
+use_batchnorm = False
+lr_mult = 1
+# Use different initial learning rate.
+if use_batchnorm:
+    base_lr = 0.0004
+else:
+    # A learning rate for batch_size = 1, num_gpus = 1.
+    base_lr = 0.00004
+
+# Modify the job name if you want.
+job_name = "refinedet_vgg16_{}".format(resize)
+# The name of the model. Modify it if you want.
+model_name = "VOC0712Plus_{}".format(job_name)
+
+# Directory which stores the model .prototxt file.
+save_dir = "models/VGGNet/VOC0712Plus/{}".format(job_name)
+# Directory which stores the snapshot of models.
+snapshot_dir = "models/VGGNet/VOC0712Plus/{}".format(job_name)
+# Directory which stores the job script and log file.
+job_dir = "jobs/VGGNet/VOC0712Plus/{}".format(job_name)
+
+# model definition files.
+train_net_file = "{}/train.prototxt".format(save_dir)
+test_net_file = "{}/test.prototxt".format(save_dir)
+deploy_net_file = "{}/deploy.prototxt".format(save_dir)
+solver_file = "{}/solver.prototxt".format(save_dir)
+# snapshot prefix.
+snapshot_prefix = "{}/{}".format(snapshot_dir, model_name)
+# job script path.
+job_file = "{}/{}.sh".format(job_dir, model_name)
+
+# Stores the test image names and sizes. Created by data/VOC0712Plus/create_list.sh
+name_size_file = "data/VOC0712Plus/test_name_size.txt"
+# The pretrained model. We use the Fully convolutional reduced (atrous) VGGNet.
+pretrain_model = "models/VGGNet/VGG_ILSVRC_16_layers_fc_reduced.caffemodel"
+# Stores LabelMapItem.
+label_map_file = "data/VOC0712Plus/labelmap_voc.prototxt"
+
+# MultiBoxLoss parameters.
+num_classes = 21
+share_location = True
+background_label_id = 0
+train_on_diff_gt = True
+normalization_mode = P.Loss.VALID
+code_type = P.PriorBox.CENTER_SIZE
+ignore_cross_boundary_bbox = False
+mining_type = P.MultiBoxLoss.MAX_NEGATIVE
+neg_pos_ratio = 3.
+loc_weight = (neg_pos_ratio + 1.) / 4.
+multibox_loss_param = {
+    'loc_loss_type': P.MultiBoxLoss.SMOOTH_L1,
+    'conf_loss_type': P.MultiBoxLoss.SOFTMAX,
+    'loc_weight': loc_weight,
+    'num_classes': num_classes,
+    'share_location': share_location,
+    'match_type': P.MultiBoxLoss.PER_PREDICTION,
+    'overlap_threshold': 0.5,
+    'use_prior_for_matching': True,
+    'background_label_id': background_label_id,
+    'use_difficult_gt': train_on_diff_gt,
+    'mining_type': mining_type,
+    'neg_pos_ratio': neg_pos_ratio,
+    'neg_overlap': 0.5,
+    'code_type': code_type,
+    'ignore_cross_boundary_bbox': ignore_cross_boundary_bbox,
+    'objectness_score': 0.01,
+    }
+loss_param = {
+    'normalization': normalization_mode,
+    }
+
+# parameters for generating priors.
+# minimum dimension of input image
+# min_dim = 320
+# conv4_3 ==> 40 x 40
+# conv5_3 ==> 20 x 20
+# fc7 ==> 10 x 10
+# conv6_2 ==> 5 x 5
+arm_source_layers = ['conv4_3', 'conv5_3', 'fc7', 'conv6_2']
+odm_source_layers = ['P3', 'P4', 'P5', 'P6']
+min_sizes = [32, 64, 128, 256]
+max_sizes = [[], [], [], []]
+steps = [8, 16, 32, 64]
+aspect_ratios = [[2], [2], [2], [2]]
+# L2 normalize conv4_3 and conv5_3.
+normalizations = [10, 8, -1, -1]
+# variance used to encode/decode prior bboxes.
+if code_type == P.PriorBox.CENTER_SIZE:
+  prior_variance = [0.1, 0.1, 0.2, 0.2]
+else:
+  prior_variance = [0.1]
+flip = True
+clip = False
+
+# Solver parameters.
+# Defining which GPUs to use.
+gpus = "0"
+gpulist = gpus.split(",")
+num_gpus = len(gpulist)
+
+# Divide the mini-batch to different GPUs.
+batch_size = 32
+accum_batch_size = 32
+iter_size = accum_batch_size / batch_size
+solver_mode = P.Solver.CPU
+device_id = 0
+batch_size_per_device = batch_size
+if num_gpus > 0:
+  batch_size_per_device = int(math.ceil(float(batch_size) / num_gpus))
+  iter_size = int(math.ceil(float(accum_batch_size) / (batch_size_per_device * num_gpus)))
+  solver_mode = P.Solver.GPU
+  device_id = int(gpulist[0])
+
+if normalization_mode == P.Loss.NONE:
+  base_lr /= batch_size_per_device
+elif normalization_mode == P.Loss.VALID:
+  base_lr *= 25. / loc_weight
+elif normalization_mode == P.Loss.FULL:
+  # Roughly there are 2000 prior bboxes per image.
+  # TODO(weiliu89): Estimate the exact # of priors.
+  base_lr *= 2000.
+
+# Evaluate on whole test set.
+num_test_image = 10991
+test_batch_size = 1
+test_iter = num_test_image / test_batch_size
+
+solver_param = {
+    # Train parameters
+    'base_lr': base_lr,
+    'weight_decay': 0.0005,
+    'lr_policy': "multistep",
+    'stepvalue': [160000, 200000, 240000],
+    'gamma': 0.1,
+    'momentum': 0.9,
+    'iter_size': iter_size,
+    'max_iter': 240000,
+    'snapshot': 10000,
+    'display': 10,
+    'average_loss': 10,
+    'type': "SGD",
+    'solver_mode': solver_mode,
+    'device_id': device_id,
+    'debug_info': False,
+    'snapshot_after_train': True,
+    # # Test parameters
+    # 'test_iter': [test_iter],
+    # 'test_interval': 10000,
+    # 'eval_type': "detection",
+    # 'ap_version': "11point",
+    # 'test_initialization': False,
+    }
+
+# parameters for generating detection output.
+det_out_param = {
+    'num_classes': num_classes,
+    'share_location': share_location,
+    'background_label_id': background_label_id,
+    'nms_param': {'nms_threshold': 0.45, 'top_k': 1000},
+    'keep_top_k': 500,
+    'confidence_threshold': 0.01,
+    'code_type': code_type,
+    'objectness_score': 0.01,
+    }
+
+# parameters for evaluating detection results.
+det_eval_param = {
+    'num_classes': num_classes,
+    'background_label_id': background_label_id,
+    'overlap_threshold': 0.5,
+    'evaluate_difficult_gt': False,
+    'name_size_file': name_size_file,
+    }
+
+### Hopefully you don't need to change the following ###
+# Check file.
+check_if_exist(train_data)
+#check_if_exist(test_data)
+check_if_exist(label_map_file)
+check_if_exist(pretrain_model)
+make_if_not_exist(save_dir)
+make_if_not_exist(job_dir)
+make_if_not_exist(snapshot_dir)
+
+# Create train net.
+net = caffe.NetSpec()
+net.data, net.label = CreateAnnotatedDataLayer(train_data, batch_size=batch_size_per_device,
+        train=True, output_label=True, label_map_file=label_map_file,
+        transform_param=train_transform_param, batch_sampler=batch_sampler)
+
+VGGNetBody(net, from_layer='data', fully_conv=True, reduced=True, dilated=False, dropout=False)
+
+AddExtraLayers(net, use_batchnorm, arm_source_layers, normalizations, lr_mult=lr_mult)
+arm_source_layers.reverse()
+normalizations.reverse()
+
+mbox_layers = CreateRefineDetHead(net, data_layer='data', from_layers=arm_source_layers,
+        use_batchnorm=use_batchnorm, min_sizes=min_sizes, max_sizes=max_sizes,
+        aspect_ratios=aspect_ratios, steps=steps, normalizations=[],
+        num_classes=num_classes, share_location=share_location, flip=flip, clip=clip,
+        prior_variance=prior_variance, kernel_size=3, pad=1, lr_mult=lr_mult, from_layers2=odm_source_layers)
+
+name = "arm_loss"
+mbox_layers_arm = []
+mbox_layers_arm.append(mbox_layers[0])
+mbox_layers_arm.append(mbox_layers[1])
+mbox_layers_arm.append(mbox_layers[2])
+mbox_layers_arm.append(net.label)
+multibox_loss_param_arm = multibox_loss_param.copy()
+multibox_loss_param_arm['num_classes'] = 2
+net[name] = L.MultiBoxLoss(*mbox_layers_arm, multibox_loss_param=multibox_loss_param_arm,
+        loss_param=loss_param, include=dict(phase=caffe_pb2.Phase.Value('TRAIN')),
+        propagate_down=[True, True, False, False])
+
+# Create the MultiBoxLossLayer.
+conf_name = "arm_conf"
+reshape_name = "{}_reshape".format(conf_name)
+net[reshape_name] = L.Reshape(net[conf_name], shape=dict(dim=[0, -1, 2]))
+softmax_name = "{}_softmax".format(conf_name)
+net[softmax_name] = L.Softmax(net[reshape_name], axis=2)
+flatten_name = "{}_flatten".format(conf_name)
+net[flatten_name] = L.Flatten(net[softmax_name], axis=1)
+
+name = "odm_loss"
+mbox_layers_odm = []
+mbox_layers_odm.append(mbox_layers[3])
+mbox_layers_odm.append(mbox_layers[4])
+mbox_layers_odm.append(mbox_layers[2])
+mbox_layers_odm.append(net.label)
+mbox_layers_odm.append(net[flatten_name])
+mbox_layers_odm.append(mbox_layers[0])
+net[name] = L.MultiBoxLoss(*mbox_layers_odm, multibox_loss_param=multibox_loss_param,
+        loss_param=loss_param, include=dict(phase=caffe_pb2.Phase.Value('TRAIN')),
+        propagate_down=[True, True, False, False, False, False])
+
+
+with open(train_net_file, 'w') as f:
+    print('name: "{}_train"'.format(model_name), file=f)
+    print(net.to_proto(), file=f)
+shutil.copy(train_net_file, job_dir)
+
+# Create test net.
+net = caffe.NetSpec()
+net.data, net.label = CreateAnnotatedDataLayer(test_data, batch_size=test_batch_size,
+        train=False, output_label=True, label_map_file=label_map_file,
+        transform_param=test_transform_param)
+
+VGGNetBody(net, from_layer='data', fully_conv=True, reduced=True, dilated=False, dropout=False)
+
+arm_source_layers = ['conv4_3', 'conv5_3', 'fc7', 'conv6_2']
+AddExtraLayers(net, use_batchnorm, arm_source_layers, normalizations, lr_mult=lr_mult)
+arm_source_layers.reverse()
+normalizations.reverse()
+
+mbox_layers = CreateRefineDetHead(net, data_layer='data', from_layers=arm_source_layers,
+        use_batchnorm=use_batchnorm, min_sizes=min_sizes, max_sizes=max_sizes,
+        aspect_ratios=aspect_ratios, steps=steps, normalizations=[],
+        num_classes=num_classes, share_location=share_location, flip=flip, clip=clip,
+        prior_variance=prior_variance, kernel_size=3, pad=1, lr_mult=lr_mult, from_layers2=odm_source_layers)
+
+mbox_layers_out = []
+mbox_layers_out.append(mbox_layers[3])
+mbox_layers_out.append(mbox_layers[4])
+mbox_layers_out.append(mbox_layers[2])
+mbox_layers_out.append(mbox_layers[1])
+mbox_layers_out.append(mbox_layers[0])
+
+conf_name = "arm_conf"
+reshape_name = "{}_reshape".format(conf_name)
+net[reshape_name] = L.Reshape(net[conf_name], shape=dict(dim=[0, -1, 2]))
+softmax_name = "{}_softmax".format(conf_name)
+net[softmax_name] = L.Softmax(net[reshape_name], axis=2)
+flatten_name = "{}_flatten".format(conf_name)
+net[flatten_name] = L.Flatten(net[softmax_name], axis=1)
+mbox_layers_out[3] = net[flatten_name]
+
+conf_name = "odm_conf"
+reshape_name = "{}_reshape".format(conf_name)
+net[reshape_name] = L.Reshape(net[conf_name], shape=dict(dim=[0, -1, num_classes]))
+softmax_name = "{}_softmax".format(conf_name)
+net[softmax_name] = L.Softmax(net[reshape_name], axis=2)
+flatten_name = "{}_flatten".format(conf_name)
+net[flatten_name] = L.Flatten(net[softmax_name], axis=1)
+mbox_layers_out[1] = net[flatten_name]
+
+net.detection_out = L.DetectionOutput(*mbox_layers_out,
+    detection_output_param=det_out_param,
+    include=dict(phase=caffe_pb2.Phase.Value('TEST')))
+net.detection_eval = L.DetectionEvaluate(net.detection_out, net.label,
+    detection_evaluate_param=det_eval_param,
+    include=dict(phase=caffe_pb2.Phase.Value('TEST')))
+
+with open(test_net_file, 'w') as f:
+    print('name: "{}_test"'.format(model_name), file=f)
+    print(net.to_proto(), file=f)
+shutil.copy(test_net_file, job_dir)
+
+# Create deploy net.
+# Remove the first and last layer from test net.
+deploy_net = net
+with open(deploy_net_file, 'w') as f:
+    net_param = deploy_net.to_proto()
+    # Remove the first (AnnotatedData) and last (DetectionEvaluate) layer from test net.
+    del net_param.layer[0]
+    del net_param.layer[-1]
+    net_param.name = '{}_deploy'.format(model_name)
+    net_param.input.extend(['data'])
+    net_param.input_shape.extend([
+        caffe_pb2.BlobShape(dim=[1, 3, resize_height, resize_width])])
+    print(net_param, file=f)
+shutil.copy(deploy_net_file, job_dir)
+
+# Create solver.
+solver = caffe_pb2.SolverParameter(
+        train_net=train_net_file,
+        # test_net=[test_net_file],
+        snapshot_prefix=snapshot_prefix,
+        **solver_param)
+
+with open(solver_file, 'w') as f:
+    print(solver, file=f)
+shutil.copy(solver_file, job_dir)
+
+max_iter = 0
+# Find most recent snapshot.
+for file in os.listdir(snapshot_dir):
+  if file.endswith(".solverstate"):
+    basename = os.path.splitext(file)[0]
+    iter = int(basename.split("{}_iter_".format(model_name))[1])
+    if iter > max_iter:
+      max_iter = iter
+
+train_src_param = '--weights="{}" \\\n'.format(pretrain_model)
+if resume_training:
+  if max_iter > 0:
+    train_src_param = '--snapshot="{}_iter_{}.solverstate" \\\n'.format(snapshot_prefix, max_iter)
+
+if remove_old_models:
+  # Remove any snapshots smaller than max_iter.
+  for file in os.listdir(snapshot_dir):
+    if file.endswith(".solverstate"):
+      basename = os.path.splitext(file)[0]
+      iter = int(basename.split("{}_iter_".format(model_name))[1])
+      if max_iter > iter:
+        os.remove("{}/{}".format(snapshot_dir, file))
+    if file.endswith(".caffemodel"):
+      basename = os.path.splitext(file)[0]
+      iter = int(basename.split("{}_iter_".format(model_name))[1])
+      if max_iter > iter:
+        os.remove("{}/{}".format(snapshot_dir, file))
+
+# Create job file.
+with open(job_file, 'w') as f:
+  f.write('cd {}\n'.format(caffe_root))
+  f.write('./build/tools/caffe train \\\n')
+  f.write('--solver="{}" \\\n'.format(solver_file))
+  f.write(train_src_param)
+  if solver_param['solver_mode'] == P.Solver.GPU:
+    f.write('--gpu {} 2>&1 | tee {}/{}.log\n'.format(gpus, job_dir, model_name))
+  else:
+    f.write('2>&1 | tee {}/{}.log\n'.format(job_dir, model_name))
+
+# Copy the python script to job_dir.
+py_file = os.path.abspath(__file__)
+shutil.copy(py_file, job_dir)
+
+# Run the job.
+os.chmod(job_file, stat.S_IRWXU)
+if run_soon:
+  subprocess.call(job_file, shell=True)
diff --git a/examples/refinedet/VGG16_VOC2012_512.py b/examples/refinedet/VGG16_VOC2012_512.py
new file mode 100644
index 0000000..fd20678
--- /dev/null
+++ b/examples/refinedet/VGG16_VOC2012_512.py
@@ -0,0 +1,605 @@
+from __future__ import print_function
+import sys
+sys.path.append("./python")
+import caffe
+from caffe.model_libs import *
+from google.protobuf import text_format
+
+import math
+import os
+import shutil
+import stat
+import subprocess
+
+# Add extra layers on top of a "base" network (e.g. VGGNet or ResNet).
+def AddExtraLayers(net, use_batchnorm=True, arm_source_layers=[], normalizations=[], lr_mult=1):
+    use_relu = True
+
+    # Add additional convolutional layers.
+    # 512/32: 16 x 16
+    from_layer = net.keys()[-1]
+
+    # 512/64: 8 x 8
+    out_layer = "conv6_1"
+    ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 1, 0, 1, lr_mult=lr_mult)
+
+    from_layer = out_layer
+    out_layer = "conv6_2"
+    ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 512, 3, 1, 2, lr_mult=lr_mult)
+
+    arm_source_layers.reverse()
+    normalizations.reverse()
+    num_p = 6
+    for index, layer in enumerate(arm_source_layers):
+        out_layer = layer
+        if normalizations:
+            if normalizations[index] != -1:
+                norm_name = "{}_norm".format(layer)
+                net[norm_name] = L.Normalize(net[layer], scale_filler=dict(type="constant", value=normalizations[index]),
+                    across_spatial=False, channel_shared=False)
+                out_layer = norm_name
+                arm_source_layers[index] = norm_name
+        from_layer = out_layer
+        out_layer = "TL{}_{}".format(num_p, 1)
+        ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 3, 1, 1, lr_mult=lr_mult)
+
+        if num_p == 6:
+            from_layer = out_layer
+            out_layer = "TL{}_{}".format(num_p, 2)
+            ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 3, 1, 1, lr_mult=lr_mult)
+
+            from_layer = out_layer
+            out_layer = "P{}".format(num_p)
+            ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 3, 1, 1, lr_mult=lr_mult)
+        else:
+            from_layer = out_layer
+            out_layer = "TL{}_{}".format(num_p, 2)
+            ConvBNLayer(net, from_layer, out_layer, use_batchnorm, False, 256, 3, 1, 1, lr_mult=lr_mult)
+
+            from_layer = "P{}".format(num_p+1)
+            out_layer = "P{}-up".format(num_p+1)
+            DeconvBNLayer(net, from_layer, out_layer, use_batchnorm, False, 256, 2, 0, 2, lr_mult=lr_mult)
+
+            from_layer = ["TL{}_{}".format(num_p, 2), "P{}-up".format(num_p+1)]
+            out_layer = "Elt{}".format(num_p)
+            EltwiseLayer(net, from_layer, out_layer)
+            relu_name = '{}_relu'.format(out_layer)
+            net[relu_name] = L.ReLU(net[out_layer], in_place=True)
+            out_layer = relu_name
+
+            from_layer = out_layer
+            out_layer = "P{}".format(num_p)
+            ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 3, 1, 1, lr_mult=lr_mult)
+
+        num_p = num_p - 1
+
+    return net
+
+
+### Modify the following parameters accordingly ###
+# The directory which contains the caffe code.
+# We assume you are running the script at the CAFFE_ROOT.
+caffe_root = os.getcwd()
+
+# Set true if you want to start training right after generating all files.
+run_soon = True
+# Set true if you want to load from most recently saved snapshot.
+# Otherwise, we will load from the pretrain_model defined below.
+resume_training = True
+# If true, Remove old model files.
+remove_old_models = False
+
+# The database file for training data. Created by data/VOC0712Plus/create_data.sh
+train_data = "examples/VOC0712Plus/VOC0712Plus_trainval_lmdb"
+# The database file for testing data. Created by data/VOC0712Plus/create_data.sh
+test_data = "examples/VOC0712Plus/VOC0712Plus_test_lmdb"
+# Specify the batch sampler.
+resize_width = 512
+resize_height = 512
+resize = "{}x{}".format(resize_width, resize_height)
+batch_sampler = [
+        {
+                'sampler': {
+                        },
+                'max_trials': 1,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.1,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.3,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.5,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.7,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.9,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'max_jaccard_overlap': 1.0,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        ]
+train_transform_param = {
+        'mirror': True,
+        'mean_value': [104, 117, 123],
+        'resize_param': {
+                'prob': 1,
+                'resize_mode': P.Resize.WARP,
+                'height': resize_height,
+                'width': resize_width,
+                'interp_mode': [
+                        P.Resize.LINEAR,
+                        P.Resize.AREA,
+                        P.Resize.NEAREST,
+                        P.Resize.CUBIC,
+                        P.Resize.LANCZOS4,
+                        ],
+                },
+        'distort_param': {
+                'brightness_prob': 0.5,
+                'brightness_delta': 32,
+                'contrast_prob': 0.5,
+                'contrast_lower': 0.5,
+                'contrast_upper': 1.5,
+                'hue_prob': 0.5,
+                'hue_delta': 18,
+                'saturation_prob': 0.5,
+                'saturation_lower': 0.5,
+                'saturation_upper': 1.5,
+                'random_order_prob': 0.0,
+                },
+        'expand_param': {
+                'prob': 0.5,
+                'max_expand_ratio': 4.0,
+                },
+        'emit_constraint': {
+            'emit_type': caffe_pb2.EmitConstraint.CENTER,
+            }
+        }
+test_transform_param = {
+        'mean_value': [104, 117, 123],
+        'resize_param': {
+                'prob': 1,
+                'resize_mode': P.Resize.WARP,
+                'height': resize_height,
+                'width': resize_width,
+                'interp_mode': [P.Resize.LINEAR],
+                },
+        }
+
+# If true, use batch norm for all newly added layers.
+# Currently only the non batch norm version has been tested.
+use_batchnorm = False
+lr_mult = 1
+# Use different initial learning rate.
+if use_batchnorm:
+    base_lr = 0.0004
+else:
+    # A learning rate for batch_size = 1, num_gpus = 1.
+    base_lr = 0.00004
+
+# Modify the job name if you want.
+job_name = "refinedet_vgg16_{}".format(resize)
+# The name of the model. Modify it if you want.
+model_name = "VOC0712Plus_{}".format(job_name)
+
+# Directory which stores the model .prototxt file.
+save_dir = "models/VGGNet/VOC0712Plus/{}".format(job_name)
+# Directory which stores the snapshot of models.
+snapshot_dir = "models/VGGNet/VOC0712Plus/{}".format(job_name)
+# Directory which stores the job script and log file.
+job_dir = "jobs/VGGNet/VOC0712Plus/{}".format(job_name)
+
+# model definition files.
+train_net_file = "{}/train.prototxt".format(save_dir)
+test_net_file = "{}/test.prototxt".format(save_dir)
+deploy_net_file = "{}/deploy.prototxt".format(save_dir)
+solver_file = "{}/solver.prototxt".format(save_dir)
+# snapshot prefix.
+snapshot_prefix = "{}/{}".format(snapshot_dir, model_name)
+# job script path.
+job_file = "{}/{}.sh".format(job_dir, model_name)
+
+# Stores the test image names and sizes. Created by data/VOC0712Plus/create_list.sh
+name_size_file = "data/VOC0712Plus/test_name_size.txt"
+# The pretrained model. We use the Fully convolutional reduced (atrous) VGGNet.
+pretrain_model = "models/VGGNet/VGG_ILSVRC_16_layers_fc_reduced.caffemodel"
+# Stores LabelMapItem.
+label_map_file = "data/VOC0712Plus/labelmap_voc.prototxt"
+
+# MultiBoxLoss parameters.
+num_classes = 21
+share_location = True
+background_label_id = 0
+train_on_diff_gt = True
+normalization_mode = P.Loss.VALID
+code_type = P.PriorBox.CENTER_SIZE
+ignore_cross_boundary_bbox = False
+mining_type = P.MultiBoxLoss.MAX_NEGATIVE
+neg_pos_ratio = 3.
+loc_weight = (neg_pos_ratio + 1.) / 4.
+multibox_loss_param = {
+    'loc_loss_type': P.MultiBoxLoss.SMOOTH_L1,
+    'conf_loss_type': P.MultiBoxLoss.SOFTMAX,
+    'loc_weight': loc_weight,
+    'num_classes': num_classes,
+    'share_location': share_location,
+    'match_type': P.MultiBoxLoss.PER_PREDICTION,
+    'overlap_threshold': 0.5,
+    'use_prior_for_matching': True,
+    'background_label_id': background_label_id,
+    'use_difficult_gt': train_on_diff_gt,
+    'mining_type': mining_type,
+    'neg_pos_ratio': neg_pos_ratio,
+    'neg_overlap': 0.5,
+    'code_type': code_type,
+    'ignore_cross_boundary_bbox': ignore_cross_boundary_bbox,
+    'objectness_score': 0.01,
+    }
+loss_param = {
+    'normalization': normalization_mode,
+    }
+
+# parameters for generating priors.
+# minimum dimension of input image
+# min_dim = 512
+# conv4_3 ==> 64 x 64
+# conv5_3 ==> 32 x 32
+# fc7 ==> 16 x 16
+# conv6_2 ==> 8 x 8
+arm_source_layers = ['conv4_3', 'conv5_3', 'fc7', 'conv6_2']
+odm_source_layers = ['P3', 'P4', 'P5', 'P6']
+min_sizes = [32, 64, 128, 256]
+max_sizes = [[], [], [], []]
+steps = [8, 16, 32, 64]
+aspect_ratios = [[2], [2], [2], [2]]
+# L2 normalize conv4_3 and conv5_3.
+normalizations = [10, 8, -1, -1]
+# variance used to encode/decode prior bboxes.
+if code_type == P.PriorBox.CENTER_SIZE:
+  prior_variance = [0.1, 0.1, 0.2, 0.2]
+else:
+  prior_variance = [0.1]
+flip = True
+clip = False
+
+# Solver parameters.
+# Defining which GPUs to use.
+gpus = "0,1,2,3"
+gpulist = gpus.split(",")
+num_gpus = len(gpulist)
+
+# Divide the mini-batch to different GPUs.
+batch_size = 32
+accum_batch_size = 32
+iter_size = accum_batch_size / batch_size
+solver_mode = P.Solver.CPU
+device_id = 0
+batch_size_per_device = batch_size
+if num_gpus > 0:
+  batch_size_per_device = int(math.ceil(float(batch_size) / num_gpus))
+  iter_size = int(math.ceil(float(accum_batch_size) / (batch_size_per_device * num_gpus)))
+  solver_mode = P.Solver.GPU
+  device_id = int(gpulist[0])
+
+if normalization_mode == P.Loss.NONE:
+  base_lr /= batch_size_per_device
+elif normalization_mode == P.Loss.VALID:
+  base_lr *= 25. / loc_weight
+elif normalization_mode == P.Loss.FULL:
+  # Roughly there are 2000 prior bboxes per image.
+  # TODO(weiliu89): Estimate the exact # of priors.
+  base_lr *= 2000.
+
+# Evaluate on whole test set.
+num_test_image = 10991
+test_batch_size = 1
+test_iter = num_test_image / test_batch_size
+
+solver_param = {
+    # Train parameters
+    'base_lr': base_lr,
+    'weight_decay': 0.0005,
+    'lr_policy': "multistep",
+    'stepvalue': [160000, 200000, 240000],
+    'gamma': 0.1,
+    'momentum': 0.9,
+    'iter_size': iter_size,
+    'max_iter': 240000,
+    'snapshot': 10000,
+    'display': 10,
+    'average_loss': 10,
+    'type': "SGD",
+    'solver_mode': solver_mode,
+    'device_id': device_id,
+    'debug_info': False,
+    'snapshot_after_train': True,
+    # # Test parameters
+    # 'test_iter': [test_iter],
+    # 'test_interval': 10000,
+    # 'eval_type': "detection",
+    # 'ap_version': "11point",
+    # 'test_initialization': False,
+    }
+
+# parameters for generating detection output.
+det_out_param = {
+    'num_classes': num_classes,
+    'share_location': share_location,
+    'background_label_id': background_label_id,
+    'nms_param': {'nms_threshold': 0.45, 'top_k': 1000},
+    'keep_top_k': 500,
+    'confidence_threshold': 0.01,
+    'code_type': code_type,
+    'objectness_score': 0.01,
+    }
+
+# parameters for evaluating detection results.
+det_eval_param = {
+    'num_classes': num_classes,
+    'background_label_id': background_label_id,
+    'overlap_threshold': 0.5,
+    'evaluate_difficult_gt': False,
+    'name_size_file': name_size_file,
+    }
+
+### Hopefully you don't need to change the following ###
+# Check file.
+check_if_exist(train_data)
+#check_if_exist(test_data)
+check_if_exist(label_map_file)
+check_if_exist(pretrain_model)
+make_if_not_exist(save_dir)
+make_if_not_exist(job_dir)
+make_if_not_exist(snapshot_dir)
+
+# Create train net.
+net = caffe.NetSpec()
+net.data, net.label = CreateAnnotatedDataLayer(train_data, batch_size=batch_size_per_device,
+        train=True, output_label=True, label_map_file=label_map_file,
+        transform_param=train_transform_param, batch_sampler=batch_sampler)
+
+VGGNetBody(net, from_layer='data', fully_conv=True, reduced=True, dilated=False, dropout=False)
+
+AddExtraLayers(net, use_batchnorm, arm_source_layers, normalizations, lr_mult=lr_mult)
+arm_source_layers.reverse()
+normalizations.reverse()
+
+mbox_layers = CreateRefineDetHead(net, data_layer='data', from_layers=arm_source_layers,
+        use_batchnorm=use_batchnorm, min_sizes=min_sizes, max_sizes=max_sizes,
+        aspect_ratios=aspect_ratios, steps=steps, normalizations=[],
+        num_classes=num_classes, share_location=share_location, flip=flip, clip=clip,
+        prior_variance=prior_variance, kernel_size=3, pad=1, lr_mult=lr_mult, from_layers2=odm_source_layers)
+
+name = "arm_loss"
+mbox_layers_arm = []
+mbox_layers_arm.append(mbox_layers[0])
+mbox_layers_arm.append(mbox_layers[1])
+mbox_layers_arm.append(mbox_layers[2])
+mbox_layers_arm.append(net.label)
+multibox_loss_param_arm = multibox_loss_param.copy()
+multibox_loss_param_arm['num_classes'] = 2
+net[name] = L.MultiBoxLoss(*mbox_layers_arm, multibox_loss_param=multibox_loss_param_arm,
+        loss_param=loss_param, include=dict(phase=caffe_pb2.Phase.Value('TRAIN')),
+        propagate_down=[True, True, False, False])
+
+# Create the MultiBoxLossLayer.
+conf_name = "arm_conf"
+reshape_name = "{}_reshape".format(conf_name)
+net[reshape_name] = L.Reshape(net[conf_name], shape=dict(dim=[0, -1, 2]))
+softmax_name = "{}_softmax".format(conf_name)
+net[softmax_name] = L.Softmax(net[reshape_name], axis=2)
+flatten_name = "{}_flatten".format(conf_name)
+net[flatten_name] = L.Flatten(net[softmax_name], axis=1)
+
+name = "odm_loss"
+mbox_layers_odm = []
+mbox_layers_odm.append(mbox_layers[3])
+mbox_layers_odm.append(mbox_layers[4])
+mbox_layers_odm.append(mbox_layers[2])
+mbox_layers_odm.append(net.label)
+mbox_layers_odm.append(net[flatten_name])
+mbox_layers_odm.append(mbox_layers[0])
+net[name] = L.MultiBoxLoss(*mbox_layers_odm, multibox_loss_param=multibox_loss_param,
+        loss_param=loss_param, include=dict(phase=caffe_pb2.Phase.Value('TRAIN')),
+        propagate_down=[True, True, False, False, False, False])
+
+
+with open(train_net_file, 'w') as f:
+    print('name: "{}_train"'.format(model_name), file=f)
+    print(net.to_proto(), file=f)
+shutil.copy(train_net_file, job_dir)
+
+# Create test net.
+net = caffe.NetSpec()
+net.data, net.label = CreateAnnotatedDataLayer(test_data, batch_size=test_batch_size,
+        train=False, output_label=True, label_map_file=label_map_file,
+        transform_param=test_transform_param)
+
+VGGNetBody(net, from_layer='data', fully_conv=True, reduced=True, dilated=False, dropout=False)
+
+arm_source_layers = ['conv4_3', 'conv5_3', 'fc7', 'conv6_2']
+AddExtraLayers(net, use_batchnorm, arm_source_layers, normalizations, lr_mult=lr_mult)
+arm_source_layers.reverse()
+normalizations.reverse()
+
+mbox_layers = CreateRefineDetHead(net, data_layer='data', from_layers=arm_source_layers,
+        use_batchnorm=use_batchnorm, min_sizes=min_sizes, max_sizes=max_sizes,
+        aspect_ratios=aspect_ratios, steps=steps, normalizations=[],
+        num_classes=num_classes, share_location=share_location, flip=flip, clip=clip,
+        prior_variance=prior_variance, kernel_size=3, pad=1, lr_mult=lr_mult, from_layers2=odm_source_layers)
+
+mbox_layers_out = []
+mbox_layers_out.append(mbox_layers[3])
+mbox_layers_out.append(mbox_layers[4])
+mbox_layers_out.append(mbox_layers[2])
+mbox_layers_out.append(mbox_layers[1])
+mbox_layers_out.append(mbox_layers[0])
+
+conf_name = "arm_conf"
+reshape_name = "{}_reshape".format(conf_name)
+net[reshape_name] = L.Reshape(net[conf_name], shape=dict(dim=[0, -1, 2]))
+softmax_name = "{}_softmax".format(conf_name)
+net[softmax_name] = L.Softmax(net[reshape_name], axis=2)
+flatten_name = "{}_flatten".format(conf_name)
+net[flatten_name] = L.Flatten(net[softmax_name], axis=1)
+mbox_layers_out[3] = net[flatten_name]
+
+conf_name = "odm_conf"
+reshape_name = "{}_reshape".format(conf_name)
+net[reshape_name] = L.Reshape(net[conf_name], shape=dict(dim=[0, -1, num_classes]))
+softmax_name = "{}_softmax".format(conf_name)
+net[softmax_name] = L.Softmax(net[reshape_name], axis=2)
+flatten_name = "{}_flatten".format(conf_name)
+net[flatten_name] = L.Flatten(net[softmax_name], axis=1)
+mbox_layers_out[1] = net[flatten_name]
+
+net.detection_out = L.DetectionOutput(*mbox_layers_out,
+    detection_output_param=det_out_param,
+    include=dict(phase=caffe_pb2.Phase.Value('TEST')))
+net.detection_eval = L.DetectionEvaluate(net.detection_out, net.label,
+    detection_evaluate_param=det_eval_param,
+    include=dict(phase=caffe_pb2.Phase.Value('TEST')))
+
+with open(test_net_file, 'w') as f:
+    print('name: "{}_test"'.format(model_name), file=f)
+    print(net.to_proto(), file=f)
+shutil.copy(test_net_file, job_dir)
+
+# Create deploy net.
+# Remove the first and last layer from test net.
+deploy_net = net
+with open(deploy_net_file, 'w') as f:
+    net_param = deploy_net.to_proto()
+    # Remove the first (AnnotatedData) and last (DetectionEvaluate) layer from test net.
+    del net_param.layer[0]
+    del net_param.layer[-1]
+    net_param.name = '{}_deploy'.format(model_name)
+    net_param.input.extend(['data'])
+    net_param.input_shape.extend([
+        caffe_pb2.BlobShape(dim=[1, 3, resize_height, resize_width])])
+    print(net_param, file=f)
+shutil.copy(deploy_net_file, job_dir)
+
+# Create solver.
+solver = caffe_pb2.SolverParameter(
+        train_net=train_net_file,
+        # test_net=[test_net_file],
+        snapshot_prefix=snapshot_prefix,
+        **solver_param)
+
+with open(solver_file, 'w') as f:
+    print(solver, file=f)
+shutil.copy(solver_file, job_dir)
+
+max_iter = 0
+# Find most recent snapshot.
+for file in os.listdir(snapshot_dir):
+  if file.endswith(".solverstate"):
+    basename = os.path.splitext(file)[0]
+    iter = int(basename.split("{}_iter_".format(model_name))[1])
+    if iter > max_iter:
+      max_iter = iter
+
+train_src_param = '--weights="{}" \\\n'.format(pretrain_model)
+if resume_training:
+  if max_iter > 0:
+    train_src_param = '--snapshot="{}_iter_{}.solverstate" \\\n'.format(snapshot_prefix, max_iter)
+
+if remove_old_models:
+  # Remove any snapshots smaller than max_iter.
+  for file in os.listdir(snapshot_dir):
+    if file.endswith(".solverstate"):
+      basename = os.path.splitext(file)[0]
+      iter = int(basename.split("{}_iter_".format(model_name))[1])
+      if max_iter > iter:
+        os.remove("{}/{}".format(snapshot_dir, file))
+    if file.endswith(".caffemodel"):
+      basename = os.path.splitext(file)[0]
+      iter = int(basename.split("{}_iter_".format(model_name))[1])
+      if max_iter > iter:
+        os.remove("{}/{}".format(snapshot_dir, file))
+
+# Create job file.
+with open(job_file, 'w') as f:
+  f.write('cd {}\n'.format(caffe_root))
+  f.write('./build/tools/caffe train \\\n')
+  f.write('--solver="{}" \\\n'.format(solver_file))
+  f.write(train_src_param)
+  if solver_param['solver_mode'] == P.Solver.GPU:
+    f.write('--gpu {} 2>&1 | tee {}/{}.log\n'.format(gpus, job_dir, model_name))
+  else:
+    f.write('2>&1 | tee {}/{}.log\n'.format(job_dir, model_name))
+
+# Copy the python script to job_dir.
+py_file = os.path.abspath(__file__)
+shutil.copy(py_file, job_dir)
+
+# Run the job.
+os.chmod(job_file, stat.S_IRWXU)
+if run_soon:
+  subprocess.call(job_file, shell=True)
diff --git a/examples/refinedet/VGG16_bdd100k_320.py b/examples/refinedet/VGG16_bdd100k_320.py
new file mode 100644
index 0000000..51e11bf
--- /dev/null
+++ b/examples/refinedet/VGG16_bdd100k_320.py
@@ -0,0 +1,607 @@
+from __future__ import print_function
+import sys
+sys.path.append("./python")
+import caffe
+from caffe.model_libs import *
+from google.protobuf import text_format
+
+import math
+import os
+import shutil
+import stat
+import subprocess
+
+# Add extra layers on top of a "base" network (e.g. VGGNet or ResNet).
+def AddExtraLayers(net, use_batchnorm=True, arm_source_layers=[], normalizations=[], lr_mult=1):
+    use_relu = True
+
+    # Add additional convolutional layers.
+    # 320/32: 10 x 10
+    from_layer = net.keys()[-1]
+
+    # 320/64: 5 x 5
+    out_layer = "conv6_1"
+    ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 1, 0, 1, lr_mult=lr_mult)
+
+    from_layer = out_layer
+    out_layer = "conv6_2"
+    ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 512, 3, 1, 2, lr_mult=lr_mult)
+
+    arm_source_layers.reverse()
+    normalizations.reverse()
+    num_p = 6
+    for index, layer in enumerate(arm_source_layers):
+        out_layer = layer
+        if normalizations:
+            if normalizations[index] != -1:
+                norm_name = "{}_norm".format(layer)
+                net[norm_name] = L.Normalize(net[layer], scale_filler=dict(type="constant", value=normalizations[index]),
+                    across_spatial=False, channel_shared=False)
+                out_layer = norm_name
+                arm_source_layers[index] = norm_name
+        from_layer = out_layer
+        out_layer = "TL{}_{}".format(num_p, 1)
+        ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 3, 1, 1, lr_mult=lr_mult)
+
+        if num_p == 6:
+            from_layer = out_layer
+            out_layer = "TL{}_{}".format(num_p, 2)
+            ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 3, 1, 1, lr_mult=lr_mult)
+
+            from_layer = out_layer
+            out_layer = "P{}".format(num_p)
+            ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 3, 1, 1, lr_mult=lr_mult)
+        else:
+            from_layer = out_layer
+            out_layer = "TL{}_{}".format(num_p, 2)
+            ConvBNLayer(net, from_layer, out_layer, use_batchnorm, False, 256, 3, 1, 1, lr_mult=lr_mult)
+
+            from_layer = "P{}".format(num_p+1)
+            out_layer = "P{}-up".format(num_p+1)
+            DeconvBNLayer(net, from_layer, out_layer, use_batchnorm, False, 256, 2, 0, 2, lr_mult=lr_mult)
+
+            from_layer = ["TL{}_{}".format(num_p, 2), "P{}-up".format(num_p+1)]
+            out_layer = "Elt{}".format(num_p)
+            EltwiseLayer(net, from_layer, out_layer)
+            relu_name = '{}_relu'.format(out_layer)
+            net[relu_name] = L.ReLU(net[out_layer], in_place=True)
+            out_layer = relu_name
+
+            from_layer = out_layer
+            out_layer = "P{}".format(num_p)
+            ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 3, 1, 1, lr_mult=lr_mult)
+
+        num_p = num_p - 1
+
+    return net
+
+
+### Modify the following parameters accordingly ###
+# The directory which contains the caffe code.
+# We assume you are running the script at the CAFFE_ROOT.
+caffe_root = os.getcwd()
+
+# Set true if you want to start training right after generating all files.
+run_soon = True
+# Set true if you want to load from most recently saved snapshot.
+# Otherwise, we will load from the pretrain_model defined below.
+resume_training = True
+# If true, Remove old model files.
+remove_old_models = False
+
+# The database file for training data. Created by data/bdd100k/create_data.sh
+train_data = "examples/bdd100k/bdd100k_train_lmdb"
+# The database file for testing data. Created by data/bdd100k/create_data.sh
+test_data = "examples/bdd100k/bdd100k_val_lmdb"
+# Specify the batch sampler.
+resize_width = 320
+resize_height = 320
+resize = "{}x{}".format(resize_width, resize_height)
+batch_sampler = [
+        {
+                'sampler': {
+                        },
+                'max_trials': 1,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.1,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.3,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.5,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.7,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.9,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'max_jaccard_overlap': 1.0,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        ]
+train_transform_param = {
+        'mirror': True,
+        'mean_value': [104, 117, 123],
+        'force_color': True,
+        'resize_param': {
+                'prob': 1,
+                'resize_mode': P.Resize.WARP,
+                'height': resize_height,
+                'width': resize_width,
+                'interp_mode': [
+                        P.Resize.LINEAR,
+                        P.Resize.AREA,
+                        P.Resize.NEAREST,
+                        P.Resize.CUBIC,
+                        P.Resize.LANCZOS4,
+                        ],
+                },
+        'distort_param': {
+                'brightness_prob': 0.5,
+                'brightness_delta': 32,
+                'contrast_prob': 0.5,
+                'contrast_lower': 0.5,
+                'contrast_upper': 1.5,
+                'hue_prob': 0.5,
+                'hue_delta': 18,
+                'saturation_prob': 0.5,
+                'saturation_lower': 0.5,
+                'saturation_upper': 1.5,
+                'random_order_prob': 0.0,
+                },
+        'expand_param': {
+                'prob': 0.5,
+                'max_expand_ratio': 4.0,
+                },
+        'emit_constraint': {
+            'emit_type': caffe_pb2.EmitConstraint.CENTER,
+            }
+        }
+test_transform_param = {
+        'mean_value': [104, 117, 123],
+        'force_color': True,
+        'resize_param': {
+                'prob': 1,
+                'resize_mode': P.Resize.WARP,
+                'height': resize_height,
+                'width': resize_width,
+                'interp_mode': [P.Resize.LINEAR],
+                },
+        }
+
+# If true, use batch norm for all newly added layers.
+# Currently only the non batch norm version has been tested.
+use_batchnorm = False
+lr_mult = 1
+# Use different initial learning rate.
+if use_batchnorm:
+    base_lr = 0.0004
+else:
+    # A learning rate for batch_size = 1, num_gpus = 1.
+    base_lr = 0.00004
+
+# Modify the job name if you want.
+job_name = "refinedet_vgg16_{}".format(resize)
+# The name of the model. Modify it if you want.
+model_name = "bdd100k_{}".format(job_name)
+
+# Directory which stores the model .prototxt file.
+save_dir = "models/VGGNet/bdd100k/{}".format(job_name)
+# Directory which stores the snapshot of models.
+snapshot_dir = "models/VGGNet/bdd100k/{}".format(job_name)
+# Directory which stores the job script and log file.
+job_dir = "jobs/VGGNet/bdd100k/{}".format(job_name)
+
+# model definition files.
+train_net_file = "{}/train.prototxt".format(save_dir)
+test_net_file = "{}/test.prototxt".format(save_dir)
+deploy_net_file = "{}/deploy.prototxt".format(save_dir)
+solver_file = "{}/solver.prototxt".format(save_dir)
+# snapshot prefix.
+snapshot_prefix = "{}/{}".format(snapshot_dir, model_name)
+# job script path.
+job_file = "{}/{}.sh".format(job_dir, model_name)
+
+# Stores the test image names and sizes. Created by data/bdd100k/create_list.sh
+name_size_file = "data/bdd100k/val_name_size.txt"
+# The pretrained model. We use the Fully convolutional reduced (atrous) VGGNet.
+pretrain_model = "models/VGGNet/VGG_ILSVRC_16_layers_fc_reduced.caffemodel"
+# Stores LabelMapItem.
+label_map_file = "data/bdd100k/labelmap_bdd100k.prototxt"
+
+# MultiBoxLoss parameters.
+num_classes = 11
+share_location = True
+background_label_id = 0
+train_on_diff_gt = False
+normalization_mode = P.Loss.VALID
+code_type = P.PriorBox.CENTER_SIZE
+ignore_cross_boundary_bbox = False
+mining_type = P.MultiBoxLoss.MAX_NEGATIVE
+neg_pos_ratio = 3.
+loc_weight = (neg_pos_ratio + 1.) / 4.
+multibox_loss_param = {
+    'loc_loss_type': P.MultiBoxLoss.SMOOTH_L1,
+    'conf_loss_type': P.MultiBoxLoss.SOFTMAX,
+    'loc_weight': loc_weight,
+    'num_classes': num_classes,
+    'share_location': share_location,
+    'match_type': P.MultiBoxLoss.PER_PREDICTION,
+    'overlap_threshold': 0.5,
+    'use_prior_for_matching': True,
+    'background_label_id': background_label_id,
+    'use_difficult_gt': train_on_diff_gt,
+    'mining_type': mining_type,
+    'neg_pos_ratio': neg_pos_ratio,
+    'neg_overlap': 0.5,
+    'code_type': code_type,
+    'ignore_cross_boundary_bbox': ignore_cross_boundary_bbox,
+    'objectness_score': 0.01,
+    }
+loss_param = {
+    'normalization': normalization_mode,
+    }
+
+# parameters for generating priors.
+# minimum dimension of input image
+# min_dim = 320
+# conv4_3 ==> 40 x 40
+# conv5_3 ==> 20 x 20
+# fc7 ==> 10 x 10
+# conv6_2 ==> 5 x 5
+arm_source_layers = ['conv4_3', 'conv5_3', 'fc7', 'conv6_2']
+odm_source_layers = ['P3', 'P4', 'P5', 'P6']
+min_sizes = [32, 64, 128, 256]
+max_sizes = [[], [], [], []]
+steps = [8, 16, 32, 64]
+aspect_ratios = [[2], [2], [2], [2]]
+# L2 normalize conv4_3 and conv5_3.
+normalizations = [10, 8, -1, -1]
+# variance used to encode/decode prior bboxes.
+if code_type == P.PriorBox.CENTER_SIZE:
+  prior_variance = [0.1, 0.1, 0.2, 0.2]
+else:
+  prior_variance = [0.1]
+flip = True
+clip = False
+
+# Solver parameters.
+# Defining which GPUs to use.
+gpus = "0"
+gpulist = gpus.split(",")
+num_gpus = len(gpulist)
+
+# Divide the mini-batch to different GPUs.
+batch_size = 4
+accum_batch_size = 32
+iter_size = accum_batch_size / batch_size
+solver_mode = P.Solver.CPU
+device_id = 0
+batch_size_per_device = batch_size
+if num_gpus > 0:
+  batch_size_per_device = int(math.ceil(float(batch_size) / num_gpus))
+  iter_size = int(math.ceil(float(accum_batch_size) / (batch_size_per_device * num_gpus)))
+  solver_mode = P.Solver.GPU
+  device_id = int(gpulist[0])
+
+if normalization_mode == P.Loss.NONE:
+  base_lr /= batch_size_per_device
+elif normalization_mode == P.Loss.VALID:
+  base_lr *= 25. / loc_weight
+elif normalization_mode == P.Loss.FULL:
+  # Roughly there are 2000 prior bboxes per image.
+  # TODO(weiliu89): Estimate the exact # of priors.
+  base_lr *= 2000.
+
+# Evaluate on whole test set.
+num_test_image = 5000
+test_batch_size = 1
+test_iter = num_test_image / test_batch_size
+
+solver_param = {
+    # Train parameters
+    'base_lr': base_lr,
+    'weight_decay': 0.0005,
+    'lr_policy': "multistep",
+    'stepvalue': [280000, 360000, 400000],
+    'gamma': 0.1,
+    'momentum': 0.9,
+    'iter_size': iter_size,
+    'max_iter': 400000,
+    'snapshot': 10000,
+    'display': 10,
+    'average_loss': 10,
+    'type': "SGD",
+    'solver_mode': solver_mode,
+    'device_id': device_id,
+    'debug_info': False,
+    'snapshot_after_train': True,
+    # Test parameters
+    # 'test_iter': [test_iter],
+    # 'test_interval': 10000,
+    # 'eval_type': "detection",
+    # 'ap_version': "11point",
+    # 'test_initialization': False,
+    }
+
+# parameters for generating detection output.
+det_out_param = {
+    'num_classes': num_classes,
+    'share_location': share_location,
+    'background_label_id': background_label_id,
+    'nms_param': {'nms_threshold': 0.45, 'top_k': 1000},
+    'keep_top_k': 500,
+    'confidence_threshold': 0.01,
+    'code_type': code_type,
+    'objectness_score': 0.01,
+    }
+
+# parameters for evaluating detection results.
+det_eval_param = {
+    'num_classes': num_classes,
+    'background_label_id': background_label_id,
+    'overlap_threshold': 0.5,
+    'evaluate_difficult_gt': False,
+    'name_size_file': name_size_file,
+    }
+
+### Hopefully you don't need to change the following ###
+# Check file.
+check_if_exist(train_data)
+check_if_exist(test_data)
+check_if_exist(label_map_file)
+check_if_exist(pretrain_model)
+make_if_not_exist(save_dir)
+make_if_not_exist(job_dir)
+make_if_not_exist(snapshot_dir)
+
+# Create train net.
+net = caffe.NetSpec()
+net.data, net.label = CreateAnnotatedDataLayer(train_data, batch_size=batch_size_per_device,
+        train=True, output_label=True, label_map_file=label_map_file,
+        transform_param=train_transform_param, batch_sampler=batch_sampler)
+
+VGGNetBody(net, from_layer='data', fully_conv=True, reduced=True, dilated=False, dropout=False)
+
+AddExtraLayers(net, use_batchnorm, arm_source_layers, normalizations, lr_mult=lr_mult)
+arm_source_layers.reverse()
+normalizations.reverse()
+
+mbox_layers = CreateRefineDetHead(net, data_layer='data', from_layers=arm_source_layers,
+        use_batchnorm=use_batchnorm, min_sizes=min_sizes, max_sizes=max_sizes,
+        aspect_ratios=aspect_ratios, steps=steps, normalizations=[],
+        num_classes=num_classes, share_location=share_location, flip=flip, clip=clip,
+        prior_variance=prior_variance, kernel_size=3, pad=1, lr_mult=lr_mult, from_layers2=odm_source_layers)
+
+name = "arm_loss"
+mbox_layers_arm = []
+mbox_layers_arm.append(mbox_layers[0])
+mbox_layers_arm.append(mbox_layers[1])
+mbox_layers_arm.append(mbox_layers[2])
+mbox_layers_arm.append(net.label)
+multibox_loss_param_arm = multibox_loss_param.copy()
+multibox_loss_param_arm['num_classes'] = 2
+net[name] = L.MultiBoxLoss(*mbox_layers_arm, multibox_loss_param=multibox_loss_param_arm,
+        loss_param=loss_param, include=dict(phase=caffe_pb2.Phase.Value('TRAIN')),
+        propagate_down=[True, True, False, False])
+
+# Create the MultiBoxLossLayer.
+conf_name = "arm_conf"
+reshape_name = "{}_reshape".format(conf_name)
+net[reshape_name] = L.Reshape(net[conf_name], shape=dict(dim=[0, -1, 2]))
+softmax_name = "{}_softmax".format(conf_name)
+net[softmax_name] = L.Softmax(net[reshape_name], axis=2)
+flatten_name = "{}_flatten".format(conf_name)
+net[flatten_name] = L.Flatten(net[softmax_name], axis=1)
+
+name = "odm_loss"
+mbox_layers_odm = []
+mbox_layers_odm.append(mbox_layers[3])
+mbox_layers_odm.append(mbox_layers[4])
+mbox_layers_odm.append(mbox_layers[2])
+mbox_layers_odm.append(net.label)
+mbox_layers_odm.append(net[flatten_name])
+mbox_layers_odm.append(mbox_layers[0])
+net[name] = L.MultiBoxLoss(*mbox_layers_odm, multibox_loss_param=multibox_loss_param,
+        loss_param=loss_param, include=dict(phase=caffe_pb2.Phase.Value('TRAIN')),
+        propagate_down=[True, True, False, False, False, False])
+
+
+with open(train_net_file, 'w') as f:
+    print('name: "{}_train"'.format(model_name), file=f)
+    print(net.to_proto(), file=f)
+shutil.copy(train_net_file, job_dir)
+
+# Create test net.
+net = caffe.NetSpec()
+net.data, net.label = CreateAnnotatedDataLayer(test_data, batch_size=test_batch_size,
+        train=False, output_label=True, label_map_file=label_map_file,
+        transform_param=test_transform_param)
+
+VGGNetBody(net, from_layer='data', fully_conv=True, reduced=True, dilated=False, dropout=False)
+
+arm_source_layers = ['conv4_3', 'conv5_3', 'fc7', 'conv6_2']
+AddExtraLayers(net, use_batchnorm, arm_source_layers, normalizations, lr_mult=lr_mult)
+arm_source_layers.reverse()
+normalizations.reverse()
+
+mbox_layers = CreateRefineDetHead(net, data_layer='data', from_layers=arm_source_layers,
+        use_batchnorm=use_batchnorm, min_sizes=min_sizes, max_sizes=max_sizes,
+        aspect_ratios=aspect_ratios, steps=steps, normalizations=[],
+        num_classes=num_classes, share_location=share_location, flip=flip, clip=clip,
+        prior_variance=prior_variance, kernel_size=3, pad=1, lr_mult=lr_mult, from_layers2=odm_source_layers)
+
+mbox_layers_out = []
+mbox_layers_out.append(mbox_layers[3])
+mbox_layers_out.append(mbox_layers[4])
+mbox_layers_out.append(mbox_layers[2])
+mbox_layers_out.append(mbox_layers[1])
+mbox_layers_out.append(mbox_layers[0])
+
+conf_name = "arm_conf"
+reshape_name = "{}_reshape".format(conf_name)
+net[reshape_name] = L.Reshape(net[conf_name], shape=dict(dim=[0, -1, 2]))
+softmax_name = "{}_softmax".format(conf_name)
+net[softmax_name] = L.Softmax(net[reshape_name], axis=2)
+flatten_name = "{}_flatten".format(conf_name)
+net[flatten_name] = L.Flatten(net[softmax_name], axis=1)
+mbox_layers_out[3] = net[flatten_name]
+
+conf_name = "odm_conf"
+reshape_name = "{}_reshape".format(conf_name)
+net[reshape_name] = L.Reshape(net[conf_name], shape=dict(dim=[0, -1, num_classes]))
+softmax_name = "{}_softmax".format(conf_name)
+net[softmax_name] = L.Softmax(net[reshape_name], axis=2)
+flatten_name = "{}_flatten".format(conf_name)
+net[flatten_name] = L.Flatten(net[softmax_name], axis=1)
+mbox_layers_out[1] = net[flatten_name]
+
+net.detection_out = L.DetectionOutput(*mbox_layers_out,
+    detection_output_param=det_out_param,
+    include=dict(phase=caffe_pb2.Phase.Value('TEST')))
+net.detection_eval = L.DetectionEvaluate(net.detection_out, net.label,
+    detection_evaluate_param=det_eval_param,
+    include=dict(phase=caffe_pb2.Phase.Value('TEST')))
+
+with open(test_net_file, 'w') as f:
+    print('name: "{}_test"'.format(model_name), file=f)
+    print(net.to_proto(), file=f)
+shutil.copy(test_net_file, job_dir)
+
+# Create deploy net.
+# Remove the first and last layer from test net.
+deploy_net = net
+with open(deploy_net_file, 'w') as f:
+    net_param = deploy_net.to_proto()
+    # Remove the first (AnnotatedData) and last (DetectionEvaluate) layer from test net.
+    del net_param.layer[0]
+    del net_param.layer[-1]
+    net_param.name = '{}_deploy'.format(model_name)
+    net_param.input.extend(['data'])
+    net_param.input_shape.extend([
+        caffe_pb2.BlobShape(dim=[1, 3, resize_height, resize_width])])
+    print(net_param, file=f)
+shutil.copy(deploy_net_file, job_dir)
+
+# Create solver.
+solver = caffe_pb2.SolverParameter(
+        train_net=train_net_file,
+        # test_net=[test_net_file],
+        snapshot_prefix=snapshot_prefix,
+        **solver_param)
+
+with open(solver_file, 'w') as f:
+    print(solver, file=f)
+shutil.copy(solver_file, job_dir)
+
+max_iter = 0
+# Find most recent snapshot.
+for file in os.listdir(snapshot_dir):
+  if file.endswith(".solverstate"):
+    basename = os.path.splitext(file)[0]
+    iter = int(basename.split("{}_iter_".format(model_name))[1])
+    if iter > max_iter:
+      max_iter = iter
+
+train_src_param = '--weights="{}" \\\n'.format(pretrain_model)
+if resume_training:
+  if max_iter > 0:
+    train_src_param = '--snapshot="{}_iter_{}.solverstate" \\\n'.format(snapshot_prefix, max_iter)
+
+if remove_old_models:
+  # Remove any snapshots smaller than max_iter.
+  for file in os.listdir(snapshot_dir):
+    if file.endswith(".solverstate"):
+      basename = os.path.splitext(file)[0]
+      iter = int(basename.split("{}_iter_".format(model_name))[1])
+      if max_iter > iter:
+        os.remove("{}/{}".format(snapshot_dir, file))
+    if file.endswith(".caffemodel"):
+      basename = os.path.splitext(file)[0]
+      iter = int(basename.split("{}_iter_".format(model_name))[1])
+      if max_iter > iter:
+        os.remove("{}/{}".format(snapshot_dir, file))
+
+# Create job file.
+with open(job_file, 'w') as f:
+  f.write('cd {}\n'.format(caffe_root))
+  f.write('./build/tools/caffe train \\\n')
+  f.write('--solver="{}" \\\n'.format(solver_file))
+  f.write(train_src_param)
+  if solver_param['solver_mode'] == P.Solver.GPU:
+    f.write('--gpu {} 2>&1 | tee {}/{}.log\n'.format(gpus, job_dir, model_name))
+  else:
+    f.write('2>&1 | tee {}/{}.log\n'.format(job_dir, model_name))
+
+# Copy the python script to job_dir.
+py_file = os.path.abspath(__file__)
+shutil.copy(py_file, job_dir)
+
+# Run the job.
+os.chmod(job_file, stat.S_IRWXU)
+if run_soon:
+  subprocess.call(job_file, shell=True)
diff --git a/examples/refinedet/finetune_VGG16_VOC2007_320.py b/examples/refinedet/finetune_VGG16_VOC2007_320.py
new file mode 100644
index 0000000..9228de7
--- /dev/null
+++ b/examples/refinedet/finetune_VGG16_VOC2007_320.py
@@ -0,0 +1,605 @@
+from __future__ import print_function
+import sys
+sys.path.append("./python")
+import caffe
+from caffe.model_libs import *
+from google.protobuf import text_format
+
+import math
+import os
+import shutil
+import stat
+import subprocess
+
+# Add extra layers on top of a "base" network (e.g. VGGNet or Inception).
+def AddExtraLayers(net, use_batchnorm=True, arm_source_layers=[], normalizations=[], lr_mult=1):
+    use_relu = True
+
+    # Add additional convolutional layers.
+    # 320/32: 10 x 10
+    from_layer = net.keys()[-1]
+
+    # 320/64: 5 x 5
+    out_layer = "conv6_1"
+    ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 1, 0, 1, lr_mult=lr_mult)
+
+    from_layer = out_layer
+    out_layer = "conv6_2"
+    ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 512, 3, 1, 2, lr_mult=lr_mult)
+
+    arm_source_layers.reverse()
+    normalizations.reverse()
+    num_p = 6
+    for index, layer in enumerate(arm_source_layers):
+        out_layer = layer
+        if normalizations:
+            if normalizations[index] != -1:
+                norm_name = "{}_norm".format(layer)
+                net[norm_name] = L.Normalize(net[layer], scale_filler=dict(type="constant", value=normalizations[index]),
+                    across_spatial=False, channel_shared=False)
+                out_layer = norm_name
+                arm_source_layers[index] = norm_name
+        from_layer = out_layer
+        out_layer = "TL{}_{}".format(num_p, 1)
+        ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 3, 1, 1, lr_mult=lr_mult)
+
+        if num_p == 6:
+            from_layer = out_layer
+            out_layer = "TL{}_{}".format(num_p, 2)
+            ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 3, 1, 1, lr_mult=lr_mult)
+
+            from_layer = out_layer
+            out_layer = "P{}".format(num_p)
+            ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 3, 1, 1, lr_mult=lr_mult)
+        else:
+            from_layer = out_layer
+            out_layer = "TL{}_{}".format(num_p, 2)
+            ConvBNLayer(net, from_layer, out_layer, use_batchnorm, False, 256, 3, 1, 1, lr_mult=lr_mult)
+
+            from_layer = "P{}".format(num_p+1)
+            out_layer = "P{}-up".format(num_p+1)
+            DeconvBNLayer(net, from_layer, out_layer, use_batchnorm, False, 256, 2, 0, 2, lr_mult=lr_mult)
+
+            from_layer = ["TL{}_{}".format(num_p, 2), "P{}-up".format(num_p+1)]
+            out_layer = "Elt{}".format(num_p)
+            EltwiseLayer(net, from_layer, out_layer)
+            relu_name = '{}_relu'.format(out_layer)
+            net[relu_name] = L.ReLU(net[out_layer], in_place=True)
+            out_layer = relu_name
+
+            from_layer = out_layer
+            out_layer = "P{}".format(num_p)
+            ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 3, 1, 1, lr_mult=lr_mult)
+
+        num_p = num_p - 1
+
+    return net
+
+
+### Modify the following parameters accordingly ###
+# The directory which contains the caffe code.
+# We assume you are running the script at the CAFFE_ROOT.
+caffe_root = os.getcwd()
+
+# Set true if you want to start training right after generating all files.
+run_soon = True
+# Set true if you want to load from most recently saved snapshot.
+# Otherwise, we will load from the pretrain_model defined below.
+resume_training = True
+# If true, Remove old model files.
+remove_old_models = False
+
+# The database file for training data. Created by data/VOC0712/create_data.sh
+train_data = "examples/VOC0712/VOC0712_trainval_lmdb"
+# The database file for testing data. Created by data/VOC0712/create_data.sh
+test_data = "examples/VOC0712/VOC0712_test_lmdb"
+# Specify the batch sampler.
+resize_width = 320
+resize_height = 320
+resize = "{}x{}".format(resize_width, resize_height)
+batch_sampler = [
+        {
+                'sampler': {
+                        },
+                'max_trials': 1,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.1,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.3,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.5,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.7,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.9,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'max_jaccard_overlap': 1.0,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        ]
+train_transform_param = {
+        'mirror': True,
+        'mean_value': [104, 117, 123],
+        'resize_param': {
+                'prob': 1,
+                'resize_mode': P.Resize.WARP,
+                'height': resize_height,
+                'width': resize_width,
+                'interp_mode': [
+                        P.Resize.LINEAR,
+                        P.Resize.AREA,
+                        P.Resize.NEAREST,
+                        P.Resize.CUBIC,
+                        P.Resize.LANCZOS4,
+                        ],
+                },
+        'distort_param': {
+                'brightness_prob': 0.5,
+                'brightness_delta': 32,
+                'contrast_prob': 0.5,
+                'contrast_lower': 0.5,
+                'contrast_upper': 1.5,
+                'hue_prob': 0.5,
+                'hue_delta': 18,
+                'saturation_prob': 0.5,
+                'saturation_lower': 0.5,
+                'saturation_upper': 1.5,
+                'random_order_prob': 0.0,
+                },
+        'expand_param': {
+                'prob': 0.5,
+                'max_expand_ratio': 4.0,
+                },
+        'emit_constraint': {
+            'emit_type': caffe_pb2.EmitConstraint.CENTER,
+            }
+        }
+test_transform_param = {
+        'mean_value': [104, 117, 123],
+        'resize_param': {
+                'prob': 1,
+                'resize_mode': P.Resize.WARP,
+                'height': resize_height,
+                'width': resize_width,
+                'interp_mode': [P.Resize.LINEAR],
+                },
+        }
+
+# If true, use batch norm for all newly added layers.
+# Currently only the non batch norm version has been tested.
+use_batchnorm = False
+lr_mult = 1
+# Use different initial learning rate.
+if use_batchnorm:
+    base_lr = 0.0004
+else:
+    # A learning rate for batch_size = 1, num_gpus = 1.
+    base_lr = 0.00004 * 0.05
+
+# Modify the job name if you want.
+job_name = "refinedet_vgg16_{}_ft".format(resize)
+# The name of the model. Modify it if you want.
+model_name = "VOC0712_{}".format(job_name)
+
+# Directory which stores the model .prototxt file.
+save_dir = "models/VGGNet/VOC0712/{}".format(job_name)
+# Directory which stores the snapshot of models.
+snapshot_dir = "models/VGGNet/VOC0712/{}".format(job_name)
+# Directory which stores the job script and log file.
+job_dir = "jobs/VGGNet/VOC0712/{}".format(job_name)
+
+# model definition files.
+train_net_file = "{}/train.prototxt".format(save_dir)
+test_net_file = "{}/test.prototxt".format(save_dir)
+deploy_net_file = "{}/deploy.prototxt".format(save_dir)
+solver_file = "{}/solver.prototxt".format(save_dir)
+# snapshot prefix.
+snapshot_prefix = "{}/{}".format(snapshot_dir, model_name)
+# job script path.
+job_file = "{}/{}.sh".format(job_dir, model_name)
+
+# Stores the test image names and sizes. Created by data/VOC0712/create_list.sh
+name_size_file = "data/VOC0712/test_name_size.txt"
+# The pretrained model. We use the RefineDet model trained on coco dataset.
+pretrain_model = "models/VGGNet/VOC0712/refinedet_vgg16_320x320_coco/coco_refinedet_vgg16_320x320.caffemodel"
+# Stores LabelMapItem.
+label_map_file = "data/VOC0712/labelmap_voc.prototxt"
+
+# MultiBoxLoss parameters.
+num_classes = 21
+share_location = True
+background_label_id = 0
+train_on_diff_gt = True
+normalization_mode = P.Loss.VALID
+code_type = P.PriorBox.CENTER_SIZE
+ignore_cross_boundary_bbox = False
+mining_type = P.MultiBoxLoss.MAX_NEGATIVE
+neg_pos_ratio = 3.
+loc_weight = (neg_pos_ratio + 1.) / 4.
+multibox_loss_param = {
+    'loc_loss_type': P.MultiBoxLoss.SMOOTH_L1,
+    'conf_loss_type': P.MultiBoxLoss.SOFTMAX,
+    'loc_weight': loc_weight,
+    'num_classes': num_classes,
+    'share_location': share_location,
+    'match_type': P.MultiBoxLoss.PER_PREDICTION,
+    'overlap_threshold': 0.5,
+    'use_prior_for_matching': True,
+    'background_label_id': background_label_id,
+    'use_difficult_gt': train_on_diff_gt,
+    'mining_type': mining_type,
+    'neg_pos_ratio': neg_pos_ratio,
+    'neg_overlap': 0.5,
+    'code_type': code_type,
+    'ignore_cross_boundary_bbox': ignore_cross_boundary_bbox,
+    'objectness_score': 0.01,
+    }
+loss_param = {
+    'normalization': normalization_mode,
+    }
+
+# parameters for generating priors.
+# minimum dimension of input image
+# min_dim = 320
+# conv4_3 ==> 40 x 40
+# conv5_3 ==> 20 x 20
+# fc7 ==> 10 x 10
+# conv6_2 ==> 5 x 5
+arm_source_layers = ['conv4_3', 'conv5_3', 'fc7', 'conv6_2']
+odm_source_layers = ['P3', 'P4', 'P5', 'P6']
+min_sizes = [32, 64, 128, 256]
+max_sizes = [[], [], [], []]
+steps = [8, 16, 32, 64]
+aspect_ratios = [[2], [2], [2], [2]]
+# L2 normalize conv4_3 and conv5_3.
+normalizations = [10, 8, -1, -1]
+# variance used to encode/decode prior bboxes.
+if code_type == P.PriorBox.CENTER_SIZE:
+  prior_variance = [0.1, 0.1, 0.2, 0.2]
+else:
+  prior_variance = [0.1]
+flip = True
+clip = False
+
+# Solver parameters.
+# Defining which GPUs to use.
+gpus = "0,1,2,3"
+gpulist = gpus.split(",")
+num_gpus = len(gpulist)
+
+# Divide the mini-batch to different GPUs.
+batch_size = 32
+accum_batch_size = 32
+iter_size = accum_batch_size / batch_size
+solver_mode = P.Solver.CPU
+device_id = 0
+batch_size_per_device = batch_size
+if num_gpus > 0:
+  batch_size_per_device = int(math.ceil(float(batch_size) / num_gpus))
+  iter_size = int(math.ceil(float(accum_batch_size) / (batch_size_per_device * num_gpus)))
+  solver_mode = P.Solver.GPU
+  device_id = int(gpulist[0])
+
+if normalization_mode == P.Loss.NONE:
+  base_lr /= batch_size_per_device
+elif normalization_mode == P.Loss.VALID:
+  base_lr *= 25. / loc_weight
+elif normalization_mode == P.Loss.FULL:
+  # Roughly there are 2000 prior bboxes per image.
+  # TODO(weiliu89): Estimate the exact # of priors.
+  base_lr *= 2000.
+
+# Evaluate on whole test set.
+num_test_image = 4952
+test_batch_size = 1
+test_iter = num_test_image / test_batch_size
+
+solver_param = {
+    # Train parameters
+    'base_lr': base_lr,
+    'weight_decay': 0.0005,
+    'lr_policy': "multistep",
+    'stepvalue': [80000, 120000],
+    'gamma': 0.2,
+    'momentum': 0.9,
+    'iter_size': iter_size,
+    'max_iter': 120000,
+    'snapshot': 5000,
+    'display': 10,
+    'average_loss': 10,
+    'type': "SGD",
+    'solver_mode': solver_mode,
+    'device_id': device_id,
+    'debug_info': False,
+    'snapshot_after_train': True,
+    # Test parameters
+    # 'test_iter': [test_iter],
+    # 'test_interval': 5000,
+    # 'eval_type': "detection",
+    # 'ap_version': "11point",
+    # 'test_initialization': False,
+    }
+
+# parameters for generating detection output.
+det_out_param = {
+    'num_classes': num_classes,
+    'share_location': share_location,
+    'background_label_id': background_label_id,
+    'nms_param': {'nms_threshold': 0.45, 'top_k': 1000},
+    'keep_top_k': 500,
+    'confidence_threshold': 0.01,
+    'code_type': code_type,
+    'objectness_score': 0.01,
+    }
+
+# parameters for evaluating detection results.
+det_eval_param = {
+    'num_classes': num_classes,
+    'background_label_id': background_label_id,
+    'overlap_threshold': 0.5,
+    'evaluate_difficult_gt': False,
+    'name_size_file': name_size_file,
+    }
+
+### Hopefully you don't need to change the following ###
+# Check file.
+check_if_exist(train_data)
+check_if_exist(test_data)
+check_if_exist(label_map_file)
+check_if_exist(pretrain_model)
+make_if_not_exist(save_dir)
+make_if_not_exist(job_dir)
+make_if_not_exist(snapshot_dir)
+
+# Create train net.
+net = caffe.NetSpec()
+net.data, net.label = CreateAnnotatedDataLayer(train_data, batch_size=batch_size_per_device,
+        train=True, output_label=True, label_map_file=label_map_file,
+        transform_param=train_transform_param, batch_sampler=batch_sampler)
+
+VGGNetBody(net, from_layer='data', fully_conv=True, reduced=True, dilated=False, dropout=False)
+
+AddExtraLayers(net, use_batchnorm, arm_source_layers, normalizations, lr_mult=lr_mult)
+arm_source_layers.reverse()
+normalizations.reverse()
+
+mbox_layers = CreateRefineDetHead(net, data_layer='data', from_layers=arm_source_layers,
+        use_batchnorm=use_batchnorm, min_sizes=min_sizes, max_sizes=max_sizes,
+        aspect_ratios=aspect_ratios, steps=steps, normalizations=[],
+        num_classes=num_classes, share_location=share_location, flip=flip, clip=clip,
+        prior_variance=prior_variance, kernel_size=3, pad=1, lr_mult=lr_mult, from_layers2=odm_source_layers)
+
+name = "arm_loss"
+mbox_layers_arm = []
+mbox_layers_arm.append(mbox_layers[0])
+mbox_layers_arm.append(mbox_layers[1])
+mbox_layers_arm.append(mbox_layers[2])
+mbox_layers_arm.append(net.label)
+multibox_loss_param_arm = multibox_loss_param.copy()
+multibox_loss_param_arm['num_classes'] = 2
+net[name] = L.MultiBoxLoss(*mbox_layers_arm, multibox_loss_param=multibox_loss_param_arm,
+        loss_param=loss_param, include=dict(phase=caffe_pb2.Phase.Value('TRAIN')),
+        propagate_down=[True, True, False, False])
+
+# Create the MultiBoxLossLayer.
+conf_name = "arm_conf"
+reshape_name = "{}_reshape".format(conf_name)
+net[reshape_name] = L.Reshape(net[conf_name], shape=dict(dim=[0, -1, 2]))
+softmax_name = "{}_softmax".format(conf_name)
+net[softmax_name] = L.Softmax(net[reshape_name], axis=2)
+flatten_name = "{}_flatten".format(conf_name)
+net[flatten_name] = L.Flatten(net[softmax_name], axis=1)
+
+name = "odm_loss"
+mbox_layers_odm = []
+mbox_layers_odm.append(mbox_layers[3])
+mbox_layers_odm.append(mbox_layers[4])
+mbox_layers_odm.append(mbox_layers[2])
+mbox_layers_odm.append(net.label)
+mbox_layers_odm.append(net[flatten_name])
+mbox_layers_odm.append(mbox_layers[0])
+net[name] = L.MultiBoxLoss(*mbox_layers_odm, multibox_loss_param=multibox_loss_param,
+        loss_param=loss_param, include=dict(phase=caffe_pb2.Phase.Value('TRAIN')),
+        propagate_down=[True, True, False, False, False, False])
+
+
+with open(train_net_file, 'w') as f:
+    print('name: "{}_train"'.format(model_name), file=f)
+    print(net.to_proto(), file=f)
+shutil.copy(train_net_file, job_dir)
+
+# Create test net.
+net = caffe.NetSpec()
+net.data, net.label = CreateAnnotatedDataLayer(test_data, batch_size=test_batch_size,
+        train=False, output_label=True, label_map_file=label_map_file,
+        transform_param=test_transform_param)
+
+VGGNetBody(net, from_layer='data', fully_conv=True, reduced=True, dilated=False, dropout=False)
+
+arm_source_layers = ['conv4_3', 'conv5_3', 'fc7', 'conv6_2']
+AddExtraLayers(net, use_batchnorm, arm_source_layers, normalizations, lr_mult=lr_mult)
+arm_source_layers.reverse()
+normalizations.reverse()
+
+mbox_layers = CreateRefineDetHead(net, data_layer='data', from_layers=arm_source_layers,
+        use_batchnorm=use_batchnorm, min_sizes=min_sizes, max_sizes=max_sizes,
+        aspect_ratios=aspect_ratios, steps=steps, normalizations=[],
+        num_classes=num_classes, share_location=share_location, flip=flip, clip=clip,
+        prior_variance=prior_variance, kernel_size=3, pad=1, lr_mult=lr_mult, from_layers2=odm_source_layers)
+
+mbox_layers_out = []
+mbox_layers_out.append(mbox_layers[3])
+mbox_layers_out.append(mbox_layers[4])
+mbox_layers_out.append(mbox_layers[2])
+mbox_layers_out.append(mbox_layers[1])
+mbox_layers_out.append(mbox_layers[0])
+
+conf_name = "arm_conf"
+reshape_name = "{}_reshape".format(conf_name)
+net[reshape_name] = L.Reshape(net[conf_name], shape=dict(dim=[0, -1, 2]))
+softmax_name = "{}_softmax".format(conf_name)
+net[softmax_name] = L.Softmax(net[reshape_name], axis=2)
+flatten_name = "{}_flatten".format(conf_name)
+net[flatten_name] = L.Flatten(net[softmax_name], axis=1)
+mbox_layers_out[3] = net[flatten_name]
+
+conf_name = "odm_conf"
+reshape_name = "{}_reshape".format(conf_name)
+net[reshape_name] = L.Reshape(net[conf_name], shape=dict(dim=[0, -1, num_classes]))
+softmax_name = "{}_softmax".format(conf_name)
+net[softmax_name] = L.Softmax(net[reshape_name], axis=2)
+flatten_name = "{}_flatten".format(conf_name)
+net[flatten_name] = L.Flatten(net[softmax_name], axis=1)
+mbox_layers_out[1] = net[flatten_name]
+
+net.detection_out = L.DetectionOutput(*mbox_layers_out,
+    detection_output_param=det_out_param,
+    include=dict(phase=caffe_pb2.Phase.Value('TEST')))
+net.detection_eval = L.DetectionEvaluate(net.detection_out, net.label,
+    detection_evaluate_param=det_eval_param,
+    include=dict(phase=caffe_pb2.Phase.Value('TEST')))
+
+with open(test_net_file, 'w') as f:
+    print('name: "{}_test"'.format(model_name), file=f)
+    print(net.to_proto(), file=f)
+shutil.copy(test_net_file, job_dir)
+
+# Create deploy net.
+# Remove the first and last layer from test net.
+deploy_net = net
+with open(deploy_net_file, 'w') as f:
+    net_param = deploy_net.to_proto()
+    # Remove the first (AnnotatedData) and last (DetectionEvaluate) layer from test net.
+    del net_param.layer[0]
+    del net_param.layer[-1]
+    net_param.name = '{}_deploy'.format(model_name)
+    net_param.input.extend(['data'])
+    net_param.input_shape.extend([
+        caffe_pb2.BlobShape(dim=[1, 3, resize_height, resize_width])])
+    print(net_param, file=f)
+shutil.copy(deploy_net_file, job_dir)
+
+# Create solver.
+solver = caffe_pb2.SolverParameter(
+        train_net=train_net_file,
+        # test_net=[test_net_file],
+        snapshot_prefix=snapshot_prefix,
+        **solver_param)
+
+with open(solver_file, 'w') as f:
+    print(solver, file=f)
+shutil.copy(solver_file, job_dir)
+
+max_iter = 0
+# Find most recent snapshot.
+for file in os.listdir(snapshot_dir):
+  if file.endswith(".solverstate"):
+    basename = os.path.splitext(file)[0]
+    iter = int(basename.split("{}_iter_".format(model_name))[1])
+    if iter > max_iter:
+      max_iter = iter
+
+train_src_param = '--weights="{}" \\\n'.format(pretrain_model)
+if resume_training:
+  if max_iter > 0:
+    train_src_param = '--snapshot="{}_iter_{}.solverstate" \\\n'.format(snapshot_prefix, max_iter)
+
+if remove_old_models:
+  # Remove any snapshots smaller than max_iter.
+  for file in os.listdir(snapshot_dir):
+    if file.endswith(".solverstate"):
+      basename = os.path.splitext(file)[0]
+      iter = int(basename.split("{}_iter_".format(model_name))[1])
+      if max_iter > iter:
+        os.remove("{}/{}".format(snapshot_dir, file))
+    if file.endswith(".caffemodel"):
+      basename = os.path.splitext(file)[0]
+      iter = int(basename.split("{}_iter_".format(model_name))[1])
+      if max_iter > iter:
+        os.remove("{}/{}".format(snapshot_dir, file))
+
+# Create job file.
+with open(job_file, 'w') as f:
+  f.write('cd {}\n'.format(caffe_root))
+  f.write('./build/tools/caffe train \\\n')
+  f.write('--solver="{}" \\\n'.format(solver_file))
+  f.write(train_src_param)
+  if solver_param['solver_mode'] == P.Solver.GPU:
+    f.write('--gpu {} 2>&1 | tee {}/{}.log\n'.format(gpus, job_dir, model_name))
+  else:
+    f.write('2>&1 | tee {}/{}.log\n'.format(job_dir, model_name))
+
+# Copy the python script to job_dir.
+py_file = os.path.abspath(__file__)
+shutil.copy(py_file, job_dir)
+
+# Run the job.
+os.chmod(job_file, stat.S_IRWXU)
+if run_soon:
+  subprocess.call(job_file, shell=True)
diff --git a/examples/refinedet/finetune_VGG16_VOC2007_512.py b/examples/refinedet/finetune_VGG16_VOC2007_512.py
new file mode 100644
index 0000000..da94836
--- /dev/null
+++ b/examples/refinedet/finetune_VGG16_VOC2007_512.py
@@ -0,0 +1,605 @@
+from __future__ import print_function
+import sys
+sys.path.append("./python")
+import caffe
+from caffe.model_libs import *
+from google.protobuf import text_format
+
+import math
+import os
+import shutil
+import stat
+import subprocess
+
+# Add extra layers on top of a "base" network (e.g. VGGNet or Inception).
+def AddExtraLayers(net, use_batchnorm=True, arm_source_layers=[], normalizations=[], lr_mult=1):
+    use_relu = True
+
+    # Add additional convolutional layers.
+    # 512/32: 16 x 16
+    from_layer = net.keys()[-1]
+
+    # 512/64: 8 x 8
+    out_layer = "conv6_1"
+    ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 1, 0, 1, lr_mult=lr_mult)
+
+    from_layer = out_layer
+    out_layer = "conv6_2"
+    ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 512, 3, 1, 2, lr_mult=lr_mult)
+
+    arm_source_layers.reverse()
+    normalizations.reverse()
+    num_p = 6
+    for index, layer in enumerate(arm_source_layers):
+        out_layer = layer
+        if normalizations:
+            if normalizations[index] != -1:
+                norm_name = "{}_norm".format(layer)
+                net[norm_name] = L.Normalize(net[layer], scale_filler=dict(type="constant", value=normalizations[index]),
+                    across_spatial=False, channel_shared=False)
+                out_layer = norm_name
+                arm_source_layers[index] = norm_name
+        from_layer = out_layer
+        out_layer = "TL{}_{}".format(num_p, 1)
+        ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 3, 1, 1, lr_mult=lr_mult)
+
+        if num_p == 6:
+            from_layer = out_layer
+            out_layer = "TL{}_{}".format(num_p, 2)
+            ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 3, 1, 1, lr_mult=lr_mult)
+
+            from_layer = out_layer
+            out_layer = "P{}".format(num_p)
+            ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 3, 1, 1, lr_mult=lr_mult)
+        else:
+            from_layer = out_layer
+            out_layer = "TL{}_{}".format(num_p, 2)
+            ConvBNLayer(net, from_layer, out_layer, use_batchnorm, False, 256, 3, 1, 1, lr_mult=lr_mult)
+
+            from_layer = "P{}".format(num_p+1)
+            out_layer = "P{}-up".format(num_p+1)
+            DeconvBNLayer(net, from_layer, out_layer, use_batchnorm, False, 256, 2, 0, 2, lr_mult=lr_mult)
+
+            from_layer = ["TL{}_{}".format(num_p, 2), "P{}-up".format(num_p+1)]
+            out_layer = "Elt{}".format(num_p)
+            EltwiseLayer(net, from_layer, out_layer)
+            relu_name = '{}_relu'.format(out_layer)
+            net[relu_name] = L.ReLU(net[out_layer], in_place=True)
+            out_layer = relu_name
+
+            from_layer = out_layer
+            out_layer = "P{}".format(num_p)
+            ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 3, 1, 1, lr_mult=lr_mult)
+
+        num_p = num_p - 1
+
+    return net
+
+
+### Modify the following parameters accordingly ###
+# The directory which contains the caffe code.
+# We assume you are running the script at the CAFFE_ROOT.
+caffe_root = os.getcwd()
+
+# Set true if you want to start training right after generating all files.
+run_soon = True
+# Set true if you want to load from most recently saved snapshot.
+# Otherwise, we will load from the pretrain_model defined below.
+resume_training = True
+# If true, Remove old model files.
+remove_old_models = False
+
+# The database file for training data. Created by data/VOC0712/create_data.sh
+train_data = "examples/VOC0712/VOC0712_trainval_lmdb"
+# The database file for testing data. Created by data/VOC0712/create_data.sh
+test_data = "examples/VOC0712/VOC0712_test_lmdb"
+# Specify the batch sampler.
+resize_width = 512
+resize_height = 512
+resize = "{}x{}".format(resize_width, resize_height)
+batch_sampler = [
+        {
+                'sampler': {
+                        },
+                'max_trials': 1,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.1,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.3,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.5,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.7,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.9,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'max_jaccard_overlap': 1.0,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        ]
+train_transform_param = {
+        'mirror': True,
+        'mean_value': [104, 117, 123],
+        'resize_param': {
+                'prob': 1,
+                'resize_mode': P.Resize.WARP,
+                'height': resize_height,
+                'width': resize_width,
+                'interp_mode': [
+                        P.Resize.LINEAR,
+                        P.Resize.AREA,
+                        P.Resize.NEAREST,
+                        P.Resize.CUBIC,
+                        P.Resize.LANCZOS4,
+                        ],
+                },
+        'distort_param': {
+                'brightness_prob': 0.5,
+                'brightness_delta': 32,
+                'contrast_prob': 0.5,
+                'contrast_lower': 0.5,
+                'contrast_upper': 1.5,
+                'hue_prob': 0.5,
+                'hue_delta': 18,
+                'saturation_prob': 0.5,
+                'saturation_lower': 0.5,
+                'saturation_upper': 1.5,
+                'random_order_prob': 0.0,
+                },
+        'expand_param': {
+                'prob': 0.5,
+                'max_expand_ratio': 4.0,
+                },
+        'emit_constraint': {
+            'emit_type': caffe_pb2.EmitConstraint.CENTER,
+            }
+        }
+test_transform_param = {
+        'mean_value': [104, 117, 123],
+        'resize_param': {
+                'prob': 1,
+                'resize_mode': P.Resize.WARP,
+                'height': resize_height,
+                'width': resize_width,
+                'interp_mode': [P.Resize.LINEAR],
+                },
+        }
+
+# If true, use batch norm for all newly added layers.
+# Currently only the non batch norm version has been tested.
+use_batchnorm = False
+lr_mult = 1
+# Use different initial learning rate.
+if use_batchnorm:
+    base_lr = 0.0004
+else:
+    # A learning rate for batch_size = 1, num_gpus = 1.
+    base_lr = 0.00004 * 0.05
+
+# Modify the job name if you want.
+job_name = "refinedet_vgg16_{}_ft".format(resize)
+# The name of the model. Modify it if you want.
+model_name = "VOC0712_{}".format(job_name)
+
+# Directory which stores the model .prototxt file.
+save_dir = "models/VGGNet/VOC0712/{}".format(job_name)
+# Directory which stores the snapshot of models.
+snapshot_dir = "models/VGGNet/VOC0712/{}".format(job_name)
+# Directory which stores the job script and log file.
+job_dir = "jobs/VGGNet/VOC0712/{}".format(job_name)
+
+# model definition files.
+train_net_file = "{}/train.prototxt".format(save_dir)
+test_net_file = "{}/test.prototxt".format(save_dir)
+deploy_net_file = "{}/deploy.prototxt".format(save_dir)
+solver_file = "{}/solver.prototxt".format(save_dir)
+# snapshot prefix.
+snapshot_prefix = "{}/{}".format(snapshot_dir, model_name)
+# job script path.
+job_file = "{}/{}.sh".format(job_dir, model_name)
+
+# Stores the test image names and sizes. Created by data/VOC0712/create_list.sh
+name_size_file = "data/VOC0712/test_name_size.txt"
+# The pretrained model. We use the RefineDet model trained on coco dataset.
+pretrain_model = "models/VGGNet/VOC0712/refinedet_vgg16_512x512_coco/coco_refinedet_vgg16_512x512.caffemodel"
+# Stores LabelMapItem.
+label_map_file = "data/VOC0712/labelmap_voc.prototxt"
+
+# MultiBoxLoss parameters.
+num_classes = 21
+share_location = True
+background_label_id = 0
+train_on_diff_gt = True
+normalization_mode = P.Loss.VALID
+code_type = P.PriorBox.CENTER_SIZE
+ignore_cross_boundary_bbox = False
+mining_type = P.MultiBoxLoss.MAX_NEGATIVE
+neg_pos_ratio = 3.
+loc_weight = (neg_pos_ratio + 1.) / 4.
+multibox_loss_param = {
+    'loc_loss_type': P.MultiBoxLoss.SMOOTH_L1,
+    'conf_loss_type': P.MultiBoxLoss.SOFTMAX,
+    'loc_weight': loc_weight,
+    'num_classes': num_classes,
+    'share_location': share_location,
+    'match_type': P.MultiBoxLoss.PER_PREDICTION,
+    'overlap_threshold': 0.5,
+    'use_prior_for_matching': True,
+    'background_label_id': background_label_id,
+    'use_difficult_gt': train_on_diff_gt,
+    'mining_type': mining_type,
+    'neg_pos_ratio': neg_pos_ratio,
+    'neg_overlap': 0.5,
+    'code_type': code_type,
+    'ignore_cross_boundary_bbox': ignore_cross_boundary_bbox,
+    'objectness_score': 0.01,
+    }
+loss_param = {
+    'normalization': normalization_mode,
+    }
+
+# parameters for generating priors.
+# minimum dimension of input image
+# min_dim = 512
+# conv4_3 ==> 64 x 64
+# conv5_3 ==> 32 x 32
+# fc7 ==> 16 x 16
+# conv6_2 ==> 8 x 8
+arm_source_layers = ['conv4_3', 'conv5_3', 'fc7', 'conv6_2']
+odm_source_layers = ['P3', 'P4', 'P5', 'P6']
+min_sizes = [32, 64, 128, 256]
+max_sizes = [[], [], [], []]
+steps = [8, 16, 32, 64]
+aspect_ratios = [[2], [2], [2], [2]]
+# L2 normalize conv4_3 and conv5_3.
+normalizations = [10, 8, -1, -1]
+# variance used to encode/decode prior bboxes.
+if code_type == P.PriorBox.CENTER_SIZE:
+  prior_variance = [0.1, 0.1, 0.2, 0.2]
+else:
+  prior_variance = [0.1]
+flip = True
+clip = False
+
+# Solver parameters.
+# Defining which GPUs to use.
+gpus = "0,1,2,3"
+gpulist = gpus.split(",")
+num_gpus = len(gpulist)
+
+# Divide the mini-batch to different GPUs.
+batch_size = 32
+accum_batch_size = 32
+iter_size = accum_batch_size / batch_size
+solver_mode = P.Solver.CPU
+device_id = 0
+batch_size_per_device = batch_size
+if num_gpus > 0:
+  batch_size_per_device = int(math.ceil(float(batch_size) / num_gpus))
+  iter_size = int(math.ceil(float(accum_batch_size) / (batch_size_per_device * num_gpus)))
+  solver_mode = P.Solver.GPU
+  device_id = int(gpulist[0])
+
+if normalization_mode == P.Loss.NONE:
+  base_lr /= batch_size_per_device
+elif normalization_mode == P.Loss.VALID:
+  base_lr *= 25. / loc_weight
+elif normalization_mode == P.Loss.FULL:
+  # Roughly there are 2000 prior bboxes per image.
+  # TODO(weiliu89): Estimate the exact # of priors.
+  base_lr *= 2000.
+
+# Evaluate on whole test set.
+num_test_image = 4952
+test_batch_size = 1
+test_iter = num_test_image / test_batch_size
+
+solver_param = {
+    # Train parameters
+    'base_lr': base_lr,
+    'weight_decay': 0.0005,
+    'lr_policy': "multistep",
+    'stepvalue': [80000, 120000],
+    'gamma': 0.2,
+    'momentum': 0.9,
+    'iter_size': iter_size,
+    'max_iter': 120000,
+    'snapshot': 5000,
+    'display': 10,
+    'average_loss': 10,
+    'type': "SGD",
+    'solver_mode': solver_mode,
+    'device_id': device_id,
+    'debug_info': False,
+    'snapshot_after_train': True,
+    # Test parameters
+    # 'test_iter': [test_iter],
+    # 'test_interval': 5000,
+    # 'eval_type': "detection",
+    # 'ap_version': "11point",
+    # 'test_initialization': False,
+    }
+
+# parameters for generating detection output.
+det_out_param = {
+    'num_classes': num_classes,
+    'share_location': share_location,
+    'background_label_id': background_label_id,
+    'nms_param': {'nms_threshold': 0.45, 'top_k': 1000},
+    'keep_top_k': 500,
+    'confidence_threshold': 0.01,
+    'code_type': code_type,
+    'objectness_score': 0.01,
+    }
+
+# parameters for evaluating detection results.
+det_eval_param = {
+    'num_classes': num_classes,
+    'background_label_id': background_label_id,
+    'overlap_threshold': 0.5,
+    'evaluate_difficult_gt': False,
+    'name_size_file': name_size_file,
+    }
+
+### Hopefully you don't need to change the following ###
+# Check file.
+check_if_exist(train_data)
+check_if_exist(test_data)
+check_if_exist(label_map_file)
+check_if_exist(pretrain_model)
+make_if_not_exist(save_dir)
+make_if_not_exist(job_dir)
+make_if_not_exist(snapshot_dir)
+
+# Create train net.
+net = caffe.NetSpec()
+net.data, net.label = CreateAnnotatedDataLayer(train_data, batch_size=batch_size_per_device,
+        train=True, output_label=True, label_map_file=label_map_file,
+        transform_param=train_transform_param, batch_sampler=batch_sampler)
+
+VGGNetBody(net, from_layer='data', fully_conv=True, reduced=True, dilated=False, dropout=False)
+
+AddExtraLayers(net, use_batchnorm, arm_source_layers, normalizations, lr_mult=lr_mult)
+arm_source_layers.reverse()
+normalizations.reverse()
+
+mbox_layers = CreateRefineDetHead(net, data_layer='data', from_layers=arm_source_layers,
+        use_batchnorm=use_batchnorm, min_sizes=min_sizes, max_sizes=max_sizes,
+        aspect_ratios=aspect_ratios, steps=steps, normalizations=[],
+        num_classes=num_classes, share_location=share_location, flip=flip, clip=clip,
+        prior_variance=prior_variance, kernel_size=3, pad=1, lr_mult=lr_mult, from_layers2=odm_source_layers)
+
+name = "arm_loss"
+mbox_layers_arm = []
+mbox_layers_arm.append(mbox_layers[0])
+mbox_layers_arm.append(mbox_layers[1])
+mbox_layers_arm.append(mbox_layers[2])
+mbox_layers_arm.append(net.label)
+multibox_loss_param_arm = multibox_loss_param.copy()
+multibox_loss_param_arm['num_classes'] = 2
+net[name] = L.MultiBoxLoss(*mbox_layers_arm, multibox_loss_param=multibox_loss_param_arm,
+        loss_param=loss_param, include=dict(phase=caffe_pb2.Phase.Value('TRAIN')),
+        propagate_down=[True, True, False, False])
+
+# Create the MultiBoxLossLayer.
+conf_name = "arm_conf"
+reshape_name = "{}_reshape".format(conf_name)
+net[reshape_name] = L.Reshape(net[conf_name], shape=dict(dim=[0, -1, 2]))
+softmax_name = "{}_softmax".format(conf_name)
+net[softmax_name] = L.Softmax(net[reshape_name], axis=2)
+flatten_name = "{}_flatten".format(conf_name)
+net[flatten_name] = L.Flatten(net[softmax_name], axis=1)
+
+name = "odm_loss"
+mbox_layers_odm = []
+mbox_layers_odm.append(mbox_layers[3])
+mbox_layers_odm.append(mbox_layers[4])
+mbox_layers_odm.append(mbox_layers[2])
+mbox_layers_odm.append(net.label)
+mbox_layers_odm.append(net[flatten_name])
+mbox_layers_odm.append(mbox_layers[0])
+net[name] = L.MultiBoxLoss(*mbox_layers_odm, multibox_loss_param=multibox_loss_param,
+        loss_param=loss_param, include=dict(phase=caffe_pb2.Phase.Value('TRAIN')),
+        propagate_down=[True, True, False, False, False, False])
+
+
+with open(train_net_file, 'w') as f:
+    print('name: "{}_train"'.format(model_name), file=f)
+    print(net.to_proto(), file=f)
+shutil.copy(train_net_file, job_dir)
+
+# Create test net.
+net = caffe.NetSpec()
+net.data, net.label = CreateAnnotatedDataLayer(test_data, batch_size=test_batch_size,
+        train=False, output_label=True, label_map_file=label_map_file,
+        transform_param=test_transform_param)
+
+VGGNetBody(net, from_layer='data', fully_conv=True, reduced=True, dilated=False, dropout=False)
+
+arm_source_layers = ['conv4_3', 'conv5_3', 'fc7', 'conv6_2']
+AddExtraLayers(net, use_batchnorm, arm_source_layers, normalizations, lr_mult=lr_mult)
+arm_source_layers.reverse()
+normalizations.reverse()
+
+mbox_layers = CreateRefineDetHead(net, data_layer='data', from_layers=arm_source_layers,
+        use_batchnorm=use_batchnorm, min_sizes=min_sizes, max_sizes=max_sizes,
+        aspect_ratios=aspect_ratios, steps=steps, normalizations=[],
+        num_classes=num_classes, share_location=share_location, flip=flip, clip=clip,
+        prior_variance=prior_variance, kernel_size=3, pad=1, lr_mult=lr_mult, from_layers2=odm_source_layers)
+
+mbox_layers_out = []
+mbox_layers_out.append(mbox_layers[3])
+mbox_layers_out.append(mbox_layers[4])
+mbox_layers_out.append(mbox_layers[2])
+mbox_layers_out.append(mbox_layers[1])
+mbox_layers_out.append(mbox_layers[0])
+
+conf_name = "arm_conf"
+reshape_name = "{}_reshape".format(conf_name)
+net[reshape_name] = L.Reshape(net[conf_name], shape=dict(dim=[0, -1, 2]))
+softmax_name = "{}_softmax".format(conf_name)
+net[softmax_name] = L.Softmax(net[reshape_name], axis=2)
+flatten_name = "{}_flatten".format(conf_name)
+net[flatten_name] = L.Flatten(net[softmax_name], axis=1)
+mbox_layers_out[3] = net[flatten_name]
+
+conf_name = "odm_conf"
+reshape_name = "{}_reshape".format(conf_name)
+net[reshape_name] = L.Reshape(net[conf_name], shape=dict(dim=[0, -1, num_classes]))
+softmax_name = "{}_softmax".format(conf_name)
+net[softmax_name] = L.Softmax(net[reshape_name], axis=2)
+flatten_name = "{}_flatten".format(conf_name)
+net[flatten_name] = L.Flatten(net[softmax_name], axis=1)
+mbox_layers_out[1] = net[flatten_name]
+
+net.detection_out = L.DetectionOutput(*mbox_layers_out,
+    detection_output_param=det_out_param,
+    include=dict(phase=caffe_pb2.Phase.Value('TEST')))
+net.detection_eval = L.DetectionEvaluate(net.detection_out, net.label,
+    detection_evaluate_param=det_eval_param,
+    include=dict(phase=caffe_pb2.Phase.Value('TEST')))
+
+with open(test_net_file, 'w') as f:
+    print('name: "{}_test"'.format(model_name), file=f)
+    print(net.to_proto(), file=f)
+shutil.copy(test_net_file, job_dir)
+
+# Create deploy net.
+# Remove the first and last layer from test net.
+deploy_net = net
+with open(deploy_net_file, 'w') as f:
+    net_param = deploy_net.to_proto()
+    # Remove the first (AnnotatedData) and last (DetectionEvaluate) layer from test net.
+    del net_param.layer[0]
+    del net_param.layer[-1]
+    net_param.name = '{}_deploy'.format(model_name)
+    net_param.input.extend(['data'])
+    net_param.input_shape.extend([
+        caffe_pb2.BlobShape(dim=[1, 3, resize_height, resize_width])])
+    print(net_param, file=f)
+shutil.copy(deploy_net_file, job_dir)
+
+# Create solver.
+solver = caffe_pb2.SolverParameter(
+        train_net=train_net_file,
+        # test_net=[test_net_file],
+        snapshot_prefix=snapshot_prefix,
+        **solver_param)
+
+with open(solver_file, 'w') as f:
+    print(solver, file=f)
+shutil.copy(solver_file, job_dir)
+
+max_iter = 0
+# Find most recent snapshot.
+for file in os.listdir(snapshot_dir):
+  if file.endswith(".solverstate"):
+    basename = os.path.splitext(file)[0]
+    iter = int(basename.split("{}_iter_".format(model_name))[1])
+    if iter > max_iter:
+      max_iter = iter
+
+train_src_param = '--weights="{}" \\\n'.format(pretrain_model)
+if resume_training:
+  if max_iter > 0:
+    train_src_param = '--snapshot="{}_iter_{}.solverstate" \\\n'.format(snapshot_prefix, max_iter)
+
+if remove_old_models:
+  # Remove any snapshots smaller than max_iter.
+  for file in os.listdir(snapshot_dir):
+    if file.endswith(".solverstate"):
+      basename = os.path.splitext(file)[0]
+      iter = int(basename.split("{}_iter_".format(model_name))[1])
+      if max_iter > iter:
+        os.remove("{}/{}".format(snapshot_dir, file))
+    if file.endswith(".caffemodel"):
+      basename = os.path.splitext(file)[0]
+      iter = int(basename.split("{}_iter_".format(model_name))[1])
+      if max_iter > iter:
+        os.remove("{}/{}".format(snapshot_dir, file))
+
+# Create job file.
+with open(job_file, 'w') as f:
+  f.write('cd {}\n'.format(caffe_root))
+  f.write('./build/tools/caffe train \\\n')
+  f.write('--solver="{}" \\\n'.format(solver_file))
+  f.write(train_src_param)
+  if solver_param['solver_mode'] == P.Solver.GPU:
+    f.write('--gpu {} 2>&1 | tee {}/{}.log\n'.format(gpus, job_dir, model_name))
+  else:
+    f.write('2>&1 | tee {}/{}.log\n'.format(job_dir, model_name))
+
+# Copy the python script to job_dir.
+py_file = os.path.abspath(__file__)
+shutil.copy(py_file, job_dir)
+
+# Run the job.
+os.chmod(job_file, stat.S_IRWXU)
+if run_soon:
+  subprocess.call(job_file, shell=True)
diff --git a/examples/refinedet/finetune_VGG16_VOC2012_320.py b/examples/refinedet/finetune_VGG16_VOC2012_320.py
new file mode 100644
index 0000000..3a309e7
--- /dev/null
+++ b/examples/refinedet/finetune_VGG16_VOC2012_320.py
@@ -0,0 +1,605 @@
+from __future__ import print_function
+import sys
+sys.path.append("./python")
+import caffe
+from caffe.model_libs import *
+from google.protobuf import text_format
+
+import math
+import os
+import shutil
+import stat
+import subprocess
+
+# Add extra layers on top of a "base" network (e.g. VGGNet or Inception).
+def AddExtraLayers(net, use_batchnorm=True, arm_source_layers=[], normalizations=[], lr_mult=1):
+    use_relu = True
+
+    # Add additional convolutional layers.
+    # 320/32: 10 x 10
+    from_layer = net.keys()[-1]
+
+    # 320/64: 5 x 5
+    out_layer = "conv6_1"
+    ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 1, 0, 1, lr_mult=lr_mult)
+
+    from_layer = out_layer
+    out_layer = "conv6_2"
+    ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 512, 3, 1, 2, lr_mult=lr_mult)
+
+    arm_source_layers.reverse()
+    normalizations.reverse()
+    num_p = 6
+    for index, layer in enumerate(arm_source_layers):
+        out_layer = layer
+        if normalizations:
+            if normalizations[index] != -1:
+                norm_name = "{}_norm".format(layer)
+                net[norm_name] = L.Normalize(net[layer], scale_filler=dict(type="constant", value=normalizations[index]),
+                    across_spatial=False, channel_shared=False)
+                out_layer = norm_name
+                arm_source_layers[index] = norm_name
+        from_layer = out_layer
+        out_layer = "TL{}_{}".format(num_p, 1)
+        ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 3, 1, 1, lr_mult=lr_mult)
+
+        if num_p == 6:
+            from_layer = out_layer
+            out_layer = "TL{}_{}".format(num_p, 2)
+            ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 3, 1, 1, lr_mult=lr_mult)
+
+            from_layer = out_layer
+            out_layer = "P{}".format(num_p)
+            ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 3, 1, 1, lr_mult=lr_mult)
+        else:
+            from_layer = out_layer
+            out_layer = "TL{}_{}".format(num_p, 2)
+            ConvBNLayer(net, from_layer, out_layer, use_batchnorm, False, 256, 3, 1, 1, lr_mult=lr_mult)
+
+            from_layer = "P{}".format(num_p+1)
+            out_layer = "P{}-up".format(num_p+1)
+            DeconvBNLayer(net, from_layer, out_layer, use_batchnorm, False, 256, 2, 0, 2, lr_mult=lr_mult)
+
+            from_layer = ["TL{}_{}".format(num_p, 2), "P{}-up".format(num_p+1)]
+            out_layer = "Elt{}".format(num_p)
+            EltwiseLayer(net, from_layer, out_layer)
+            relu_name = '{}_relu'.format(out_layer)
+            net[relu_name] = L.ReLU(net[out_layer], in_place=True)
+            out_layer = relu_name
+
+            from_layer = out_layer
+            out_layer = "P{}".format(num_p)
+            ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 3, 1, 1, lr_mult=lr_mult)
+
+        num_p = num_p - 1
+
+    return net
+
+
+### Modify the following parameters accordingly ###
+# The directory which contains the caffe code.
+# We assume you are running the script at the CAFFE_ROOT.
+caffe_root = os.getcwd()
+
+# Set true if you want to start training right after generating all files.
+run_soon = True
+# Set true if you want to load from most recently saved snapshot.
+# Otherwise, we will load from the pretrain_model defined below.
+resume_training = True
+# If true, Remove old model files.
+remove_old_models = False
+
+# The database file for training data. Created by data/VOC0712Plus/create_data.sh
+train_data = "examples/VOC0712Plus/VOC0712Plus_trainval_lmdb"
+# The database file for testing data. Created by data/VOC0712Plus/create_data.sh
+test_data = "examples/VOC0712Plus/VOC0712Plus_test_lmdb"
+# Specify the batch sampler.
+resize_width = 320
+resize_height = 320
+resize = "{}x{}".format(resize_width, resize_height)
+batch_sampler = [
+        {
+                'sampler': {
+                        },
+                'max_trials': 1,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.1,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.3,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.5,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.7,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.9,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'max_jaccard_overlap': 1.0,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        ]
+train_transform_param = {
+        'mirror': True,
+        'mean_value': [104, 117, 123],
+        'resize_param': {
+                'prob': 1,
+                'resize_mode': P.Resize.WARP,
+                'height': resize_height,
+                'width': resize_width,
+                'interp_mode': [
+                        P.Resize.LINEAR,
+                        P.Resize.AREA,
+                        P.Resize.NEAREST,
+                        P.Resize.CUBIC,
+                        P.Resize.LANCZOS4,
+                        ],
+                },
+        'distort_param': {
+                'brightness_prob': 0.5,
+                'brightness_delta': 32,
+                'contrast_prob': 0.5,
+                'contrast_lower': 0.5,
+                'contrast_upper': 1.5,
+                'hue_prob': 0.5,
+                'hue_delta': 18,
+                'saturation_prob': 0.5,
+                'saturation_lower': 0.5,
+                'saturation_upper': 1.5,
+                'random_order_prob': 0.0,
+                },
+        'expand_param': {
+                'prob': 0.5,
+                'max_expand_ratio': 4.0,
+                },
+        'emit_constraint': {
+            'emit_type': caffe_pb2.EmitConstraint.CENTER,
+            }
+        }
+test_transform_param = {
+        'mean_value': [104, 117, 123],
+        'resize_param': {
+                'prob': 1,
+                'resize_mode': P.Resize.WARP,
+                'height': resize_height,
+                'width': resize_width,
+                'interp_mode': [P.Resize.LINEAR],
+                },
+        }
+
+# If true, use batch norm for all newly added layers.
+# Currently only the non batch norm version has been tested.
+use_batchnorm = False
+lr_mult = 1
+# Use different initial learning rate.
+if use_batchnorm:
+    base_lr = 0.0004
+else:
+    # A learning rate for batch_size = 1, num_gpus = 1.
+    base_lr = 0.00004 * 0.05
+
+# Modify the job name if you want.
+job_name = "refinedet_vgg16_{}_ft".format(resize)
+# The name of the model. Modify it if you want.
+model_name = "VOC0712Plus_{}".format(job_name)
+
+# Directory which stores the model .prototxt file.
+save_dir = "models/VGGNet/VOC0712Plus/{}".format(job_name)
+# Directory which stores the snapshot of models.
+snapshot_dir = "models/VGGNet/VOC0712Plus/{}".format(job_name)
+# Directory which stores the job script and log file.
+job_dir = "jobs/VGGNet/VOC0712Plus/{}".format(job_name)
+
+# model definition files.
+train_net_file = "{}/train.prototxt".format(save_dir)
+test_net_file = "{}/test.prototxt".format(save_dir)
+deploy_net_file = "{}/deploy.prototxt".format(save_dir)
+solver_file = "{}/solver.prototxt".format(save_dir)
+# snapshot prefix.
+snapshot_prefix = "{}/{}".format(snapshot_dir, model_name)
+# job script path.
+job_file = "{}/{}.sh".format(job_dir, model_name)
+
+# Stores the test image names and sizes. Created by data/VOC0712Plus/create_list.sh
+name_size_file = "data/VOC0712Plus/test_name_size.txt"
+# The pretrained model. We use the RefineDet model trained on coco dataset.
+pretrain_model = "models/VGGNet/VOC0712/refinedet_vgg16_320x320_coco/coco_refinedet_vgg16_320x320.caffemodel"
+# Stores LabelMapItem.
+label_map_file = "data/VOC0712Plus/labelmap_voc.prototxt"
+
+# MultiBoxLoss parameters.
+num_classes = 21
+share_location = True
+background_label_id = 0
+train_on_diff_gt = True
+normalization_mode = P.Loss.VALID
+code_type = P.PriorBox.CENTER_SIZE
+ignore_cross_boundary_bbox = False
+mining_type = P.MultiBoxLoss.MAX_NEGATIVE
+neg_pos_ratio = 3.
+loc_weight = (neg_pos_ratio + 1.) / 4.
+multibox_loss_param = {
+    'loc_loss_type': P.MultiBoxLoss.SMOOTH_L1,
+    'conf_loss_type': P.MultiBoxLoss.SOFTMAX,
+    'loc_weight': loc_weight,
+    'num_classes': num_classes,
+    'share_location': share_location,
+    'match_type': P.MultiBoxLoss.PER_PREDICTION,
+    'overlap_threshold': 0.5,
+    'use_prior_for_matching': True,
+    'background_label_id': background_label_id,
+    'use_difficult_gt': train_on_diff_gt,
+    'mining_type': mining_type,
+    'neg_pos_ratio': neg_pos_ratio,
+    'neg_overlap': 0.5,
+    'code_type': code_type,
+    'ignore_cross_boundary_bbox': ignore_cross_boundary_bbox,
+    'objectness_score': 0.01,
+    }
+loss_param = {
+    'normalization': normalization_mode,
+    }
+
+# parameters for generating priors.
+# minimum dimension of input image
+# min_dim = 320
+# conv4_3 ==> 40 x 40
+# conv5_3 ==> 20 x 20
+# fc7 ==> 10 x 10
+# conv6_2 ==> 5 x 5
+arm_source_layers = ['conv4_3', 'conv5_3', 'fc7', 'conv6_2']
+odm_source_layers = ['P3', 'P4', 'P5', 'P6']
+min_sizes = [32, 64, 128, 256]
+max_sizes = [[], [], [], []]
+steps = [8, 16, 32, 64]
+aspect_ratios = [[2], [2], [2], [2]]
+# L2 normalize conv4_3 and conv5_3.
+normalizations = [10, 8, -1, -1]
+# variance used to encode/decode prior bboxes.
+if code_type == P.PriorBox.CENTER_SIZE:
+  prior_variance = [0.1, 0.1, 0.2, 0.2]
+else:
+  prior_variance = [0.1]
+flip = True
+clip = False
+
+# Solver parameters.
+# Defining which GPUs to use.
+gpus = "0,1,2,3"
+gpulist = gpus.split(",")
+num_gpus = len(gpulist)
+
+# Divide the mini-batch to different GPUs.
+batch_size = 32
+accum_batch_size = 32
+iter_size = accum_batch_size / batch_size
+solver_mode = P.Solver.CPU
+device_id = 0
+batch_size_per_device = batch_size
+if num_gpus > 0:
+  batch_size_per_device = int(math.ceil(float(batch_size) / num_gpus))
+  iter_size = int(math.ceil(float(accum_batch_size) / (batch_size_per_device * num_gpus)))
+  solver_mode = P.Solver.GPU
+  device_id = int(gpulist[0])
+
+if normalization_mode == P.Loss.NONE:
+  base_lr /= batch_size_per_device
+elif normalization_mode == P.Loss.VALID:
+  base_lr *= 25. / loc_weight
+elif normalization_mode == P.Loss.FULL:
+  # Roughly there are 2000 prior bboxes per image.
+  # TODO(weiliu89): Estimate the exact # of priors.
+  base_lr *= 2000.
+
+# Evaluate on whole test set.
+num_test_image = 10991
+test_batch_size = 1
+test_iter = num_test_image / test_batch_size
+
+solver_param = {
+    # Train parameters
+    'base_lr': base_lr,
+    'weight_decay': 0.0005,
+    'lr_policy': "multistep",
+    'stepvalue': [120000, 160000],
+    'gamma': 0.2,
+    'momentum': 0.9,
+    'iter_size': iter_size,
+    'max_iter': 160000,
+    'snapshot': 10000,
+    'display': 10,
+    'average_loss': 10,
+    'type': "SGD",
+    'solver_mode': solver_mode,
+    'device_id': device_id,
+    'debug_info': False,
+    'snapshot_after_train': True,
+    # # Test parameters
+    # 'test_iter': [test_iter],
+    # 'test_interval': 160000,
+    # 'eval_type': "detection",
+    # 'ap_version': "11point",
+    # 'test_initialization': False,
+    }
+
+# parameters for generating detection output.
+det_out_param = {
+    'num_classes': num_classes,
+    'share_location': share_location,
+    'background_label_id': background_label_id,
+    'nms_param': {'nms_threshold': 0.45, 'top_k': 1000},
+    'keep_top_k': 500,
+    'confidence_threshold': 0.01,
+    'code_type': code_type,
+    'objectness_score': 0.01,
+    }
+
+# parameters for evaluating detection results.
+det_eval_param = {
+    'num_classes': num_classes,
+    'background_label_id': background_label_id,
+    'overlap_threshold': 0.5,
+    'evaluate_difficult_gt': False,
+    'name_size_file': name_size_file,
+    }
+
+### Hopefully you don't need to change the following ###
+# Check file.
+check_if_exist(train_data)
+#check_if_exist(test_data)
+check_if_exist(label_map_file)
+check_if_exist(pretrain_model)
+make_if_not_exist(save_dir)
+make_if_not_exist(job_dir)
+make_if_not_exist(snapshot_dir)
+
+# Create train net.
+net = caffe.NetSpec()
+net.data, net.label = CreateAnnotatedDataLayer(train_data, batch_size=batch_size_per_device,
+        train=True, output_label=True, label_map_file=label_map_file,
+        transform_param=train_transform_param, batch_sampler=batch_sampler)
+
+VGGNetBody(net, from_layer='data', fully_conv=True, reduced=True, dilated=False, dropout=False)
+
+AddExtraLayers(net, use_batchnorm, arm_source_layers, normalizations, lr_mult=lr_mult)
+arm_source_layers.reverse()
+normalizations.reverse()
+
+mbox_layers = CreateRefineDetHead(net, data_layer='data', from_layers=arm_source_layers,
+        use_batchnorm=use_batchnorm, min_sizes=min_sizes, max_sizes=max_sizes,
+        aspect_ratios=aspect_ratios, steps=steps, normalizations=[],
+        num_classes=num_classes, share_location=share_location, flip=flip, clip=clip,
+        prior_variance=prior_variance, kernel_size=3, pad=1, lr_mult=lr_mult, from_layers2=odm_source_layers)
+
+name = "arm_loss"
+mbox_layers_arm = []
+mbox_layers_arm.append(mbox_layers[0])
+mbox_layers_arm.append(mbox_layers[1])
+mbox_layers_arm.append(mbox_layers[2])
+mbox_layers_arm.append(net.label)
+multibox_loss_param_arm = multibox_loss_param.copy()
+multibox_loss_param_arm['num_classes'] = 2
+net[name] = L.MultiBoxLoss(*mbox_layers_arm, multibox_loss_param=multibox_loss_param_arm,
+        loss_param=loss_param, include=dict(phase=caffe_pb2.Phase.Value('TRAIN')),
+        propagate_down=[True, True, False, False])
+
+# Create the MultiBoxLossLayer.
+conf_name = "arm_conf"
+reshape_name = "{}_reshape".format(conf_name)
+net[reshape_name] = L.Reshape(net[conf_name], shape=dict(dim=[0, -1, 2]))
+softmax_name = "{}_softmax".format(conf_name)
+net[softmax_name] = L.Softmax(net[reshape_name], axis=2)
+flatten_name = "{}_flatten".format(conf_name)
+net[flatten_name] = L.Flatten(net[softmax_name], axis=1)
+
+name = "odm_loss"
+mbox_layers_odm = []
+mbox_layers_odm.append(mbox_layers[3])
+mbox_layers_odm.append(mbox_layers[4])
+mbox_layers_odm.append(mbox_layers[2])
+mbox_layers_odm.append(net.label)
+mbox_layers_odm.append(net[flatten_name])
+mbox_layers_odm.append(mbox_layers[0])
+net[name] = L.MultiBoxLoss(*mbox_layers_odm, multibox_loss_param=multibox_loss_param,
+        loss_param=loss_param, include=dict(phase=caffe_pb2.Phase.Value('TRAIN')),
+        propagate_down=[True, True, False, False, False, False])
+
+
+with open(train_net_file, 'w') as f:
+    print('name: "{}_train"'.format(model_name), file=f)
+    print(net.to_proto(), file=f)
+shutil.copy(train_net_file, job_dir)
+
+# Create test net.
+net = caffe.NetSpec()
+net.data, net.label = CreateAnnotatedDataLayer(test_data, batch_size=test_batch_size,
+        train=False, output_label=True, label_map_file=label_map_file,
+        transform_param=test_transform_param)
+
+VGGNetBody(net, from_layer='data', fully_conv=True, reduced=True, dilated=False, dropout=False)
+
+arm_source_layers = ['conv4_3', 'conv5_3', 'fc7', 'conv6_2']
+AddExtraLayers(net, use_batchnorm, arm_source_layers, normalizations, lr_mult=lr_mult)
+arm_source_layers.reverse()
+normalizations.reverse()
+
+mbox_layers = CreateRefineDetHead(net, data_layer='data', from_layers=arm_source_layers,
+        use_batchnorm=use_batchnorm, min_sizes=min_sizes, max_sizes=max_sizes,
+        aspect_ratios=aspect_ratios, steps=steps, normalizations=[],
+        num_classes=num_classes, share_location=share_location, flip=flip, clip=clip,
+        prior_variance=prior_variance, kernel_size=3, pad=1, lr_mult=lr_mult, from_layers2=odm_source_layers)
+
+mbox_layers_out = []
+mbox_layers_out.append(mbox_layers[3])
+mbox_layers_out.append(mbox_layers[4])
+mbox_layers_out.append(mbox_layers[2])
+mbox_layers_out.append(mbox_layers[1])
+mbox_layers_out.append(mbox_layers[0])
+
+conf_name = "arm_conf"
+reshape_name = "{}_reshape".format(conf_name)
+net[reshape_name] = L.Reshape(net[conf_name], shape=dict(dim=[0, -1, 2]))
+softmax_name = "{}_softmax".format(conf_name)
+net[softmax_name] = L.Softmax(net[reshape_name], axis=2)
+flatten_name = "{}_flatten".format(conf_name)
+net[flatten_name] = L.Flatten(net[softmax_name], axis=1)
+mbox_layers_out[3] = net[flatten_name]
+
+conf_name = "odm_conf"
+reshape_name = "{}_reshape".format(conf_name)
+net[reshape_name] = L.Reshape(net[conf_name], shape=dict(dim=[0, -1, num_classes]))
+softmax_name = "{}_softmax".format(conf_name)
+net[softmax_name] = L.Softmax(net[reshape_name], axis=2)
+flatten_name = "{}_flatten".format(conf_name)
+net[flatten_name] = L.Flatten(net[softmax_name], axis=1)
+mbox_layers_out[1] = net[flatten_name]
+
+net.detection_out = L.DetectionOutput(*mbox_layers_out,
+    detection_output_param=det_out_param,
+    include=dict(phase=caffe_pb2.Phase.Value('TEST')))
+net.detection_eval = L.DetectionEvaluate(net.detection_out, net.label,
+    detection_evaluate_param=det_eval_param,
+    include=dict(phase=caffe_pb2.Phase.Value('TEST')))
+
+with open(test_net_file, 'w') as f:
+    print('name: "{}_test"'.format(model_name), file=f)
+    print(net.to_proto(), file=f)
+shutil.copy(test_net_file, job_dir)
+
+# Create deploy net.
+# Remove the first and last layer from test net.
+deploy_net = net
+with open(deploy_net_file, 'w') as f:
+    net_param = deploy_net.to_proto()
+    # Remove the first (AnnotatedData) and last (DetectionEvaluate) layer from test net.
+    del net_param.layer[0]
+    del net_param.layer[-1]
+    net_param.name = '{}_deploy'.format(model_name)
+    net_param.input.extend(['data'])
+    net_param.input_shape.extend([
+        caffe_pb2.BlobShape(dim=[1, 3, resize_height, resize_width])])
+    print(net_param, file=f)
+shutil.copy(deploy_net_file, job_dir)
+
+# Create solver.
+solver = caffe_pb2.SolverParameter(
+        train_net=train_net_file,
+        # test_net=[test_net_file],
+        snapshot_prefix=snapshot_prefix,
+        **solver_param)
+
+with open(solver_file, 'w') as f:
+    print(solver, file=f)
+shutil.copy(solver_file, job_dir)
+
+max_iter = 0
+# Find most recent snapshot.
+for file in os.listdir(snapshot_dir):
+  if file.endswith(".solverstate"):
+    basename = os.path.splitext(file)[0]
+    iter = int(basename.split("{}_iter_".format(model_name))[1])
+    if iter > max_iter:
+      max_iter = iter
+
+train_src_param = '--weights="{}" \\\n'.format(pretrain_model)
+if resume_training:
+  if max_iter > 0:
+    train_src_param = '--snapshot="{}_iter_{}.solverstate" \\\n'.format(snapshot_prefix, max_iter)
+
+if remove_old_models:
+  # Remove any snapshots smaller than max_iter.
+  for file in os.listdir(snapshot_dir):
+    if file.endswith(".solverstate"):
+      basename = os.path.splitext(file)[0]
+      iter = int(basename.split("{}_iter_".format(model_name))[1])
+      if max_iter > iter:
+        os.remove("{}/{}".format(snapshot_dir, file))
+    if file.endswith(".caffemodel"):
+      basename = os.path.splitext(file)[0]
+      iter = int(basename.split("{}_iter_".format(model_name))[1])
+      if max_iter > iter:
+        os.remove("{}/{}".format(snapshot_dir, file))
+
+# Create job file.
+with open(job_file, 'w') as f:
+  f.write('cd {}\n'.format(caffe_root))
+  f.write('./build/tools/caffe train \\\n')
+  f.write('--solver="{}" \\\n'.format(solver_file))
+  f.write(train_src_param)
+  if solver_param['solver_mode'] == P.Solver.GPU:
+    f.write('--gpu {} 2>&1 | tee {}/{}.log\n'.format(gpus, job_dir, model_name))
+  else:
+    f.write('2>&1 | tee {}/{}.log\n'.format(job_dir, model_name))
+
+# Copy the python script to job_dir.
+py_file = os.path.abspath(__file__)
+shutil.copy(py_file, job_dir)
+
+# Run the job.
+os.chmod(job_file, stat.S_IRWXU)
+if run_soon:
+  subprocess.call(job_file, shell=True)
diff --git a/examples/refinedet/finetune_VGG16_VOC2012_512.py b/examples/refinedet/finetune_VGG16_VOC2012_512.py
new file mode 100644
index 0000000..adbe1f1
--- /dev/null
+++ b/examples/refinedet/finetune_VGG16_VOC2012_512.py
@@ -0,0 +1,605 @@
+from __future__ import print_function
+import sys
+sys.path.append("./python")
+import caffe
+from caffe.model_libs import *
+from google.protobuf import text_format
+
+import math
+import os
+import shutil
+import stat
+import subprocess
+
+# Add extra layers on top of a "base" network (e.g. VGGNet or Inception).
+def AddExtraLayers(net, use_batchnorm=True, arm_source_layers=[], normalizations=[], lr_mult=1):
+    use_relu = True
+
+    # Add additional convolutional layers.
+    # 512/32: 16 x 16
+    from_layer = net.keys()[-1]
+
+    # 512/64: 8 x 8
+    out_layer = "conv6_1"
+    ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 1, 0, 1, lr_mult=lr_mult)
+
+    from_layer = out_layer
+    out_layer = "conv6_2"
+    ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 512, 3, 1, 2, lr_mult=lr_mult)
+
+    arm_source_layers.reverse()
+    normalizations.reverse()
+    num_p = 6
+    for index, layer in enumerate(arm_source_layers):
+        out_layer = layer
+        if normalizations:
+            if normalizations[index] != -1:
+                norm_name = "{}_norm".format(layer)
+                net[norm_name] = L.Normalize(net[layer], scale_filler=dict(type="constant", value=normalizations[index]),
+                    across_spatial=False, channel_shared=False)
+                out_layer = norm_name
+                arm_source_layers[index] = norm_name
+        from_layer = out_layer
+        out_layer = "TL{}_{}".format(num_p, 1)
+        ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 3, 1, 1, lr_mult=lr_mult)
+
+        if num_p == 6:
+            from_layer = out_layer
+            out_layer = "TL{}_{}".format(num_p, 2)
+            ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 3, 1, 1, lr_mult=lr_mult)
+
+            from_layer = out_layer
+            out_layer = "P{}".format(num_p)
+            ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 3, 1, 1, lr_mult=lr_mult)
+        else:
+            from_layer = out_layer
+            out_layer = "TL{}_{}".format(num_p, 2)
+            ConvBNLayer(net, from_layer, out_layer, use_batchnorm, False, 256, 3, 1, 1, lr_mult=lr_mult)
+
+            from_layer = "P{}".format(num_p+1)
+            out_layer = "P{}-up".format(num_p+1)
+            DeconvBNLayer(net, from_layer, out_layer, use_batchnorm, False, 256, 2, 0, 2, lr_mult=lr_mult)
+
+            from_layer = ["TL{}_{}".format(num_p, 2), "P{}-up".format(num_p+1)]
+            out_layer = "Elt{}".format(num_p)
+            EltwiseLayer(net, from_layer, out_layer)
+            relu_name = '{}_relu'.format(out_layer)
+            net[relu_name] = L.ReLU(net[out_layer], in_place=True)
+            out_layer = relu_name
+
+            from_layer = out_layer
+            out_layer = "P{}".format(num_p)
+            ConvBNLayer(net, from_layer, out_layer, use_batchnorm, use_relu, 256, 3, 1, 1, lr_mult=lr_mult)
+
+        num_p = num_p - 1
+
+    return net
+
+
+### Modify the following parameters accordingly ###
+# The directory which contains the caffe code.
+# We assume you are running the script at the CAFFE_ROOT.
+caffe_root = os.getcwd()
+
+# Set true if you want to start training right after generating all files.
+run_soon = True
+# Set true if you want to load from most recently saved snapshot.
+# Otherwise, we will load from the pretrain_model defined below.
+resume_training = True
+# If true, Remove old model files.
+remove_old_models = False
+
+# The database file for training data. Created by data/VOC0712Plus/create_data.sh
+train_data = "examples/VOC0712Plus/VOC0712Plus_trainval_lmdb"
+# The database file for testing data. Created by data/VOC0712Plus/create_data.sh
+test_data = "examples/VOC0712Plus/VOC0712Plus_test_lmdb"
+# Specify the batch sampler.
+resize_width = 512
+resize_height = 512
+resize = "{}x{}".format(resize_width, resize_height)
+batch_sampler = [
+        {
+                'sampler': {
+                        },
+                'max_trials': 1,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.1,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.3,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.5,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.7,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'min_jaccard_overlap': 0.9,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        {
+                'sampler': {
+                        'min_scale': 0.3,
+                        'max_scale': 1.0,
+                        'min_aspect_ratio': 0.5,
+                        'max_aspect_ratio': 2.0,
+                        },
+                'sample_constraint': {
+                        'max_jaccard_overlap': 1.0,
+                        },
+                'max_trials': 50,
+                'max_sample': 1,
+        },
+        ]
+train_transform_param = {
+        'mirror': True,
+        'mean_value': [104, 117, 123],
+        'resize_param': {
+                'prob': 1,
+                'resize_mode': P.Resize.WARP,
+                'height': resize_height,
+                'width': resize_width,
+                'interp_mode': [
+                        P.Resize.LINEAR,
+                        P.Resize.AREA,
+                        P.Resize.NEAREST,
+                        P.Resize.CUBIC,
+                        P.Resize.LANCZOS4,
+                        ],
+                },
+        'distort_param': {
+                'brightness_prob': 0.5,
+                'brightness_delta': 32,
+                'contrast_prob': 0.5,
+                'contrast_lower': 0.5,
+                'contrast_upper': 1.5,
+                'hue_prob': 0.5,
+                'hue_delta': 18,
+                'saturation_prob': 0.5,
+                'saturation_lower': 0.5,
+                'saturation_upper': 1.5,
+                'random_order_prob': 0.0,
+                },
+        'expand_param': {
+                'prob': 0.5,
+                'max_expand_ratio': 4.0,
+                },
+        'emit_constraint': {
+            'emit_type': caffe_pb2.EmitConstraint.CENTER,
+            }
+        }
+test_transform_param = {
+        'mean_value': [104, 117, 123],
+        'resize_param': {
+                'prob': 1,
+                'resize_mode': P.Resize.WARP,
+                'height': resize_height,
+                'width': resize_width,
+                'interp_mode': [P.Resize.LINEAR],
+                },
+        }
+
+# If true, use batch norm for all newly added layers.
+# Currently only the non batch norm version has been tested.
+use_batchnorm = False
+lr_mult = 1
+# Use different initial learning rate.
+if use_batchnorm:
+    base_lr = 0.0004
+else:
+    # A learning rate for batch_size = 1, num_gpus = 1.
+    base_lr = 0.00004 * 0.05
+
+# Modify the job name if you want.
+job_name = "refinedet_vgg16_{}_ft".format(resize)
+# The name of the model. Modify it if you want.
+model_name = "VOC0712Plus_{}".format(job_name)
+
+# Directory which stores the model .prototxt file.
+save_dir = "models/VGGNet/VOC0712Plus/{}".format(job_name)
+# Directory which stores the snapshot of models.
+snapshot_dir = "models/VGGNet/VOC0712Plus/{}".format(job_name)
+# Directory which stores the job script and log file.
+job_dir = "jobs/VGGNet/VOC0712Plus/{}".format(job_name)
+
+# model definition files.
+train_net_file = "{}/train.prototxt".format(save_dir)
+test_net_file = "{}/test.prototxt".format(save_dir)
+deploy_net_file = "{}/deploy.prototxt".format(save_dir)
+solver_file = "{}/solver.prototxt".format(save_dir)
+# snapshot prefix.
+snapshot_prefix = "{}/{}".format(snapshot_dir, model_name)
+# job script path.
+job_file = "{}/{}.sh".format(job_dir, model_name)
+
+# Stores the test image names and sizes. Created by data/VOC0712Plus/create_list.sh
+name_size_file = "data/VOC0712Plus/test_name_size.txt"
+# The pretrained model. We use the RefineDet model trained on coco dataset.
+pretrain_model = "models/VGGNet/VOC0712/refinedet_vgg16_512x512_coco/coco_refinedet_vgg16_512x512.caffemodel"
+# Stores LabelMapItem.
+label_map_file = "data/VOC0712Plus/labelmap_voc.prototxt"
+
+# MultiBoxLoss parameters.
+num_classes = 21
+share_location = True
+background_label_id = 0
+train_on_diff_gt = True
+normalization_mode = P.Loss.VALID
+code_type = P.PriorBox.CENTER_SIZE
+ignore_cross_boundary_bbox = False
+mining_type = P.MultiBoxLoss.MAX_NEGATIVE
+neg_pos_ratio = 3.
+loc_weight = (neg_pos_ratio + 1.) / 4.
+multibox_loss_param = {
+    'loc_loss_type': P.MultiBoxLoss.SMOOTH_L1,
+    'conf_loss_type': P.MultiBoxLoss.SOFTMAX,
+    'loc_weight': loc_weight,
+    'num_classes': num_classes,
+    'share_location': share_location,
+    'match_type': P.MultiBoxLoss.PER_PREDICTION,
+    'overlap_threshold': 0.5,
+    'use_prior_for_matching': True,
+    'background_label_id': background_label_id,
+    'use_difficult_gt': train_on_diff_gt,
+    'mining_type': mining_type,
+    'neg_pos_ratio': neg_pos_ratio,
+    'neg_overlap': 0.5,
+    'code_type': code_type,
+    'ignore_cross_boundary_bbox': ignore_cross_boundary_bbox,
+    'objectness_score': 0.01,
+    }
+loss_param = {
+    'normalization': normalization_mode,
+    }
+
+# parameters for generating priors.
+# minimum dimension of input image
+# min_dim = 512
+# conv4_3 ==> 64 x 64
+# conv5_3 ==> 32 x 32
+# fc7 ==> 16 x 16
+# conv6_2 ==> 8 x 8
+arm_source_layers = ['conv4_3', 'conv5_3', 'fc7', 'conv6_2']
+odm_source_layers = ['P3', 'P4', 'P5', 'P6']
+min_sizes = [32, 64, 128, 256]
+max_sizes = [[], [], [], []]
+steps = [8, 16, 32, 64]
+aspect_ratios = [[2], [2], [2], [2]]
+# L2 normalize conv4_3 and conv5_3.
+normalizations = [10, 8, -1, -1]
+# variance used to encode/decode prior bboxes.
+if code_type == P.PriorBox.CENTER_SIZE:
+  prior_variance = [0.1, 0.1, 0.2, 0.2]
+else:
+  prior_variance = [0.1]
+flip = True
+clip = False
+
+# Solver parameters.
+# Defining which GPUs to use.
+gpus = "0,1,2,3"
+gpulist = gpus.split(",")
+num_gpus = len(gpulist)
+
+# Divide the mini-batch to different GPUs.
+batch_size = 32
+accum_batch_size = 32
+iter_size = accum_batch_size / batch_size
+solver_mode = P.Solver.CPU
+device_id = 0
+batch_size_per_device = batch_size
+if num_gpus > 0:
+  batch_size_per_device = int(math.ceil(float(batch_size) / num_gpus))
+  iter_size = int(math.ceil(float(accum_batch_size) / (batch_size_per_device * num_gpus)))
+  solver_mode = P.Solver.GPU
+  device_id = int(gpulist[0])
+
+if normalization_mode == P.Loss.NONE:
+  base_lr /= batch_size_per_device
+elif normalization_mode == P.Loss.VALID:
+  base_lr *= 25. / loc_weight
+elif normalization_mode == P.Loss.FULL:
+  # Roughly there are 2000 prior bboxes per image.
+  # TODO(weiliu89): Estimate the exact # of priors.
+  base_lr *= 2000.
+
+# Evaluate on whole test set.
+num_test_image = 10991
+test_batch_size = 1
+test_iter = num_test_image / test_batch_size
+
+solver_param = {
+    # Train parameters
+    'base_lr': base_lr,
+    'weight_decay': 0.0005,
+    'lr_policy': "multistep",
+    'stepvalue': [120000, 160000],
+    'gamma': 0.2,
+    'momentum': 0.9,
+    'iter_size': iter_size,
+    'max_iter': 160000,
+    'snapshot': 10000,
+    'display': 10,
+    'average_loss': 10,
+    'type': "SGD",
+    'solver_mode': solver_mode,
+    'device_id': device_id,
+    'debug_info': False,
+    'snapshot_after_train': True,
+    # # Test parameters
+    # 'test_iter': [test_iter],
+    # 'test_interval': 160000,
+    # 'eval_type': "detection",
+    # 'ap_version': "11point",
+    # 'test_initialization': False,
+    }
+
+# parameters for generating detection output.
+det_out_param = {
+    'num_classes': num_classes,
+    'share_location': share_location,
+    'background_label_id': background_label_id,
+    'nms_param': {'nms_threshold': 0.45, 'top_k': 1000},
+    'keep_top_k': 500,
+    'confidence_threshold': 0.01,
+    'code_type': code_type,
+    'objectness_score': 0.01,
+    }
+
+# parameters for evaluating detection results.
+det_eval_param = {
+    'num_classes': num_classes,
+    'background_label_id': background_label_id,
+    'overlap_threshold': 0.5,
+    'evaluate_difficult_gt': False,
+    'name_size_file': name_size_file,
+    }
+
+### Hopefully you don't need to change the following ###
+# Check file.
+check_if_exist(train_data)
+#check_if_exist(test_data)
+check_if_exist(label_map_file)
+check_if_exist(pretrain_model)
+make_if_not_exist(save_dir)
+make_if_not_exist(job_dir)
+make_if_not_exist(snapshot_dir)
+
+# Create train net.
+net = caffe.NetSpec()
+net.data, net.label = CreateAnnotatedDataLayer(train_data, batch_size=batch_size_per_device,
+        train=True, output_label=True, label_map_file=label_map_file,
+        transform_param=train_transform_param, batch_sampler=batch_sampler)
+
+VGGNetBody(net, from_layer='data', fully_conv=True, reduced=True, dilated=False, dropout=False)
+
+AddExtraLayers(net, use_batchnorm, arm_source_layers, normalizations, lr_mult=lr_mult)
+arm_source_layers.reverse()
+normalizations.reverse()
+
+mbox_layers = CreateRefineDetHead(net, data_layer='data', from_layers=arm_source_layers,
+        use_batchnorm=use_batchnorm, min_sizes=min_sizes, max_sizes=max_sizes,
+        aspect_ratios=aspect_ratios, steps=steps, normalizations=[],
+        num_classes=num_classes, share_location=share_location, flip=flip, clip=clip,
+        prior_variance=prior_variance, kernel_size=3, pad=1, lr_mult=lr_mult, from_layers2=odm_source_layers)
+
+name = "arm_loss"
+mbox_layers_arm = []
+mbox_layers_arm.append(mbox_layers[0])
+mbox_layers_arm.append(mbox_layers[1])
+mbox_layers_arm.append(mbox_layers[2])
+mbox_layers_arm.append(net.label)
+multibox_loss_param_arm = multibox_loss_param.copy()
+multibox_loss_param_arm['num_classes'] = 2
+net[name] = L.MultiBoxLoss(*mbox_layers_arm, multibox_loss_param=multibox_loss_param_arm,
+        loss_param=loss_param, include=dict(phase=caffe_pb2.Phase.Value('TRAIN')),
+        propagate_down=[True, True, False, False])
+
+# Create the MultiBoxLossLayer.
+conf_name = "arm_conf"
+reshape_name = "{}_reshape".format(conf_name)
+net[reshape_name] = L.Reshape(net[conf_name], shape=dict(dim=[0, -1, 2]))
+softmax_name = "{}_softmax".format(conf_name)
+net[softmax_name] = L.Softmax(net[reshape_name], axis=2)
+flatten_name = "{}_flatten".format(conf_name)
+net[flatten_name] = L.Flatten(net[softmax_name], axis=1)
+
+name = "odm_loss"
+mbox_layers_odm = []
+mbox_layers_odm.append(mbox_layers[3])
+mbox_layers_odm.append(mbox_layers[4])
+mbox_layers_odm.append(mbox_layers[2])
+mbox_layers_odm.append(net.label)
+mbox_layers_odm.append(net[flatten_name])
+mbox_layers_odm.append(mbox_layers[0])
+net[name] = L.MultiBoxLoss(*mbox_layers_odm, multibox_loss_param=multibox_loss_param,
+        loss_param=loss_param, include=dict(phase=caffe_pb2.Phase.Value('TRAIN')),
+        propagate_down=[True, True, False, False, False, False])
+
+
+with open(train_net_file, 'w') as f:
+    print('name: "{}_train"'.format(model_name), file=f)
+    print(net.to_proto(), file=f)
+shutil.copy(train_net_file, job_dir)
+
+# Create test net.
+net = caffe.NetSpec()
+net.data, net.label = CreateAnnotatedDataLayer(test_data, batch_size=test_batch_size,
+        train=False, output_label=True, label_map_file=label_map_file,
+        transform_param=test_transform_param)
+
+VGGNetBody(net, from_layer='data', fully_conv=True, reduced=True, dilated=False, dropout=False)
+
+arm_source_layers = ['conv4_3', 'conv5_3', 'fc7', 'conv6_2']
+AddExtraLayers(net, use_batchnorm, arm_source_layers, normalizations, lr_mult=lr_mult)
+arm_source_layers.reverse()
+normalizations.reverse()
+
+mbox_layers = CreateRefineDetHead(net, data_layer='data', from_layers=arm_source_layers,
+        use_batchnorm=use_batchnorm, min_sizes=min_sizes, max_sizes=max_sizes,
+        aspect_ratios=aspect_ratios, steps=steps, normalizations=[],
+        num_classes=num_classes, share_location=share_location, flip=flip, clip=clip,
+        prior_variance=prior_variance, kernel_size=3, pad=1, lr_mult=lr_mult, from_layers2=odm_source_layers)
+
+mbox_layers_out = []
+mbox_layers_out.append(mbox_layers[3])
+mbox_layers_out.append(mbox_layers[4])
+mbox_layers_out.append(mbox_layers[2])
+mbox_layers_out.append(mbox_layers[1])
+mbox_layers_out.append(mbox_layers[0])
+
+conf_name = "arm_conf"
+reshape_name = "{}_reshape".format(conf_name)
+net[reshape_name] = L.Reshape(net[conf_name], shape=dict(dim=[0, -1, 2]))
+softmax_name = "{}_softmax".format(conf_name)
+net[softmax_name] = L.Softmax(net[reshape_name], axis=2)
+flatten_name = "{}_flatten".format(conf_name)
+net[flatten_name] = L.Flatten(net[softmax_name], axis=1)
+mbox_layers_out[3] = net[flatten_name]
+
+conf_name = "odm_conf"
+reshape_name = "{}_reshape".format(conf_name)
+net[reshape_name] = L.Reshape(net[conf_name], shape=dict(dim=[0, -1, num_classes]))
+softmax_name = "{}_softmax".format(conf_name)
+net[softmax_name] = L.Softmax(net[reshape_name], axis=2)
+flatten_name = "{}_flatten".format(conf_name)
+net[flatten_name] = L.Flatten(net[softmax_name], axis=1)
+mbox_layers_out[1] = net[flatten_name]
+
+net.detection_out = L.DetectionOutput(*mbox_layers_out,
+    detection_output_param=det_out_param,
+    include=dict(phase=caffe_pb2.Phase.Value('TEST')))
+net.detection_eval = L.DetectionEvaluate(net.detection_out, net.label,
+    detection_evaluate_param=det_eval_param,
+    include=dict(phase=caffe_pb2.Phase.Value('TEST')))
+
+with open(test_net_file, 'w') as f:
+    print('name: "{}_test"'.format(model_name), file=f)
+    print(net.to_proto(), file=f)
+shutil.copy(test_net_file, job_dir)
+
+# Create deploy net.
+# Remove the first and last layer from test net.
+deploy_net = net
+with open(deploy_net_file, 'w') as f:
+    net_param = deploy_net.to_proto()
+    # Remove the first (AnnotatedData) and last (DetectionEvaluate) layer from test net.
+    del net_param.layer[0]
+    del net_param.layer[-1]
+    net_param.name = '{}_deploy'.format(model_name)
+    net_param.input.extend(['data'])
+    net_param.input_shape.extend([
+        caffe_pb2.BlobShape(dim=[1, 3, resize_height, resize_width])])
+    print(net_param, file=f)
+shutil.copy(deploy_net_file, job_dir)
+
+# Create solver.
+solver = caffe_pb2.SolverParameter(
+        train_net=train_net_file,
+        # test_net=[test_net_file],
+        snapshot_prefix=snapshot_prefix,
+        **solver_param)
+
+with open(solver_file, 'w') as f:
+    print(solver, file=f)
+shutil.copy(solver_file, job_dir)
+
+max_iter = 0
+# Find most recent snapshot.
+for file in os.listdir(snapshot_dir):
+  if file.endswith(".solverstate"):
+    basename = os.path.splitext(file)[0]
+    iter = int(basename.split("{}_iter_".format(model_name))[1])
+    if iter > max_iter:
+      max_iter = iter
+
+train_src_param = '--weights="{}" \\\n'.format(pretrain_model)
+if resume_training:
+  if max_iter > 0:
+    train_src_param = '--snapshot="{}_iter_{}.solverstate" \\\n'.format(snapshot_prefix, max_iter)
+
+if remove_old_models:
+  # Remove any snapshots smaller than max_iter.
+  for file in os.listdir(snapshot_dir):
+    if file.endswith(".solverstate"):
+      basename = os.path.splitext(file)[0]
+      iter = int(basename.split("{}_iter_".format(model_name))[1])
+      if max_iter > iter:
+        os.remove("{}/{}".format(snapshot_dir, file))
+    if file.endswith(".caffemodel"):
+      basename = os.path.splitext(file)[0]
+      iter = int(basename.split("{}_iter_".format(model_name))[1])
+      if max_iter > iter:
+        os.remove("{}/{}".format(snapshot_dir, file))
+
+# Create job file.
+with open(job_file, 'w') as f:
+  f.write('cd {}\n'.format(caffe_root))
+  f.write('./build/tools/caffe train \\\n')
+  f.write('--solver="{}" \\\n'.format(solver_file))
+  f.write(train_src_param)
+  if solver_param['solver_mode'] == P.Solver.GPU:
+    f.write('--gpu {} 2>&1 | tee {}/{}.log\n'.format(gpus, job_dir, model_name))
+  else:
+    f.write('2>&1 | tee {}/{}.log\n'.format(job_dir, model_name))
+
+# Copy the python script to job_dir.
+py_file = os.path.abspath(__file__)
+shutil.copy(py_file, job_dir)
+
+# Run the job.
+os.chmod(job_file, stat.S_IRWXU)
+if run_soon:
+  subprocess.call(job_file, shell=True)
diff --git a/include/caffe/layers/multibox_loss_layer.hpp b/include/caffe/layers/multibox_loss_layer.hpp
index d723656..daa8a38 100644
--- a/include/caffe/layers/multibox_loss_layer.hpp
+++ b/include/caffe/layers/multibox_loss_layer.hpp
@@ -39,7 +39,7 @@ class MultiBoxLossLayer : public LossLayer<Ftype, Btype> {
   // bottom[1] stores the confidence predictions.
   // bottom[2] stores the prior bounding boxes.
   // bottom[3] stores the ground truth bounding boxes.
-  virtual inline int ExactNumBottomBlobs() const { return 4; }
+  virtual inline int ExactNumBottomBlobs() const { return -1; }
   virtual inline int ExactNumTopBlobs() const { return 1; }
 
  protected:
diff --git a/python/caffe/model_libs.py b/python/caffe/model_libs.py
new file mode 100644
index 0000000..b45a925
--- /dev/null
+++ b/python/caffe/model_libs.py
@@ -0,0 +1,1167 @@
+import os
+
+import caffe
+from caffe import layers as L
+from caffe import params as P
+from caffe.proto import caffe_pb2
+
+def check_if_exist(path):
+    return os.path.exists(path)
+
+def make_if_not_exist(path):
+    if not os.path.exists(path):
+        os.makedirs(path)
+
+def UnpackVariable(var, num):
+  assert len > 0
+  if type(var) is list and len(var) == num:
+    return var
+  else:
+    ret = []
+    if type(var) is list:
+      assert len(var) == 1
+      for i in xrange(0, num):
+        ret.append(var[0])
+    else:
+      for i in xrange(0, num):
+        ret.append(var)
+    return ret
+
+def ConvBNLayer(net, from_layer, out_layer, use_bn, use_relu, num_output,
+    kernel_size, pad, stride, dilation=1, use_scale=True, lr_mult=1,
+    conv_prefix='', conv_postfix='', bn_prefix='', bn_postfix='_bn',
+    scale_prefix='', scale_postfix='_scale', bias_prefix='', bias_postfix='_bias',
+    **bn_params):
+  if use_bn:
+    # parameters for convolution layer with batchnorm.
+    kwargs = {
+        'param': [dict(lr_mult=lr_mult, decay_mult=1)],
+        'weight_filler': dict(type='gaussian', std=0.01),
+        'bias_term': False,
+        }
+    # parameters for scale bias layer after batchnorm.
+    if use_scale:
+      sb_kwargs = {
+          'bias_term': True,
+          }
+    else:
+      bias_kwargs = {
+          'param': [dict(lr_mult=lr_mult, decay_mult=0)],
+          'filler': dict(type='constant', value=0.0),
+          }
+  else:
+    kwargs = {
+        'param': [
+            dict(lr_mult=lr_mult, decay_mult=1),
+            dict(lr_mult=2 * lr_mult, decay_mult=0)],
+        'weight_filler': dict(type='xavier'),
+        'bias_filler': dict(type='constant', value=0)
+        }
+
+  conv_name = '{}{}{}'.format(conv_prefix, out_layer, conv_postfix)
+  [kernel_h, kernel_w] = UnpackVariable(kernel_size, 2)
+  [pad_h, pad_w] = UnpackVariable(pad, 2)
+  [stride_h, stride_w] = UnpackVariable(stride, 2)
+  if kernel_h == kernel_w:
+    net[conv_name] = L.Convolution(net[from_layer], num_output=num_output,
+        kernel_size=kernel_h, pad=pad_h, stride=stride_h, **kwargs)
+  else:
+    net[conv_name] = L.Convolution(net[from_layer], num_output=num_output,
+        kernel_h=kernel_h, kernel_w=kernel_w, pad_h=pad_h, pad_w=pad_w,
+        stride_h=stride_h, stride_w=stride_w, **kwargs)
+  if dilation > 1:
+    net.update(conv_name, {'dilation': dilation})
+  if use_bn:
+    bn_name = '{}{}{}'.format(bn_prefix, out_layer, bn_postfix)
+    net[bn_name] = L.BatchNorm(net[conv_name], in_place=True)
+    if use_scale:
+      sb_name = '{}{}{}'.format(scale_prefix, out_layer, scale_postfix)
+      net[sb_name] = L.Scale(net[bn_name], in_place=True, **sb_kwargs)
+    else:
+      bias_name = '{}{}{}'.format(bias_prefix, out_layer, bias_postfix)
+      net[bias_name] = L.Bias(net[bn_name], in_place=True, **bias_kwargs)
+  if use_relu:
+    relu_name = '{}_relu'.format(conv_name)
+    net[relu_name] = L.ReLU(net[conv_name], in_place=True)
+
+def DeconvBNLayer(net, from_layer, out_layer, use_bn, use_relu, num_output,
+    kernel_size, pad, stride, use_scale=True, lr_mult=1, deconv_prefix='', deconv_postfix='',
+    bn_prefix='', bn_postfix='_bn', scale_prefix='', scale_postfix='_scale', bias_prefix='',
+    bias_postfix='_bias', **bn_params):
+  if use_bn:
+    kwargs = {
+        'param': [dict(lr_mult=lr_mult, decay_mult=1)],
+        'weight_filler': dict(type='gaussian', std=0.01),
+        'bias_term': False,
+        }
+    # parameters for scale bias layer after batchnorm.
+    if use_scale:
+      sb_kwargs = {
+          'bias_term': True,
+          }
+    else:
+      bias_kwargs = {
+          'param': [dict(lr_mult=lr_mult, decay_mult=0)],
+          'filler': dict(type='constant', value=0.0),
+          }
+  else:
+    kwargs = {
+        'param': [
+            dict(lr_mult=lr_mult, decay_mult=1),
+            dict(lr_mult=2 * lr_mult, decay_mult=0)],
+        'weight_filler': dict(type='xavier'),
+        'bias_filler': dict(type='constant', value=0)
+        }
+  deconv_name = '{}{}{}'.format(deconv_prefix, out_layer, deconv_postfix)
+  net[deconv_name] = L.Deconvolution(net[from_layer], num_output=num_output,
+      kernel_size=kernel_size, pad=pad, stride=stride, **kwargs)
+
+  if use_bn:
+      bn_name = '{}{}{}'.format(bn_prefix, out_layer, bn_postfix)
+      net[bn_name] = L.BatchNorm(net[deconv_name], in_place=True)
+      if use_scale:
+          sb_name = '{}{}{}'.format(scale_prefix, out_layer, scale_postfix)
+          net[sb_name] = L.Scale(net[bn_name], in_place=True, **sb_kwargs)
+      else:
+          bias_name = '{}{}{}'.format(bias_prefix, out_layer, bias_postfix)
+          net[bias_name] = L.Bias(net[bn_name], in_place=True, **bias_kwargs)
+
+  if use_relu:
+      relu_name = '{}_relu'.format(deconv_name)
+      net[relu_name] = L.ReLU(net[deconv_name], in_place=True)
+
+
+def EltwiseLayer(net, from_layer, out_layer):
+  elt_name = out_layer
+  net[elt_name] = L.Eltwise(net[from_layer[0]], net[from_layer[1]])
+
+
+def ResBody(net, from_layer, block_name, out2a, out2b, out2c, stride, use_branch1, dilation=1, **bn_param):
+  # ResBody(net, 'pool1', '2a', 64, 64, 256, 1, True)
+
+  conv_prefix = 'res{}_'.format(block_name)
+  conv_postfix = ''
+  bn_prefix = 'bn{}_'.format(block_name)
+  bn_postfix = ''
+  scale_prefix = 'scale{}_'.format(block_name)
+  scale_postfix = ''
+  use_scale = True
+
+  if use_branch1:
+    branch_name = 'branch1'
+    ConvBNLayer(net, from_layer, branch_name, use_bn=True, use_relu=False,
+        num_output=out2c, kernel_size=1, pad=0, stride=stride, use_scale=use_scale,
+        conv_prefix=conv_prefix, conv_postfix=conv_postfix,
+        bn_prefix=bn_prefix, bn_postfix=bn_postfix,
+        scale_prefix=scale_prefix, scale_postfix=scale_postfix, **bn_param)
+    branch1 = '{}{}'.format(conv_prefix, branch_name)
+  else:
+    branch1 = from_layer
+
+  branch_name = 'branch2a'
+  ConvBNLayer(net, from_layer, branch_name, use_bn=True, use_relu=True,
+      num_output=out2a, kernel_size=1, pad=0, stride=stride, use_scale=use_scale,
+      conv_prefix=conv_prefix, conv_postfix=conv_postfix,
+      bn_prefix=bn_prefix, bn_postfix=bn_postfix,
+      scale_prefix=scale_prefix, scale_postfix=scale_postfix, **bn_param)
+  out_name = '{}{}'.format(conv_prefix, branch_name)
+
+  branch_name = 'branch2b'
+  if dilation == 1:
+    ConvBNLayer(net, out_name, branch_name, use_bn=True, use_relu=True,
+        num_output=out2b, kernel_size=3, pad=1, stride=1, use_scale=use_scale,
+        conv_prefix=conv_prefix, conv_postfix=conv_postfix,
+        bn_prefix=bn_prefix, bn_postfix=bn_postfix,
+        scale_prefix=scale_prefix, scale_postfix=scale_postfix, **bn_param)
+  else:
+    pad = int((3 + (dilation - 1) * 2) - 1) / 2
+    ConvBNLayer(net, out_name, branch_name, use_bn=True, use_relu=True,
+        num_output=out2b, kernel_size=3, pad=pad, stride=1, use_scale=use_scale,
+        dilation=dilation, conv_prefix=conv_prefix, conv_postfix=conv_postfix,
+        bn_prefix=bn_prefix, bn_postfix=bn_postfix,
+        scale_prefix=scale_prefix, scale_postfix=scale_postfix, **bn_param)
+  out_name = '{}{}'.format(conv_prefix, branch_name)
+
+  branch_name = 'branch2c'
+  ConvBNLayer(net, out_name, branch_name, use_bn=True, use_relu=False,
+      num_output=out2c, kernel_size=1, pad=0, stride=1, use_scale=use_scale,
+      conv_prefix=conv_prefix, conv_postfix=conv_postfix,
+      bn_prefix=bn_prefix, bn_postfix=bn_postfix,
+      scale_prefix=scale_prefix, scale_postfix=scale_postfix, **bn_param)
+  branch2 = '{}{}'.format(conv_prefix, branch_name)
+
+  res_name = 'res{}'.format(block_name)
+  net[res_name] = L.Eltwise(net[branch1], net[branch2])
+  relu_name = '{}_relu'.format(res_name)
+  net[relu_name] = L.ReLU(net[res_name], in_place=True)
+
+
+def InceptionTower(net, from_layer, tower_name, layer_params, **bn_param):
+  use_scale = False
+  for param in layer_params:
+    tower_layer = '{}/{}'.format(tower_name, param['name'])
+    del param['name']
+    if 'pool' in tower_layer:
+      net[tower_layer] = L.Pooling(net[from_layer], **param)
+    else:
+      param.update(bn_param)
+      ConvBNLayer(net, from_layer, tower_layer, use_bn=True, use_relu=True,
+          use_scale=use_scale, **param)
+    from_layer = tower_layer
+  return net[from_layer]
+
+def CreateAnnotatedDataLayer(source, batch_size=32, backend=P.Data.LMDB,
+        output_label=True, train=True, label_map_file='', anno_type=None,
+        transform_param={}, batch_sampler=[{}]):
+    if train:
+        kwargs = {
+                'include': dict(phase=caffe_pb2.Phase.Value('TRAIN')),
+                'transform_param': transform_param,
+                }
+    else:
+        kwargs = {
+                'include': dict(phase=caffe_pb2.Phase.Value('TEST')),
+                'transform_param': transform_param,
+                }
+    ntop = 1
+    if output_label:
+        ntop = 2
+    annotated_data_param = {
+        'label_map_file': label_map_file,
+        'batch_sampler': batch_sampler,
+        }
+    if anno_type is not None:
+        annotated_data_param.update({'anno_type': anno_type})
+    return L.AnnotatedData(name="data", annotated_data_param=annotated_data_param,
+        data_param=dict(batch_size=batch_size, backend=backend, source=source),
+        ntop=ntop, **kwargs)
+
+
+def ZFNetBody(net, from_layer, need_fc=True, fully_conv=False, reduced=False,
+        dilated=False, dropout=True, need_fc8=False, freeze_layers=[]):
+    kwargs = {
+            'param': [dict(lr_mult=1, decay_mult=1), dict(lr_mult=2, decay_mult=0)],
+            'weight_filler': dict(type='xavier'),
+            'bias_filler': dict(type='constant', value=0)}
+
+    assert from_layer in net.keys()
+    net.conv1 = L.Convolution(net[from_layer], num_output=96, pad=3, kernel_size=7, stride=2, **kwargs)
+    net.relu1 = L.ReLU(net.conv1, in_place=True)
+
+    net.norm1 = L.LRN(net.relu1, local_size=3, alpha=0.00005, beta=0.75,
+            norm_region=P.LRN.WITHIN_CHANNEL, engine=P.LRN.CAFFE)
+
+    net.pool1 = L.Pooling(net.norm1, pool=P.Pooling.MAX, pad=1, kernel_size=3, stride=2)
+
+    net.conv2 = L.Convolution(net.pool1, num_output=256, pad=2, kernel_size=5, stride=2, **kwargs)
+    net.relu2 = L.ReLU(net.conv2, in_place=True)
+
+    net.norm2 = L.LRN(net.relu2, local_size=3, alpha=0.00005, beta=0.75,
+            norm_region=P.LRN.WITHIN_CHANNEL, engine=P.LRN.CAFFE)
+
+    net.pool2 = L.Pooling(net.norm2, pool=P.Pooling.MAX, pad=1, kernel_size=3, stride=2)
+
+    net.conv3 = L.Convolution(net.pool2, num_output=384, pad=1, kernel_size=3, **kwargs)
+    net.relu3 = L.ReLU(net.conv3, in_place=True)
+    net.conv4 = L.Convolution(net.relu3, num_output=384, pad=1, kernel_size=3, **kwargs)
+    net.relu4 = L.ReLU(net.conv4, in_place=True)
+    net.conv5 = L.Convolution(net.relu4, num_output=256, pad=1, kernel_size=3, **kwargs)
+    net.relu5 = L.ReLU(net.conv5, in_place=True)
+
+    if need_fc:
+        if dilated:
+            name = 'pool5'
+            net[name] = L.Pooling(net.relu5, pool=P.Pooling.MAX, pad=1, kernel_size=3, stride=1)
+        else:
+            name = 'pool5'
+            net[name] = L.Pooling(net.relu5, pool=P.Pooling.MAX, pad=1, kernel_size=3, stride=2)
+
+        if fully_conv:
+            if dilated:
+                if reduced:
+                    net.fc6 = L.Convolution(net[name], num_output=1024, pad=5, kernel_size=3, dilation=5, **kwargs)
+                else:
+                    net.fc6 = L.Convolution(net[name], num_output=4096, pad=5, kernel_size=6, dilation=2, **kwargs)
+            else:
+                if reduced:
+                    net.fc6 = L.Convolution(net[name], num_output=1024, pad=2, kernel_size=3, dilation=2,  **kwargs)
+                else:
+                    net.fc6 = L.Convolution(net[name], num_output=4096, pad=2, kernel_size=6, **kwargs)
+
+            net.relu6 = L.ReLU(net.fc6, in_place=True)
+            if dropout:
+                net.drop6 = L.Dropout(net.relu6, dropout_ratio=0.5, in_place=True)
+
+            if reduced:
+                net.fc7 = L.Convolution(net.relu6, num_output=1024, kernel_size=1, **kwargs)
+            else:
+                net.fc7 = L.Convolution(net.relu6, num_output=4096, kernel_size=1, **kwargs)
+            net.relu7 = L.ReLU(net.fc7, in_place=True)
+            if dropout:
+                net.drop7 = L.Dropout(net.relu7, dropout_ratio=0.5, in_place=True)
+        else:
+            net.fc6 = L.InnerProduct(net.pool5, num_output=4096)
+            net.relu6 = L.ReLU(net.fc6, in_place=True)
+            if dropout:
+                net.drop6 = L.Dropout(net.relu6, dropout_ratio=0.5, in_place=True)
+            net.fc7 = L.InnerProduct(net.relu6, num_output=4096)
+            net.relu7 = L.ReLU(net.fc7, in_place=True)
+            if dropout:
+                net.drop7 = L.Dropout(net.relu7, dropout_ratio=0.5, in_place=True)
+    if need_fc8:
+        from_layer = net.keys()[-1]
+        if fully_conv:
+            net.fc8 = L.Convolution(net[from_layer], num_output=1000, kernel_size=1, **kwargs)
+        else:
+            net.fc8 = L.InnerProduct(net[from_layer], num_output=1000)
+        net.prob = L.Softmax(net.fc8)
+
+    # Update freeze layers.
+    kwargs['param'] = [dict(lr_mult=0, decay_mult=0), dict(lr_mult=0, decay_mult=0)]
+    layers = net.keys()
+    for freeze_layer in freeze_layers:
+        if freeze_layer in layers:
+            net.update(freeze_layer, kwargs)
+
+    return net
+
+
+def VGGNetBody(net, from_layer, need_fc=True, fully_conv=False, reduced=False,
+        dilated=False, nopool=False, dropout=True, freeze_layers=[], dilate_pool4=False):
+    kwargs = {
+            'param': [dict(lr_mult=1, decay_mult=1), dict(lr_mult=2, decay_mult=0)],
+            'weight_filler': dict(type='xavier'),
+            'bias_filler': dict(type='constant', value=0)}
+
+    assert from_layer in net.keys()
+    net.conv1_1 = L.Convolution(net[from_layer], num_output=64, pad=1, kernel_size=3, **kwargs)
+
+    net.relu1_1 = L.ReLU(net.conv1_1, in_place=True)
+    net.conv1_2 = L.Convolution(net.relu1_1, num_output=64, pad=1, kernel_size=3, **kwargs)
+    net.relu1_2 = L.ReLU(net.conv1_2, in_place=True)
+
+    if nopool:
+        name = 'conv1_3'
+        net[name] = L.Convolution(net.relu1_2, num_output=64, pad=1, kernel_size=3, stride=2, **kwargs)
+    else:
+        name = 'pool1'
+        net.pool1 = L.Pooling(net.relu1_2, pool=P.Pooling.MAX, kernel_size=2, stride=2)
+
+    net.conv2_1 = L.Convolution(net[name], num_output=128, pad=1, kernel_size=3, **kwargs)
+    net.relu2_1 = L.ReLU(net.conv2_1, in_place=True)
+    net.conv2_2 = L.Convolution(net.relu2_1, num_output=128, pad=1, kernel_size=3, **kwargs)
+    net.relu2_2 = L.ReLU(net.conv2_2, in_place=True)
+
+    if nopool:
+        name = 'conv2_3'
+        net[name] = L.Convolution(net.relu2_2, num_output=128, pad=1, kernel_size=3, stride=2, **kwargs)
+    else:
+        name = 'pool2'
+        net[name] = L.Pooling(net.relu2_2, pool=P.Pooling.MAX, kernel_size=2, stride=2)
+
+    net.conv3_1 = L.Convolution(net[name], num_output=256, pad=1, kernel_size=3, **kwargs)
+    net.relu3_1 = L.ReLU(net.conv3_1, in_place=True)
+    net.conv3_2 = L.Convolution(net.relu3_1, num_output=256, pad=1, kernel_size=3, **kwargs)
+    net.relu3_2 = L.ReLU(net.conv3_2, in_place=True)
+    net.conv3_3 = L.Convolution(net.relu3_2, num_output=256, pad=1, kernel_size=3, **kwargs)
+    net.relu3_3 = L.ReLU(net.conv3_3, in_place=True)
+
+    if nopool:
+        name = 'conv3_4'
+        net[name] = L.Convolution(net.relu3_3, num_output=256, pad=1, kernel_size=3, stride=2, **kwargs)
+    else:
+        name = 'pool3'
+        net[name] = L.Pooling(net.relu3_3, pool=P.Pooling.MAX, kernel_size=2, stride=2)
+
+    net.conv4_1 = L.Convolution(net[name], num_output=512, pad=1, kernel_size=3, **kwargs)
+    net.relu4_1 = L.ReLU(net.conv4_1, in_place=True)
+    net.conv4_2 = L.Convolution(net.relu4_1, num_output=512, pad=1, kernel_size=3, **kwargs)
+    net.relu4_2 = L.ReLU(net.conv4_2, in_place=True)
+    net.conv4_3 = L.Convolution(net.relu4_2, num_output=512, pad=1, kernel_size=3, **kwargs)
+    net.relu4_3 = L.ReLU(net.conv4_3, in_place=True)
+
+    if nopool:
+        name = 'conv4_4'
+        net[name] = L.Convolution(net.relu4_3, num_output=512, pad=1, kernel_size=3, stride=2, **kwargs)
+    else:
+        name = 'pool4'
+        if dilate_pool4:
+            net[name] = L.Pooling(net.relu4_3, pool=P.Pooling.MAX, kernel_size=3, stride=1, pad=1)
+            dilation = 2
+        else:
+            net[name] = L.Pooling(net.relu4_3, pool=P.Pooling.MAX, kernel_size=2, stride=2)
+            dilation = 1
+
+    kernel_size = 3
+    pad = int((kernel_size + (dilation - 1) * (kernel_size - 1)) - 1) / 2
+    net.conv5_1 = L.Convolution(net[name], num_output=512, pad=pad, kernel_size=kernel_size, dilation=dilation, **kwargs)
+    net.relu5_1 = L.ReLU(net.conv5_1, in_place=True)
+    net.conv5_2 = L.Convolution(net.relu5_1, num_output=512, pad=pad, kernel_size=kernel_size, dilation=dilation, **kwargs)
+    net.relu5_2 = L.ReLU(net.conv5_2, in_place=True)
+    net.conv5_3 = L.Convolution(net.relu5_2, num_output=512, pad=pad, kernel_size=kernel_size, dilation=dilation, **kwargs)
+    net.relu5_3 = L.ReLU(net.conv5_3, in_place=True)
+
+    if need_fc:
+        if dilated:
+            if nopool:
+                name = 'conv5_4'
+                net[name] = L.Convolution(net.relu5_3, num_output=512, pad=1, kernel_size=3, stride=1, **kwargs)
+            else:
+                name = 'pool5'
+                net[name] = L.Pooling(net.relu5_3, pool=P.Pooling.MAX, pad=1, kernel_size=3, stride=1)
+        else:
+            if nopool:
+                name = 'conv5_4'
+                net[name] = L.Convolution(net.relu5_3, num_output=512, pad=1, kernel_size=3, stride=2, **kwargs)
+            else:
+                name = 'pool5'
+                net[name] = L.Pooling(net.relu5_3, pool=P.Pooling.MAX, kernel_size=2, stride=2)
+
+        if fully_conv:
+            if dilated:
+                if reduced:
+                    dilation = dilation * 6
+                    kernel_size = 3
+                    num_output = 1024
+                else:
+                    dilation = dilation * 2
+                    kernel_size = 7
+                    num_output = 4096
+            else:
+                if reduced:
+                    dilation = dilation * 3
+                    kernel_size = 3
+                    num_output = 1024
+                else:
+                    kernel_size = 7
+                    num_output = 4096
+            pad = int((kernel_size + (dilation - 1) * (kernel_size - 1)) - 1) / 2
+            net.fc6 = L.Convolution(net[name], num_output=num_output, pad=pad, kernel_size=kernel_size, dilation=dilation, **kwargs)
+
+            net.relu6 = L.ReLU(net.fc6, in_place=True)
+            if dropout:
+                net.drop6 = L.Dropout(net.relu6, dropout_ratio=0.5, in_place=True)
+
+            if reduced:
+                net.fc7 = L.Convolution(net.relu6, num_output=1024, kernel_size=1, **kwargs)
+            else:
+                net.fc7 = L.Convolution(net.relu6, num_output=4096, kernel_size=1, **kwargs)
+            net.relu7 = L.ReLU(net.fc7, in_place=True)
+            if dropout:
+                net.drop7 = L.Dropout(net.relu7, dropout_ratio=0.5, in_place=True)
+        else:
+            net.fc6 = L.InnerProduct(net.pool5, num_output=4096)
+            net.relu6 = L.ReLU(net.fc6, in_place=True)
+            if dropout:
+                net.drop6 = L.Dropout(net.relu6, dropout_ratio=0.5, in_place=True)
+            net.fc7 = L.InnerProduct(net.relu6, num_output=4096)
+            net.relu7 = L.ReLU(net.fc7, in_place=True)
+            if dropout:
+                net.drop7 = L.Dropout(net.relu7, dropout_ratio=0.5, in_place=True)
+
+    # Update freeze layers.
+    kwargs['param'] = [dict(lr_mult=0, decay_mult=0), dict(lr_mult=0, decay_mult=0)]
+    layers = net.keys()
+    for freeze_layer in freeze_layers:
+        if freeze_layer in layers:
+            net.update(freeze_layer, kwargs)
+
+    return net
+
+
+def ResNet101Body(net, from_layer, use_pool5=True, use_dilation_conv5=False, **bn_param):
+    conv_prefix = ''
+    conv_postfix = ''
+    bn_prefix = 'bn_'
+    bn_postfix = ''
+    scale_prefix = 'scale_'
+    scale_postfix = ''
+    ConvBNLayer(net, from_layer, 'conv1', use_bn=True, use_relu=True,
+        num_output=64, kernel_size=7, pad=3, stride=2,
+        conv_prefix=conv_prefix, conv_postfix=conv_postfix,
+        bn_prefix=bn_prefix, bn_postfix=bn_postfix,
+        scale_prefix=scale_prefix, scale_postfix=scale_postfix, **bn_param)
+
+    net.pool1 = L.Pooling(net.conv1, pool=P.Pooling.MAX, kernel_size=3, stride=2)
+
+    ResBody(net, 'pool1', '2a', out2a=64, out2b=64, out2c=256, stride=1, use_branch1=True, **bn_param)
+    ResBody(net, 'res2a', '2b', out2a=64, out2b=64, out2c=256, stride=1, use_branch1=False, **bn_param)
+    ResBody(net, 'res2b', '2c', out2a=64, out2b=64, out2c=256, stride=1, use_branch1=False, **bn_param)
+
+    ResBody(net, 'res2c', '3a', out2a=128, out2b=128, out2c=512, stride=2, use_branch1=True, **bn_param)
+
+    from_layer = 'res3a'
+    for i in xrange(1, 4):
+      block_name = '3b{}'.format(i)
+      ResBody(net, from_layer, block_name, out2a=128, out2b=128, out2c=512, stride=1, use_branch1=False, **bn_param)
+      from_layer = 'res{}'.format(block_name)
+
+    ResBody(net, from_layer, '4a', out2a=256, out2b=256, out2c=1024, stride=2, use_branch1=True, **bn_param)
+
+    from_layer = 'res4a'
+    for i in xrange(1, 23):
+      block_name = '4b{}'.format(i)
+      ResBody(net, from_layer, block_name, out2a=256, out2b=256, out2c=1024, stride=1, use_branch1=False, **bn_param)
+      from_layer = 'res{}'.format(block_name)
+
+    stride = 2
+    dilation = 1
+    if use_dilation_conv5:
+      stride = 1
+      dilation = 2
+
+    ResBody(net, from_layer, '5a', out2a=512, out2b=512, out2c=2048, stride=stride, use_branch1=True, dilation=dilation, **bn_param)
+    ResBody(net, 'res5a', '5b', out2a=512, out2b=512, out2c=2048, stride=1, use_branch1=False, dilation=dilation, **bn_param)
+    ResBody(net, 'res5b', '5c', out2a=512, out2b=512, out2c=2048, stride=1, use_branch1=False, dilation=dilation, **bn_param)
+
+    if use_pool5:
+      net.pool5 = L.Pooling(net.res5c, pool=P.Pooling.AVE, global_pooling=True)
+
+    return net
+
+
+def ResNet152Body(net, from_layer, use_pool5=True, use_dilation_conv5=False, **bn_param):
+    conv_prefix = ''
+    conv_postfix = ''
+    bn_prefix = 'bn_'
+    bn_postfix = ''
+    scale_prefix = 'scale_'
+    scale_postfix = ''
+    ConvBNLayer(net, from_layer, 'conv1', use_bn=True, use_relu=True,
+        num_output=64, kernel_size=7, pad=3, stride=2,
+        conv_prefix=conv_prefix, conv_postfix=conv_postfix,
+        bn_prefix=bn_prefix, bn_postfix=bn_postfix,
+        scale_prefix=scale_prefix, scale_postfix=scale_postfix, **bn_param)
+
+    net.pool1 = L.Pooling(net.conv1, pool=P.Pooling.MAX, kernel_size=3, stride=2)
+
+    ResBody(net, 'pool1', '2a', out2a=64, out2b=64, out2c=256, stride=1, use_branch1=True, **bn_param)
+    ResBody(net, 'res2a', '2b', out2a=64, out2b=64, out2c=256, stride=1, use_branch1=False, **bn_param)
+    ResBody(net, 'res2b', '2c', out2a=64, out2b=64, out2c=256, stride=1, use_branch1=False, **bn_param)
+
+    ResBody(net, 'res2c', '3a', out2a=128, out2b=128, out2c=512, stride=2, use_branch1=True, **bn_param)
+
+    from_layer = 'res3a'
+    for i in xrange(1, 8):
+      block_name = '3b{}'.format(i)
+      ResBody(net, from_layer, block_name, out2a=128, out2b=128, out2c=512, stride=1, use_branch1=False, **bn_param)
+      from_layer = 'res{}'.format(block_name)
+
+    ResBody(net, from_layer, '4a', out2a=256, out2b=256, out2c=1024, stride=2, use_branch1=True, **bn_param)
+
+    from_layer = 'res4a'
+    for i in xrange(1, 36):
+      block_name = '4b{}'.format(i)
+      ResBody(net, from_layer, block_name, out2a=256, out2b=256, out2c=1024, stride=1, use_branch1=False, **bn_param)
+      from_layer = 'res{}'.format(block_name)
+
+    stride = 2
+    dilation = 1
+    if use_dilation_conv5:
+      stride = 1
+      dilation = 2
+
+    ResBody(net, from_layer, '5a', out2a=512, out2b=512, out2c=2048, stride=stride, use_branch1=True, dilation=dilation, **bn_param)
+    ResBody(net, 'res5a', '5b', out2a=512, out2b=512, out2c=2048, stride=1, use_branch1=False, dilation=dilation, **bn_param)
+    ResBody(net, 'res5b', '5c', out2a=512, out2b=512, out2c=2048, stride=1, use_branch1=False, dilation=dilation, **bn_param)
+
+    if use_pool5:
+      net.pool5 = L.Pooling(net.res5c, pool=P.Pooling.AVE, global_pooling=True)
+
+    return net
+
+
+def InceptionV3Body(net, from_layer, output_pred=False, **bn_param):
+  # scale is fixed to 1, thus we ignore it.
+  use_scale = False
+
+  out_layer = 'conv'
+  ConvBNLayer(net, from_layer, out_layer, use_bn=True, use_relu=True,
+      num_output=32, kernel_size=3, pad=0, stride=2, use_scale=use_scale,
+      **bn_param)
+  from_layer = out_layer
+
+  out_layer = 'conv_1'
+  ConvBNLayer(net, from_layer, out_layer, use_bn=True, use_relu=True,
+      num_output=32, kernel_size=3, pad=0, stride=1, use_scale=use_scale,
+      **bn_param)
+  from_layer = out_layer
+
+  out_layer = 'conv_2'
+  ConvBNLayer(net, from_layer, out_layer, use_bn=True, use_relu=True,
+      num_output=64, kernel_size=3, pad=1, stride=1, use_scale=use_scale,
+      **bn_param)
+  from_layer = out_layer
+
+  out_layer = 'pool'
+  net[out_layer] = L.Pooling(net[from_layer], pool=P.Pooling.MAX,
+      kernel_size=3, stride=2, pad=0)
+  from_layer = out_layer
+
+  out_layer = 'conv_3'
+  ConvBNLayer(net, from_layer, out_layer, use_bn=True, use_relu=True,
+      num_output=80, kernel_size=1, pad=0, stride=1, use_scale=use_scale,
+      **bn_param)
+  from_layer = out_layer
+
+  out_layer = 'conv_4'
+  ConvBNLayer(net, from_layer, out_layer, use_bn=True, use_relu=True,
+      num_output=192, kernel_size=3, pad=0, stride=1, use_scale=use_scale,
+      **bn_param)
+  from_layer = out_layer
+
+  out_layer = 'pool_1'
+  net[out_layer] = L.Pooling(net[from_layer], pool=P.Pooling.MAX,
+      kernel_size=3, stride=2, pad=0)
+  from_layer = out_layer
+
+  # inceptions with 1x1, 3x3, 5x5 convolutions
+  for inception_id in xrange(0, 3):
+    if inception_id == 0:
+      out_layer = 'mixed'
+      tower_2_conv_num_output = 32
+    else:
+      out_layer = 'mixed_{}'.format(inception_id)
+      tower_2_conv_num_output = 64
+    towers = []
+    tower_name = '{}'.format(out_layer)
+    tower = InceptionTower(net, from_layer, tower_name, [
+        dict(name='conv', num_output=64, kernel_size=1, pad=0, stride=1),
+        ], **bn_param)
+    towers.append(tower)
+    tower_name = '{}/tower'.format(out_layer)
+    tower = InceptionTower(net, from_layer, tower_name, [
+        dict(name='conv', num_output=48, kernel_size=1, pad=0, stride=1),
+        dict(name='conv_1', num_output=64, kernel_size=5, pad=2, stride=1),
+        ], **bn_param)
+    towers.append(tower)
+    tower_name = '{}/tower_1'.format(out_layer)
+    tower = InceptionTower(net, from_layer, tower_name, [
+        dict(name='conv', num_output=64, kernel_size=1, pad=0, stride=1),
+        dict(name='conv_1', num_output=96, kernel_size=3, pad=1, stride=1),
+        dict(name='conv_2', num_output=96, kernel_size=3, pad=1, stride=1),
+        ], **bn_param)
+    towers.append(tower)
+    tower_name = '{}/tower_2'.format(out_layer)
+    tower = InceptionTower(net, from_layer, tower_name, [
+        dict(name='pool', pool=P.Pooling.AVE, kernel_size=3, pad=1, stride=1),
+        dict(name='conv', num_output=tower_2_conv_num_output, kernel_size=1, pad=0, stride=1),
+        ], **bn_param)
+    towers.append(tower)
+    out_layer = '{}/join'.format(out_layer)
+    net[out_layer] = L.Concat(*towers, axis=1)
+    from_layer = out_layer
+
+  # inceptions with 1x1, 3x3(in sequence) convolutions
+  out_layer = 'mixed_3'
+  towers = []
+  tower_name = '{}'.format(out_layer)
+  tower = InceptionTower(net, from_layer, tower_name, [
+      dict(name='conv', num_output=384, kernel_size=3, pad=0, stride=2),
+      ], **bn_param)
+  towers.append(tower)
+  tower_name = '{}/tower'.format(out_layer)
+  tower = InceptionTower(net, from_layer, tower_name, [
+      dict(name='conv', num_output=64, kernel_size=1, pad=0, stride=1),
+      dict(name='conv_1', num_output=96, kernel_size=3, pad=1, stride=1),
+      dict(name='conv_2', num_output=96, kernel_size=3, pad=0, stride=2),
+      ], **bn_param)
+  towers.append(tower)
+  tower_name = '{}'.format(out_layer)
+  tower = InceptionTower(net, from_layer, tower_name, [
+      dict(name='pool', pool=P.Pooling.MAX, kernel_size=3, pad=0, stride=2),
+      ], **bn_param)
+  towers.append(tower)
+  out_layer = '{}/join'.format(out_layer)
+  net[out_layer] = L.Concat(*towers, axis=1)
+  from_layer = out_layer
+
+  # inceptions with 1x1, 7x1, 1x7 convolutions
+  for inception_id in xrange(4, 8):
+    if inception_id == 4:
+      num_output = 128
+    elif inception_id == 5 or inception_id == 6:
+      num_output = 160
+    elif inception_id == 7:
+      num_output = 192
+    out_layer = 'mixed_{}'.format(inception_id)
+    towers = []
+    tower_name = '{}'.format(out_layer)
+    tower = InceptionTower(net, from_layer, tower_name, [
+        dict(name='conv', num_output=192, kernel_size=1, pad=0, stride=1),
+        ], **bn_param)
+    towers.append(tower)
+    tower_name = '{}/tower'.format(out_layer)
+    tower = InceptionTower(net, from_layer, tower_name, [
+        dict(name='conv', num_output=num_output, kernel_size=1, pad=0, stride=1),
+        dict(name='conv_1', num_output=num_output, kernel_size=[1, 7], pad=[0, 3], stride=[1, 1]),
+        dict(name='conv_2', num_output=192, kernel_size=[7, 1], pad=[3, 0], stride=[1, 1]),
+        ], **bn_param)
+    towers.append(tower)
+    tower_name = '{}/tower_1'.format(out_layer)
+    tower = InceptionTower(net, from_layer, tower_name, [
+        dict(name='conv', num_output=num_output, kernel_size=1, pad=0, stride=1),
+        dict(name='conv_1', num_output=num_output, kernel_size=[7, 1], pad=[3, 0], stride=[1, 1]),
+        dict(name='conv_2', num_output=num_output, kernel_size=[1, 7], pad=[0, 3], stride=[1, 1]),
+        dict(name='conv_3', num_output=num_output, kernel_size=[7, 1], pad=[3, 0], stride=[1, 1]),
+        dict(name='conv_4', num_output=192, kernel_size=[1, 7], pad=[0, 3], stride=[1, 1]),
+        ], **bn_param)
+    towers.append(tower)
+    tower_name = '{}/tower_2'.format(out_layer)
+    tower = InceptionTower(net, from_layer, tower_name, [
+        dict(name='pool', pool=P.Pooling.AVE, kernel_size=3, pad=1, stride=1),
+        dict(name='conv', num_output=192, kernel_size=1, pad=0, stride=1),
+        ], **bn_param)
+    towers.append(tower)
+    out_layer = '{}/join'.format(out_layer)
+    net[out_layer] = L.Concat(*towers, axis=1)
+    from_layer = out_layer
+
+  # inceptions with 1x1, 3x3, 1x7, 7x1 filters
+  out_layer = 'mixed_8'
+  towers = []
+  tower_name = '{}/tower'.format(out_layer)
+  tower = InceptionTower(net, from_layer, tower_name, [
+      dict(name='conv', num_output=192, kernel_size=1, pad=0, stride=1),
+      dict(name='conv_1', num_output=320, kernel_size=3, pad=0, stride=2),
+      ], **bn_param)
+  towers.append(tower)
+  tower_name = '{}/tower_1'.format(out_layer)
+  tower = InceptionTower(net, from_layer, tower_name, [
+      dict(name='conv', num_output=192, kernel_size=1, pad=0, stride=1),
+      dict(name='conv_1', num_output=192, kernel_size=[1, 7], pad=[0, 3], stride=[1, 1]),
+      dict(name='conv_2', num_output=192, kernel_size=[7, 1], pad=[3, 0], stride=[1, 1]),
+      dict(name='conv_3', num_output=192, kernel_size=3, pad=0, stride=2),
+      ], **bn_param)
+  towers.append(tower)
+  tower_name = '{}'.format(out_layer)
+  tower = InceptionTower(net, from_layer, tower_name, [
+      dict(name='pool', pool=P.Pooling.MAX, kernel_size=3, pad=0, stride=2),
+      ], **bn_param)
+  towers.append(tower)
+  out_layer = '{}/join'.format(out_layer)
+  net[out_layer] = L.Concat(*towers, axis=1)
+  from_layer = out_layer
+
+  for inception_id in xrange(9, 11):
+    num_output = 384
+    num_output2 = 448
+    if inception_id == 9:
+      pool = P.Pooling.AVE
+    else:
+      pool = P.Pooling.MAX
+    out_layer = 'mixed_{}'.format(inception_id)
+    towers = []
+    tower_name = '{}'.format(out_layer)
+    tower = InceptionTower(net, from_layer, tower_name, [
+        dict(name='conv', num_output=320, kernel_size=1, pad=0, stride=1),
+        ], **bn_param)
+    towers.append(tower)
+
+    tower_name = '{}/tower'.format(out_layer)
+    tower = InceptionTower(net, from_layer, tower_name, [
+        dict(name='conv', num_output=num_output, kernel_size=1, pad=0, stride=1),
+        ], **bn_param)
+    subtowers = []
+    subtower_name = '{}/mixed'.format(tower_name)
+    subtower = InceptionTower(net, '{}/conv'.format(tower_name), subtower_name, [
+        dict(name='conv', num_output=num_output, kernel_size=[1, 3], pad=[0, 1], stride=[1, 1]),
+        ], **bn_param)
+    subtowers.append(subtower)
+    subtower = InceptionTower(net, '{}/conv'.format(tower_name), subtower_name, [
+        dict(name='conv_1', num_output=num_output, kernel_size=[3, 1], pad=[1, 0], stride=[1, 1]),
+        ], **bn_param)
+    subtowers.append(subtower)
+    net[subtower_name] = L.Concat(*subtowers, axis=1)
+    towers.append(net[subtower_name])
+
+    tower_name = '{}/tower_1'.format(out_layer)
+    tower = InceptionTower(net, from_layer, tower_name, [
+        dict(name='conv', num_output=num_output2, kernel_size=1, pad=0, stride=1),
+        dict(name='conv_1', num_output=num_output, kernel_size=3, pad=1, stride=1),
+        ], **bn_param)
+    subtowers = []
+    subtower_name = '{}/mixed'.format(tower_name)
+    subtower = InceptionTower(net, '{}/conv_1'.format(tower_name), subtower_name, [
+        dict(name='conv', num_output=num_output, kernel_size=[1, 3], pad=[0, 1], stride=[1, 1]),
+        ], **bn_param)
+    subtowers.append(subtower)
+    subtower = InceptionTower(net, '{}/conv_1'.format(tower_name), subtower_name, [
+        dict(name='conv_1', num_output=num_output, kernel_size=[3, 1], pad=[1, 0], stride=[1, 1]),
+        ], **bn_param)
+    subtowers.append(subtower)
+    net[subtower_name] = L.Concat(*subtowers, axis=1)
+    towers.append(net[subtower_name])
+
+    tower_name = '{}/tower_2'.format(out_layer)
+    tower = InceptionTower(net, from_layer, tower_name, [
+        dict(name='pool', pool=pool, kernel_size=3, pad=1, stride=1),
+        dict(name='conv', num_output=192, kernel_size=1, pad=0, stride=1),
+        ], **bn_param)
+    towers.append(tower)
+    out_layer = '{}/join'.format(out_layer)
+    net[out_layer] = L.Concat(*towers, axis=1)
+    from_layer = out_layer
+
+  if output_pred:
+    net.pool_3 = L.Pooling(net[from_layer], pool=P.Pooling.AVE, kernel_size=8, pad=0, stride=1)
+    net.softmax = L.InnerProduct(net.pool_3, num_output=1008)
+    net.softmax_prob = L.Softmax(net.softmax)
+
+  return net
+
+def CreateMultiBoxHead(net, data_layer="data", num_classes=[], from_layers=[],
+        use_objectness=False, normalizations=[], use_batchnorm=True, lr_mult=1,
+        use_scale=True, min_sizes=[], max_sizes=[], prior_variance = [0.1],
+        aspect_ratios=[], steps=[], img_height=0, img_width=0, share_location=True,
+        flip=True, clip=True, offset=0.5, inter_layer_depth=[], kernel_size=1, pad=0, prefix='',
+        conf_postfix='', loc_postfix='', **bn_param):
+    assert num_classes, "must provide num_classes"
+    assert num_classes > 0, "num_classes must be positive number"
+    if normalizations:
+        assert len(from_layers) == len(normalizations), "from_layers and normalizations should have same length"
+    assert len(from_layers) == len(min_sizes), "from_layers and min_sizes should have same length"
+    if max_sizes:
+        assert len(from_layers) == len(max_sizes), "from_layers and max_sizes should have same length"
+    if aspect_ratios:
+        assert len(from_layers) == len(aspect_ratios), "from_layers and aspect_ratios should have same length"
+    if steps:
+        assert len(from_layers) == len(steps), "from_layers and steps should have same length"
+    net_layers = net.keys()
+    assert data_layer in net_layers, "data_layer is not in net's layers"
+    if inter_layer_depth:
+        assert len(from_layers) == len(inter_layer_depth), "from_layers and inter_layer_depth should have same length"
+
+    num = len(from_layers)
+    priorbox_layers = []
+    loc_layers = []
+    conf_layers = []
+    objectness_layers = []
+    for i in range(0, num):
+        from_layer = from_layers[i]
+
+        # Get the normalize value.
+        if normalizations:
+            if normalizations[i] != -1:
+                norm_name = "{}_norm".format(from_layer)
+                net[norm_name] = L.Normalize(net[from_layer], scale_filler=dict(type="constant", value=normalizations[i]),
+                    across_spatial=False, channel_shared=False)
+                from_layer = norm_name
+
+        # Add intermediate layers.
+        if inter_layer_depth:
+            if inter_layer_depth[i] > 0:
+                inter_name = "{}_inter".format(from_layer)
+                ConvBNLayer(net, from_layer, inter_name, use_bn=use_batchnorm, use_relu=True, lr_mult=lr_mult,
+                      num_output=inter_layer_depth[i], kernel_size=3, pad=1, stride=1, **bn_param)
+                from_layer = inter_name
+
+        # Estimate number of priors per location given provided parameters.
+        min_size = min_sizes[i]
+        if type(min_size) is not list:
+            min_size = [min_size]
+        aspect_ratio = []
+        if len(aspect_ratios) > i:
+            aspect_ratio = aspect_ratios[i]
+            if type(aspect_ratio) is not list:
+                aspect_ratio = [aspect_ratio]
+        max_size = []
+        if len(max_sizes) > i:
+            max_size = max_sizes[i]
+            if type(max_size) is not list:
+                max_size = [max_size]
+            if max_size:
+                assert len(max_size) == len(min_size), "max_size and min_size should have same length."
+        if max_size:
+            num_priors_per_location = (2 + len(aspect_ratio)) * len(min_size)
+        else:
+            num_priors_per_location = (1 + len(aspect_ratio)) * len(min_size)
+        if flip:
+            num_priors_per_location += len(aspect_ratio) * len(min_size)
+        step = []
+        if len(steps) > i:
+            step = steps[i]
+
+        # Create location prediction layer.
+        name = "{}_mbox_loc{}".format(from_layer, loc_postfix)
+        num_loc_output = num_priors_per_location * 4;
+        if not share_location:
+            num_loc_output *= num_classes
+        ConvBNLayer(net, from_layer, name, use_bn=use_batchnorm, use_relu=False, lr_mult=lr_mult,
+            num_output=num_loc_output, kernel_size=kernel_size, pad=pad, stride=1, **bn_param)
+        permute_name = "{}_perm".format(name)
+        net[permute_name] = L.Permute(net[name], order=[0, 2, 3, 1])
+        flatten_name = "{}_flat".format(name)
+        net[flatten_name] = L.Flatten(net[permute_name], axis=1)
+        loc_layers.append(net[flatten_name])
+
+        # Create confidence prediction layer.
+        name = "{}_mbox_conf{}".format(from_layer, conf_postfix)
+        num_conf_output = num_priors_per_location * num_classes;
+        ConvBNLayer(net, from_layer, name, use_bn=use_batchnorm, use_relu=False, lr_mult=lr_mult,
+            num_output=num_conf_output, kernel_size=kernel_size, pad=pad, stride=1, **bn_param)
+        permute_name = "{}_perm".format(name)
+        net[permute_name] = L.Permute(net[name], order=[0, 2, 3, 1])
+        flatten_name = "{}_flat".format(name)
+        net[flatten_name] = L.Flatten(net[permute_name], axis=1)
+        conf_layers.append(net[flatten_name])
+
+        # Create prior generation layer.
+        name = "{}_mbox_priorbox".format(from_layer)
+        net[name] = L.PriorBox(net[from_layer], net[data_layer], min_size=min_size,
+                clip=clip, variance=prior_variance, offset=offset)
+        if max_size:
+            net.update(name, {'max_size': max_size})
+        if aspect_ratio:
+            net.update(name, {'aspect_ratio': aspect_ratio, 'flip': flip})
+        if step:
+            net.update(name, {'step': step})
+        if img_height != 0 and img_width != 0:
+            if img_height == img_width:
+                net.update(name, {'img_size': img_height})
+            else:
+                net.update(name, {'img_h': img_height, 'img_w': img_width})
+        priorbox_layers.append(net[name])
+
+        # Create objectness prediction layer.
+        if use_objectness:
+            name = "{}_mbox_objectness".format(from_layer)
+            num_obj_output = num_priors_per_location * 2;
+            ConvBNLayer(net, from_layer, name, use_bn=use_batchnorm, use_relu=False, lr_mult=lr_mult,
+                num_output=num_obj_output, kernel_size=kernel_size, pad=pad, stride=1, **bn_param)
+            permute_name = "{}_perm".format(name)
+            net[permute_name] = L.Permute(net[name], order=[0, 2, 3, 1])
+            flatten_name = "{}_flat".format(name)
+            net[flatten_name] = L.Flatten(net[permute_name], axis=1)
+            objectness_layers.append(net[flatten_name])
+
+    # Concatenate priorbox, loc, and conf layers.
+    mbox_layers = []
+    name = '{}{}'.format(prefix, "_loc")
+    net[name] = L.Concat(*loc_layers, axis=1)
+    mbox_layers.append(net[name])
+    name = '{}{}'.format(prefix, "_conf")
+    net[name] = L.Concat(*conf_layers, axis=1)
+    mbox_layers.append(net[name])
+    name = '{}{}'.format(prefix, "_priorbox")
+    net[name] = L.Concat(*priorbox_layers, axis=2)
+    mbox_layers.append(net[name])
+    if use_objectness:
+        name = '{}{}'.format(prefix, "_objectness")
+        net[name] = L.Concat(*objectness_layers, axis=1)
+        mbox_layers.append(net[name])
+
+    return mbox_layers
+
+
+
+def CreateRefineDetHead(net, data_layer="data", num_classes=[], from_layers=[], from_layers2=[],
+        normalizations=[], use_batchnorm=True, lr_mult=1, min_sizes=[], max_sizes=[], prior_variance = [0.1],
+        aspect_ratios=[], steps=[], img_height=0, img_width=0, share_location=True,
+        flip=True, clip=True, offset=0.5, inter_layer_depth=[], kernel_size=1, pad=0,
+        conf_postfix='', loc_postfix='', **bn_param):
+    assert num_classes, "must provide num_classes"
+    assert num_classes > 0, "num_classes must be positive number"
+    if normalizations:
+        assert len(from_layers) == len(normalizations), "from_layers and normalizations should have same length"
+    assert len(from_layers) == len(min_sizes), "from_layers and min_sizes should have same length"
+    if max_sizes:
+        assert len(from_layers) == len(max_sizes), "from_layers and max_sizes should have same length"
+    if aspect_ratios:
+        assert len(from_layers) == len(aspect_ratios), "from_layers and aspect_ratios should have same length"
+    if steps:
+        assert len(from_layers) == len(steps), "from_layers and steps should have same length"
+    net_layers = net.keys()
+    assert data_layer in net_layers, "data_layer is not in net's layers"
+    if inter_layer_depth:
+        assert len(from_layers) == len(inter_layer_depth), "from_layers and inter_layer_depth should have same length"
+
+    prefix = 'arm'
+    num_classes_rpn = 2
+    num = len(from_layers)
+    priorbox_layers = []
+    loc_layers = []
+    conf_layers = []
+    for i in range(0, num):
+        from_layer = from_layers[i]
+
+        # Get the normalize value.
+        if normalizations:
+            if normalizations[i] != -1:
+                norm_name = "{}_norm".format(from_layer)
+                net[norm_name] = L.Normalize(net[from_layer], scale_filler=dict(type="constant", value=normalizations[i]),
+                    across_spatial=False, channel_shared=False)
+                from_layer = norm_name
+
+        # Add intermediate layers.
+        if inter_layer_depth:
+            if inter_layer_depth[i] > 0:
+                inter_name = "{}_inter".format(from_layer)
+                ResBody(net, from_layer, inter_name, out2a=256, out2b=256, out2c=1024, stride=1, use_branch1=True)
+                # ConvBNLayer(net, from_layer, inter_name, use_bn=use_batchnorm, use_relu=True, lr_mult=lr_mult,
+                #       num_output=inter_layer_depth[i], kernel_size=3, pad=1, stride=1, **bn_param)
+                from_layer = "res{}".format(inter_name)
+
+        # Estimate number of priors per location given provided parameters.
+        min_size = min_sizes[i]
+        if type(min_size) is not list:
+            min_size = [min_size]
+        aspect_ratio = []
+        if len(aspect_ratios) > i:
+            aspect_ratio = aspect_ratios[i]
+            if type(aspect_ratio) is not list:
+                aspect_ratio = [aspect_ratio]
+        max_size = []
+        if len(max_sizes) > i:
+            max_size = max_sizes[i]
+            if type(max_size) is not list:
+                max_size = [max_size]
+            if max_size:
+                assert len(max_size) == len(min_size), "max_size and min_size should have same length."
+        if max_size:
+            num_priors_per_location = (2 + len(aspect_ratio)) * len(min_size)
+        else:
+            num_priors_per_location = (1 + len(aspect_ratio)) * len(min_size)
+        if flip:
+            num_priors_per_location += len(aspect_ratio) * len(min_size)
+        step = []
+        if len(steps) > i:
+            step = steps[i]
+
+        # Create location prediction layer.
+        name = "{}_mbox_loc{}".format(from_layer, loc_postfix)
+        num_loc_output = num_priors_per_location * 4
+        if not share_location:
+            num_loc_output *= num_classes_rpn
+        ConvBNLayer(net, from_layer, name, use_bn=use_batchnorm, use_relu=False, lr_mult=lr_mult,
+            num_output=num_loc_output, kernel_size=kernel_size, pad=pad, stride=1, **bn_param)
+        permute_name = "{}_perm".format(name)
+        net[permute_name] = L.Permute(net[name], order=[0, 2, 3, 1])
+        flatten_name = "{}_flat".format(name)
+        net[flatten_name] = L.Flatten(net[permute_name], axis=1)
+        loc_layers.append(net[flatten_name])
+
+        # Create confidence prediction layer.
+        name = "{}_mbox_conf{}".format(from_layer, conf_postfix)
+        num_conf_output = num_priors_per_location * num_classes_rpn
+        ConvBNLayer(net, from_layer, name, use_bn=use_batchnorm, use_relu=False, lr_mult=lr_mult,
+            num_output=num_conf_output, kernel_size=kernel_size, pad=pad, stride=1, **bn_param)
+        permute_name = "{}_perm".format(name)
+        net[permute_name] = L.Permute(net[name], order=[0, 2, 3, 1])
+        flatten_name = "{}_flat".format(name)
+        net[flatten_name] = L.Flatten(net[permute_name], axis=1)
+        conf_layers.append(net[flatten_name])
+
+        # Create prior generation layer.
+        name = "{}_mbox_priorbox".format(from_layer)
+        net[name] = L.PriorBox(net[from_layer], net[data_layer], min_size=min_size,
+                clip=clip, variance=prior_variance, offset=offset)
+        if max_size:
+            net.update(name, {'max_size': max_size})
+        if aspect_ratio:
+            net.update(name, {'aspect_ratio': aspect_ratio, 'flip': flip})
+        if step:
+            net.update(name, {'step': step})
+        if img_height != 0 and img_width != 0:
+            if img_height == img_width:
+                net.update(name, {'img_size': img_height})
+            else:
+                net.update(name, {'img_h': img_height, 'img_w': img_width})
+        priorbox_layers.append(net[name])
+
+    # Concatenate priorbox, loc, and conf layers.
+    mbox_layers = []
+    name = '{}{}'.format(prefix, "_loc")
+    net[name] = L.Concat(*loc_layers, axis=1)
+    mbox_layers.append(net[name])
+    name = '{}{}'.format(prefix, "_conf")
+    net[name] = L.Concat(*conf_layers, axis=1)
+    mbox_layers.append(net[name])
+    name = '{}{}'.format(prefix, "_priorbox")
+    net[name] = L.Concat(*priorbox_layers, axis=2)
+    mbox_layers.append(net[name])
+
+
+
+    prefix = 'odm'
+    num = len(from_layers2)
+    loc_layers = []
+    conf_layers = []
+    for i in range(0, num):
+        from_layer = from_layers2[i]
+
+        # Get the normalize value.
+        if normalizations:
+            if normalizations[i] != -1:
+                norm_name = "{}_norm".format(from_layer)
+                net[norm_name] = L.Normalize(net[from_layer], scale_filler=dict(type="constant", value=normalizations[i]),
+                    across_spatial=False, channel_shared=False)
+                from_layer = norm_name
+
+        # Add intermediate layers.
+        if inter_layer_depth:
+            if inter_layer_depth[i] > 0:
+                inter_name = "{}_inter".format(from_layer)
+                ResBody(net, from_layer, inter_name, out2a=256, out2b=256, out2c=1024, stride=1, use_branch1=True)
+                # ConvBNLayer(net, from_layer, inter_name, use_bn=use_batchnorm, use_relu=True, lr_mult=lr_mult,
+                #       num_output=inter_layer_depth[i], kernel_size=3, pad=1, stride=1, **bn_param)
+                # from_layer = inter_name
+                from_layer = "res{}".format(inter_name)
+
+        # Estimate number of priors per location given provided parameters.
+        min_size = min_sizes[i]
+        if type(min_size) is not list:
+            min_size = [min_size]
+        aspect_ratio = []
+        if len(aspect_ratios) > i:
+            aspect_ratio = aspect_ratios[i]
+            if type(aspect_ratio) is not list:
+                aspect_ratio = [aspect_ratio]
+        max_size = []
+        if len(max_sizes) > i:
+            max_size = max_sizes[i]
+            if type(max_size) is not list:
+                max_size = [max_size]
+            if max_size:
+                assert len(max_size) == len(min_size), "max_size and min_size should have same length."
+        if max_size:
+            num_priors_per_location = (2 + len(aspect_ratio)) * len(min_size)
+        else:
+            num_priors_per_location = (1 + len(aspect_ratio)) * len(min_size)
+        if flip:
+            num_priors_per_location += len(aspect_ratio) * len(min_size)
+
+        # Create location prediction layer.
+        name = "{}_mbox_loc{}".format(from_layer, loc_postfix)
+        num_loc_output = num_priors_per_location * 4
+        if not share_location:
+            num_loc_output *= num_classes
+        ConvBNLayer(net, from_layer, name, use_bn=use_batchnorm, use_relu=False, lr_mult=lr_mult,
+                    num_output=num_loc_output, kernel_size=kernel_size, pad=pad, stride=1, **bn_param)
+        permute_name = "{}_perm".format(name)
+        net[permute_name] = L.Permute(net[name], order=[0, 2, 3, 1])
+        flatten_name = "{}_flat".format(name)
+        net[flatten_name] = L.Flatten(net[permute_name], axis=1)
+        loc_layers.append(net[flatten_name])
+
+        # Create confidence prediction layer.
+        name = "{}_mbox_conf{}".format(from_layer, conf_postfix)
+        num_conf_output = num_priors_per_location * num_classes
+        ConvBNLayer(net, from_layer, name, use_bn=use_batchnorm, use_relu=False, lr_mult=lr_mult,
+                    num_output=num_conf_output, kernel_size=kernel_size, pad=pad, stride=1, **bn_param)
+        permute_name = "{}_perm".format(name)
+        net[permute_name] = L.Permute(net[name], order=[0, 2, 3, 1])
+        flatten_name = "{}_flat".format(name)
+        net[flatten_name] = L.Flatten(net[permute_name], axis=1)
+        conf_layers.append(net[flatten_name])
+
+
+    # Concatenate priorbox, loc, and conf layers.
+    name = '{}{}'.format(prefix, "_loc")
+    net[name] = L.Concat(*loc_layers, axis=1)
+    mbox_layers.append(net[name])
+    name = '{}{}'.format(prefix, "_conf")
+    net[name] = L.Concat(*conf_layers, axis=1)
+    mbox_layers.append(net[name])
+
+    return mbox_layers
\ No newline at end of file
diff --git a/python/caffe/net_spec.py b/python/caffe/net_spec.py
index 5d13067..f60e8bf 100644
--- a/python/caffe/net_spec.py
+++ b/python/caffe/net_spec.py
@@ -37,7 +37,9 @@ def param_name_dict():
     # strip the final '_param' or 'Parameter'
     param_names = [s[:-len('_param')] for s in param_names]
     param_type_names = [s[:-len('Parameter')] for s in param_type_names]
-    return dict(zip(param_type_names, param_names))
+    res = dict(zip(param_type_names, param_names))
+    res["Deconvolution"] = "convolution"
+    return res
 
 
 def to_proto(*tops):
diff --git a/src/caffe/proto/caffe.proto b/src/caffe/proto/caffe.proto
index eb3ba92..e5f8c1f 100644
--- a/src/caffe/proto/caffe.proto
+++ b/src/caffe/proto/caffe.proto
@@ -1404,6 +1404,8 @@ message DetectionOutputParameter {
   optional float visualize_threshold = 11;
   // If provided, save outputs to video file.
   optional string save_file = 12;
+  //the objectness score is used for the anchor refinement module to filter easy negative anchor.
+  optional float objectness_score = 24 [default = 0.01];
 }
 
 message DropoutParameter {
@@ -1704,6 +1706,9 @@ message MultiBoxLossParameter {
   optional int32 sample_size = 22 [default = 64];
   optional bool use_prior_for_nms = 23 [default = false];
   optional bool ignore_difficult_gt = 24 [default = false];
+
+  //the objectness score is used for the anchor refinement module to filter easy negative anchor.
+  optional float objectness_score = 25 [default = 0.01];
 }
 
 message MVNParameter {
